{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a572b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–ê–ô–¢–ò –ê–£–î–ò–û –ò–ó –î–ê–¢–ê–°–ï–¢–ê –° WER 0\n",
    "# –ü–û–°–¢–†–û–ò–¢–¨ GENERATE –° –≠–¢–ò–ú –ê–£–î–ò–û –ë–ï–ó BATCH DECODE - –ù–£–ñ–ù–´ –¢–û–ö–ï–ù–´\n",
    "# –í–ó–Ø–¢–¨ AUDIO + –¢–ï–ö–°–¢ –ò –° –ü–û–ú–û–©–¨–Æ DATASET –ó–ê–ì–†–£–ó–ò–¢–¨ –ò–• - –ü–†–û–í–ï–†–ò–¢–¨ –ß–¢–û –¢–û–ö–ï–ù–´ –°–û–í–ü–ê–î–ê–Æ–¢\n",
    "# –ü–†–û–ë–õ–ï–ú–ê –î–û–õ–ñ–ù–ê –ë–´–¢–¨ –í –ù–ê–ß–ê–õ–¨–ù–´–• –¢–û–ö–ï–ù–ê–•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "502d21c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "from src.data import DataManager\n",
    "from src.config import load_config\n",
    "from jiwer import wer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6befb0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞...\n",
      "Device: cuda\n",
      "\n",
      "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...\n",
      "–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: 26654 samples\n",
      "\n",
      "–ü–æ–∏—Å–∫ sample —Å WER=0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26654 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "  0%|          | 31/26654 [00:04<1:10:01,  6.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ –ù–∞–π–¥–µ–Ω sample —Å WER=0 –Ω–∞ –∏–Ω–¥–µ–∫—Å–µ 31\n",
      "Reference: –°–ø—Ä—è—Ç–∞—Ç—å—Å—è —Ç—É—Ç –Ω–µ–≥–¥–µ.\n",
      "Prediction:  –°–ø—Ä—è—Ç–∞—Ç—å—Å—è —Ç—É—Ç –Ω–µ–≥–¥–µ.\n",
      "\n",
      "üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º sample #31 –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# –ë–õ–û–ö 1: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –ø–æ–∏—Å–∫ sample —Å WER=0\n",
    "\n",
    "\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞...\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞ –∏ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "print(\"\\n–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...\")\n",
    "config = load_config(\"../configs/whisper_base.yaml\")\n",
    "data_manager = DataManager(config)\n",
    "data_manager.set_already_loaded_processor(processor)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º train –¥–∞—Ç–∞—Å–µ—Ç\n",
    "train_dataset = data_manager.create_dataset('train')  # –û–≥—Ä–∞–Ω–∏—á–∏–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
    "print(f\"–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {len(train_dataset)} samples\")\n",
    "\n",
    "# –ò—â–µ–º sample —Å WER=0\n",
    "print(\"\\n–ü–æ–∏—Å–∫ sample —Å WER=0...\")\n",
    "perfect_sample_idx = None\n",
    "\n",
    "for idx in tqdm(range(len(train_dataset))):\n",
    "    try:\n",
    "        # –ü–æ–ª—É—á–∞–µ–º sample –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "        sample = train_dataset[idx]\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º reference —Ç–µ–∫—Å—Ç (–∫–ª—é—á \"text\", –∞ –Ω–µ \"reference\")\n",
    "        reference = sample['text']\n",
    "        \n",
    "        # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –º–æ–¥–µ–ª–∏\n",
    "        input_features = sample['input_features'].unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                language=\"ru\",\n",
    "                task=\"transcribe\",\n",
    "                max_length=448\n",
    "            )\n",
    "        \n",
    "        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –±–µ–∑ skip_special_tokens –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "        prediction = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º WER\n",
    "        sample_wer = wer(reference, prediction)\n",
    "        \n",
    "        if sample_wer == 0.0:\n",
    "            perfect_sample_idx = idx\n",
    "            print(f\"\\n‚úÖ –ù–∞–π–¥–µ–Ω sample —Å WER=0 –Ω–∞ –∏–Ω–¥–µ–∫—Å–µ {idx}\")\n",
    "            print(f\"Reference: {reference}\")\n",
    "            print(f\"Prediction: {prediction}\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –Ω–∞ sample {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "if perfect_sample_idx is None:\n",
    "    print(\"\\n‚ö†Ô∏è Sample —Å WER=0 –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–≤—ã—Ö 500 samples\")\n",
    "    print(\"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–≤–µ–ª–∏—á–∏—Ç—å limit –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å validation –¥–∞—Ç–∞—Å–µ—Ç\")\n",
    "else:\n",
    "    print(f\"\\nüéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º sample #{perfect_sample_idx} –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "l1lxudfosqs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ê–Ω–∞–ª–∏–∑ sample #31\n",
      "================================================================================\n",
      "\n",
      "üîÆ GENERATE (–º–æ–¥–µ–ª—å):\n",
      "\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: 13\n",
      "\n",
      "–ü–µ—Ä–≤—ã–µ 20 —Ç–æ–∫–µ–Ω–æ–≤: [2933, 14566, 2229, 1008, 776, 5243, 681, 11, 1725, 2344, 678, 14016, 13]\n",
      "\n",
      "–ü–æ—Å–ª–µ–¥–Ω–∏–µ 20 —Ç–æ–∫–µ–Ω–æ–≤: [2933, 14566, 2229, 1008, 776, 5243, 681, 11, 1725, 2344, 678, 14016, 13]\n",
      "\n",
      "–¢–µ–∫—Å—Ç —Å —Å–ø–µ—Ü —Ç–æ–∫–µ–Ω–∞–º–∏:\n",
      "' –°–ø–µ—Ä–µ–¥–µ–Ω —Å–µ–Ω—Ç—è, –Ω–µ –ª—å–¥–∏.'\n",
      "\n",
      "–¢–µ–∫—Å—Ç –ë–ï–ó —Å–ø–µ—Ü —Ç–æ–∫–µ–Ω–æ–≤:\n",
      " –°–ø–µ—Ä–µ–¥–µ–Ω —Å–µ–Ω—Ç—è, –Ω–µ –ª—å–¥–∏.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# –ë–õ–û–ö 2: Generate –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ sample - –≤—ã–≤–æ–¥ –¢–û–ö–ï–ù–û–í (—Å —Å–ø–µ—Ü —Ç–æ–∫–µ–Ω–∞–º–∏)\n",
    "\n",
    "if perfect_sample_idx is None:\n",
    "    print(\"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º - sample –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "else:\n",
    "    print(f\"–ê–Ω–∞–ª–∏–∑ sample #{perfect_sample_idx}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º sample\n",
    "    sample = train_dataset[perfect_sample_idx]\n",
    "    input_features = sample['input_features'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate - –ø–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω—ã\n",
    "    print(\"\\nüîÆ GENERATE (–º–æ–¥–µ–ª—å):\")\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(\n",
    "            input_features,\n",
    "            language=\"ru\",\n",
    "            task=\"transcribe\",\n",
    "            max_length=448\n",
    "        )\n",
    "    \n",
    "    # –í—ã–≤–æ–¥–∏–º —Ç–æ–∫–µ–Ω—ã\n",
    "    predicted_ids_list = predicted_ids[0].cpu().tolist()\n",
    "    print(f\"\\n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(predicted_ids_list)}\")\n",
    "    print(f\"\\n–ü–µ—Ä–≤—ã–µ 20 —Ç–æ–∫–µ–Ω–æ–≤: {predicted_ids_list[:20]}\")\n",
    "    print(f\"\\n–ü–æ—Å–ª–µ–¥–Ω–∏–µ 20 —Ç–æ–∫–µ–Ω–æ–≤: {predicted_ids_list[-20:]}\")\n",
    "    \n",
    "    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –ë–ï–ó skip_special_tokens\n",
    "    predicted_text_with_special = processor.decode(predicted_ids[0], skip_special_tokens=False)\n",
    "    print(f\"\\n–¢–µ–∫—Å—Ç —Å —Å–ø–µ—Ü —Ç–æ–∫–µ–Ω–∞–º–∏:\\n{repr(predicted_text_with_special)}\")\n",
    "    \n",
    "    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –° skip_special_tokens –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "    predicted_text = processor.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "    print(f\"\\n–¢–µ–∫—Å—Ç –ë–ï–ó —Å–ø–µ—Ü —Ç–æ–∫–µ–Ω–æ–≤:\\n{predicted_text}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6jit758ycvq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–ê–Ω–∞–ª–∏–∑ labels –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è sample #31\n",
      "================================================================================\n",
      "\n",
      "üìã LABELS (–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞):\n",
      "\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (–≤–∫–ª—é—á–∞—è -100): 12\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –±–µ–∑ -100: 12\n",
      "\n",
      "–ü–µ—Ä–≤—ã–µ 20 —Ç–æ–∫–µ–Ω–æ–≤: [50258, 50363, 8068, 39263, 4558, 8525, 12848, 757, 4953, 6790, 13, 50257]\n",
      "\n",
      "–ü–æ—Å–ª–µ–¥–Ω–∏–µ 20 —Ç–æ–∫–µ–Ω–æ–≤: [50258, 50363, 8068, 39263, 4558, 8525, 12848, 757, 4953, 6790, 13, 50257]\n",
      "\n",
      "–¢–µ–∫—Å—Ç —Å —Å–ø–µ—Ü —Ç–æ–∫–µ–Ω–∞–º–∏:\n",
      "'<|startoftranscript|><|notimestamps|>–°–ø—Ä—è—Ç–∞—Ç—å—Å—è —Ç—É—Ç –Ω–µ–≥–¥–µ.<|endoftext|>'\n",
      "\n",
      "–¢–µ–∫—Å—Ç –ë–ï–ó —Å–ø–µ—Ü —Ç–æ–∫–µ–Ω–æ–≤:\n",
      "–°–ø—Ä—è—Ç–∞—Ç—å—Å—è —Ç—É—Ç –Ω–µ–≥–¥–µ.\n",
      "\n",
      "Reference (–æ—Ä–∏–≥–∏–Ω–∞–ª):\n",
      "–°–ø—Ä—è—Ç–∞—Ç—å—Å—è —Ç—É—Ç –Ω–µ–≥–¥–µ.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# –ë–õ–û–ö 3: –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–≥–æ –∂–µ sample —á–µ—Ä–µ–∑ dataset - –≤—ã–≤–æ–¥ labels (—Å —Å–ø–µ—Ü —Ç–æ–∫–µ–Ω–∞–º–∏)\n",
    "\n",
    "if perfect_sample_idx is None:\n",
    "    print(\"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º - sample –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "else:\n",
    "    print(f\"\\n–ê–Ω–∞–ª–∏–∑ labels –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è sample #{perfect_sample_idx}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º sample –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "    sample = train_dataset[perfect_sample_idx]\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º labels (—Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–Ω—É—Ç—Ä–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞)\n",
    "    labels = sample['labels']  # Tensor shape: (sequence_length,)\n",
    "    \n",
    "    print(\"\\nüìã LABELS (–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞):\")\n",
    "    \n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫, –∑–∞–º–µ–Ω—è—è -100 –Ω–∞ pad_token_id –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "    labels_list = labels.cpu().tolist()\n",
    "    \n",
    "    # –§–∏–ª—å—Ç—Ä—É–µ–º -100 (padding –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –¥–ª—è loss)\n",
    "    labels_without_ignore = [token_id if token_id != -100 else processor.tokenizer.pad_token_id \n",
    "                              for token_id in labels_list]\n",
    "    \n",
    "    print(f\"\\n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (–≤–∫–ª—é—á–∞—è -100): {len(labels_list)}\")\n",
    "    print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –±–µ–∑ -100: {len([x for x in labels_list if x != -100])}\")\n",
    "    \n",
    "    print(f\"\\n–ü–µ—Ä–≤—ã–µ 20 —Ç–æ–∫–µ–Ω–æ–≤: {labels_list[:20]}\")\n",
    "    print(f\"\\n–ü–æ—Å–ª–µ–¥–Ω–∏–µ 20 —Ç–æ–∫–µ–Ω–æ–≤: {labels_list[-20:]}\")\n",
    "    \n",
    "    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –ë–ï–ó skip_special_tokens\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ -100 —Ç–æ–∫–µ–Ω—ã –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "    labels_for_decode = [t for t in labels_list if t != -100]\n",
    "    labels_text_with_special = processor.tokenizer.decode(labels_for_decode, skip_special_tokens=False)\n",
    "    print(f\"\\n–¢–µ–∫—Å—Ç —Å —Å–ø–µ—Ü —Ç–æ–∫–µ–Ω–∞–º–∏:\\n{repr(labels_text_with_special)}\")\n",
    "    \n",
    "    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –° skip_special_tokens\n",
    "    labels_text = processor.tokenizer.decode(labels_for_decode, skip_special_tokens=True)\n",
    "    print(f\"\\n–¢–µ–∫—Å—Ç –ë–ï–ó —Å–ø–µ—Ü —Ç–æ–∫–µ–Ω–æ–≤:\\n{labels_text}\")\n",
    "    \n",
    "    # Reference –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–∫–ª—é—á \"text\")\n",
    "    print(f\"\\nReference (–æ—Ä–∏–≥–∏–Ω–∞–ª):\\n{sample['text']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1jwvrbi8kawh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç –°–†–ê–í–ù–ï–ù–ò–ï –¢–û–ö–ï–ù–û–í\n",
      "================================================================================\n",
      "\n",
      "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n",
      "Generate tokens: 12\n",
      "Labels tokens: 12\n",
      "–†–∞–∑–Ω–∏—Ü–∞ –≤ –¥–ª–∏–Ω–µ: 0\n",
      "\n",
      "üîç –ü–µ—Ä–≤—ã–µ 30 —Ç–æ–∫–µ–Ω–æ–≤:\n",
      "–ü–æ–∑–∏—Ü–∏—è    Generate        Labels          –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ\n",
      "------------------------------------------------------------\n",
      "0          2933            50258           ‚ùå\n",
      "1          1552            50363           ‚ùå\n",
      "2          1675            8068            ‚ùå\n",
      "3          681             39263           ‚ùå\n",
      "4          11508           4558            ‚ùå\n",
      "5          4396            8525            ‚ùå\n",
      "6          1814            12848           ‚ùå\n",
      "7          1414            757             ‚ùå\n",
      "8          1725            4953            ‚ùå\n",
      "9          17255           6790            ‚ùå\n",
      "10         3396            13              ‚ùå\n",
      "11         13              50257           ‚ùå\n",
      "\n",
      "üîç –ü–æ—Å–ª–µ–¥–Ω–∏–µ 20 —Ç–æ–∫–µ–Ω–æ–≤:\n",
      "–ü–æ–∑–∏—Ü–∏—è    Generate        Labels          –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ\n",
      "------------------------------------------------------------\n",
      "0          2933            50258           ‚ùå\n",
      "1          1552            50363           ‚ùå\n",
      "2          1675            8068            ‚ùå\n",
      "3          681             39263           ‚ùå\n",
      "4          11508           4558            ‚ùå\n",
      "5          4396            8525            ‚ùå\n",
      "6          1814            12848           ‚ùå\n",
      "7          1414            757             ‚ùå\n",
      "8          1725            4953            ‚ùå\n",
      "9          17255           6790            ‚ùå\n",
      "10         3396            13              ‚ùå\n",
      "11         13              50257           ‚ùå\n",
      "\n",
      "‚ùå –ù–ê–ô–î–ï–ù–û 10 —Ä–∞–∑–ª–∏—á–∏–π (–ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10):\n",
      "  –ü–æ–∑–∏—Ü–∏—è 0: Generate token 2933 ‚Üí ' –°'\n",
      "  –ü–æ–∑–∏—Ü–∏—è 0: Labels token 50258 ‚Üí '<|startoftranscript|>'\n",
      "\n",
      "  –ü–æ–∑–∏—Ü–∏—è 1: Generate token 1552 ‚Üí '–±'\n",
      "  –ü–æ–∑–∏—Ü–∏—è 1: Labels token 50363 ‚Üí '<|notimestamps|>'\n",
      "\n",
      "  –ü–æ–∑–∏—Ü–∏—è 2: Generate token 1675 ‚Üí '–ª–∏'\n",
      "  –ü–æ–∑–∏—Ü–∏—è 2: Labels token 8068 ‚Üí '–°'\n",
      "\n",
      "  –ü–æ–∑–∏—Ü–∏—è 3: Generate token 681 ‚Üí '—è'\n",
      "  –ü–æ–∑–∏—Ü–∏—è 3: Labels token 39263 ‚Üí '–ø—Ä'\n",
      "\n",
      "  –ü–æ–∑–∏—Ü–∏—è 4: Generate token 11508 ‚Üí ' —ç—Ç–æ—Ç'\n",
      "  –ü–æ–∑–∏—Ü–∏—è 4: Labels token 4558 ‚Üí '—è—Ç'\n",
      "\n",
      "  –ü–æ–∑–∏—Ü–∏—è 5: Generate token 4396 ‚Üí ' –∑–∞'\n",
      "  –ü–æ–∑–∏—Ü–∏—è 5: Labels token 8525 ‚Üí '–∞—Ç—å—Å—è'\n",
      "\n",
      "  –ü–æ–∑–∏—Ü–∏—è 6: Generate token 1814 ‚Üí '—Ü'\n",
      "  –ü–æ–∑–∏—Ü–∏—è 6: Labels token 12848 ‚Üí ' —Ç—É—Ç'\n",
      "\n",
      "  –ü–æ–∑–∏—Ü–∏—è 7: Generate token 1414 ‚Üí '–µ–ª'\n",
      "  –ü–æ–∑–∏—Ü–∏—è 7: Labels token 757 ‚Üí ' –Ω'\n",
      "\n",
      "  –ü–æ–∑–∏—Ü–∏—è 8: Generate token 1725 ‚Üí ' –Ω–µ'\n",
      "  –ü–æ–∑–∏—Ü–∏—è 8: Labels token 4953 ‚Üí '–µ–≥'\n",
      "\n",
      "  –ü–æ–∑–∏—Ü–∏—è 9: Generate token 17255 ‚Üí ' –∏–¥'\n",
      "  –ü–æ–∑–∏—Ü–∏—è 9: Labels token 6790 ‚Üí '–¥–µ'\n",
      "\n",
      "\n",
      "üéØ –°–ü–ï–¶–ò–ê–õ–¨–ù–´–ï –¢–û–ö–ï–ù–´:\n",
      "BOS token: <|endoftext|> (id=50257)\n",
      "EOS token: <|endoftext|> (id=50257)\n",
      "PAD token: <|endoftext|> (id=50257)\n",
      "\n",
      "–í Generate:\n",
      "  BOS: ‚ùå\n",
      "  EOS: ‚ùå\n",
      "  PAD: ‚ùå\n",
      "\n",
      "–í Labels:\n",
      "  BOS: ‚úÖ\n",
      "  EOS: ‚úÖ\n",
      "  PAD: ‚úÖ\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# –ë–õ–û–ö 4: –°–†–ê–í–ù–ï–ù–ò–ï —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ generate –∏ labels\n",
    "\n",
    "if perfect_sample_idx is None:\n",
    "    print(\"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º - sample –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "else:\n",
    "    print(\"\\nüîç –°–†–ê–í–ù–ï–ù–ò–ï –¢–û–ö–ï–ù–û–í\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º –æ–±–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "    sample = train_dataset[perfect_sample_idx]\n",
    "    \n",
    "    # Generate tokens\n",
    "    input_features = sample['input_features'].unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(\n",
    "            input_features,\n",
    "            language=\"ru\",\n",
    "            task=\"transcribe\",\n",
    "            max_length=448\n",
    "        )\n",
    "    predicted_tokens = predicted_ids[0].cpu().tolist()\n",
    "    \n",
    "    # Dataset labels\n",
    "    labels = sample['labels'].cpu().tolist()\n",
    "    labels_tokens = [t for t in labels if t != -100]  # –£–±–∏—Ä–∞–µ–º -100\n",
    "    \n",
    "    print(f\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
    "    print(f\"Generate tokens: {len(predicted_tokens)}\")\n",
    "    print(f\"Labels tokens: {len(labels_tokens)}\")\n",
    "    print(f\"–†–∞–∑–Ω–∏—Ü–∞ –≤ –¥–ª–∏–Ω–µ: {abs(len(predicted_tokens) - len(labels_tokens))}\")\n",
    "    \n",
    "    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ N —Ç–æ–∫–µ–Ω–æ–≤\n",
    "    print(f\"\\nüîç –ü–µ—Ä–≤—ã–µ 30 —Ç–æ–∫–µ–Ω–æ–≤:\")\n",
    "    print(f\"{'–ü–æ–∑–∏—Ü–∏—è':<10} {'Generate':<15} {'Labels':<15} {'–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ'}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    max_len = max(len(predicted_tokens), len(labels_tokens))\n",
    "    differences = []\n",
    "    \n",
    "    for i in range(min(30, max_len)):\n",
    "        pred_tok = predicted_tokens[i] if i < len(predicted_tokens) else \"---\"\n",
    "        label_tok = labels_tokens[i] if i < len(labels_tokens) else \"---\"\n",
    "        match = \"‚úÖ\" if pred_tok == label_tok else \"‚ùå\"\n",
    "        \n",
    "        if pred_tok != label_tok:\n",
    "            differences.append(i)\n",
    "        \n",
    "        print(f\"{i:<10} {str(pred_tok):<15} {str(label_tok):<15} {match}\")\n",
    "    \n",
    "    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Ç–æ–∫–µ–Ω–æ–≤\n",
    "    print(f\"\\nüîç –ü–æ—Å–ª–µ–¥–Ω–∏–µ 20 —Ç–æ–∫–µ–Ω–æ–≤:\")\n",
    "    print(f\"{'–ü–æ–∑–∏—Ü–∏—è':<10} {'Generate':<15} {'Labels':<15} {'–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ'}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i in range(max(0, max_len - 20), max_len):\n",
    "        pred_tok = predicted_tokens[i] if i < len(predicted_tokens) else \"---\"\n",
    "        label_tok = labels_tokens[i] if i < len(labels_tokens) else \"---\"\n",
    "        match = \"‚úÖ\" if pred_tok == label_tok else \"‚ùå\"\n",
    "        \n",
    "        if pred_tok != label_tok and i not in differences:\n",
    "            differences.append(i)\n",
    "        \n",
    "        print(f\"{i:<10} {str(pred_tok):<15} {str(label_tok):<15} {match}\")\n",
    "    \n",
    "    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–∞–∑–ª–∏—á–∞—é—â–∏–µ—Å—è —Ç–æ–∫–µ–Ω—ã\n",
    "    if differences:\n",
    "        print(f\"\\n‚ùå –ù–ê–ô–î–ï–ù–û {len(differences[:10])} —Ä–∞–∑–ª–∏—á–∏–π (–ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10):\")\n",
    "        for diff_idx in differences[:10]:\n",
    "            if diff_idx < len(predicted_tokens):\n",
    "                pred_tok = predicted_tokens[diff_idx]\n",
    "                pred_decoded = processor.tokenizer.decode([pred_tok])\n",
    "                print(f\"  –ü–æ–∑–∏—Ü–∏—è {diff_idx}: Generate token {pred_tok} ‚Üí '{pred_decoded}'\")\n",
    "            \n",
    "            if diff_idx < len(labels_tokens):\n",
    "                label_tok = labels_tokens[diff_idx]\n",
    "                label_decoded = processor.tokenizer.decode([label_tok])\n",
    "                print(f\"  –ü–æ–∑–∏—Ü–∏—è {diff_idx}: Labels token {label_tok} ‚Üí '{label_decoded}'\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"\\n‚úÖ –í—Å–µ —Ç–æ–∫–µ–Ω—ã –ò–î–ï–ù–¢–ò–ß–ù–´!\")\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–µ—Ü —Ç–æ–∫–µ–Ω—ã\n",
    "    print(f\"\\nüéØ –°–ü–ï–¶–ò–ê–õ–¨–ù–´–ï –¢–û–ö–ï–ù–´:\")\n",
    "    print(f\"BOS token: {processor.tokenizer.bos_token} (id={processor.tokenizer.bos_token_id})\")\n",
    "    print(f\"EOS token: {processor.tokenizer.eos_token} (id={processor.tokenizer.eos_token_id})\")\n",
    "    print(f\"PAD token: {processor.tokenizer.pad_token} (id={processor.tokenizer.pad_token_id})\")\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–ø–µ—Ü —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ–±–µ–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö\n",
    "    print(f\"\\n–í Generate:\")\n",
    "    print(f\"  BOS: {'‚úÖ' if processor.tokenizer.bos_token_id in predicted_tokens else '‚ùå'}\")\n",
    "    print(f\"  EOS: {'‚úÖ' if processor.tokenizer.eos_token_id in predicted_tokens else '‚ùå'}\")\n",
    "    print(f\"  PAD: {'‚úÖ' if processor.tokenizer.pad_token_id in predicted_tokens else '‚ùå'}\")\n",
    "    \n",
    "    print(f\"\\n–í Labels:\")\n",
    "    print(f\"  BOS: {'‚úÖ' if processor.tokenizer.bos_token_id in labels_tokens else '‚ùå'}\")\n",
    "    print(f\"  EOS: {'‚úÖ' if processor.tokenizer.eos_token_id in labels_tokens else '‚ùå'}\")\n",
    "    print(f\"  PAD: {'‚úÖ' if processor.tokenizer.pad_token_id in labels_tokens else '‚ùå'}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "etpu0d45ypt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí∞ –í–´–ß–ò–°–õ–ï–ù–ò–ï LOSS\n",
      "================================================================================\n",
      "–í—ã—á–∏—Å–ª—è–µ–º loss —á–µ—Ä–µ–∑ forward pass...\n",
      "\n",
      "üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:\n",
      "Loss –¥–ª—è —ç—Ç–æ–≥–æ sample: 9.0791\n",
      "Logits shape: torch.Size([1, 12, 51865])\n",
      "\n",
      "üîç –ê–ù–ê–õ–ò–ó:\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è loss: 12\n",
      "\n",
      "üéØ Loss –ø–æ –ø–µ—Ä–≤—ã–º 10 —Ç–æ–∫–µ–Ω–∞–º:\n",
      "  –¢–æ–∫–µ–Ω 0: id=50258 '<|startoftranscript|>' | P=0.0000 | loss=14.2814\n",
      "  –¢–æ–∫–µ–Ω 1: id=50363 '<|notimestamps|>' | P=0.0000 | loss=12.3371\n",
      "  –¢–æ–∫–µ–Ω 2: id= 8068 '–°         ' | P=0.0000 | loss=13.1259\n",
      "  –¢–æ–∫–µ–Ω 3: id=39263 '–ø—Ä        ' | P=0.0000 | loss=12.3315\n",
      "  –¢–æ–∫–µ–Ω 4: id= 4558 '—è—Ç        ' | P=0.0001 | loss=8.8245\n",
      "  –¢–æ–∫–µ–Ω 5: id= 8525 '–∞—Ç—å—Å—è     ' | P=0.0000 | loss=11.3015\n",
      "  –¢–æ–∫–µ–Ω 6: id=12848 ' —Ç—É—Ç      ' | P=0.0000 | loss=12.2148\n",
      "  –¢–æ–∫–µ–Ω 7: id=  757 ' –Ω        ' | P=0.0109 | loss=4.5165\n",
      "  –¢–æ–∫–µ–Ω 8: id= 4953 '–µ–≥        ' | P=0.0003 | loss=8.0281\n",
      "  –¢–æ–∫–µ–Ω 9: id= 6790 '–¥–µ        ' | P=0.0015 | loss=6.5041\n",
      "\n",
      "‚úÖ –ï—Å–ª–∏ –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã, –Ω–æ loss –≤—ã—Å–æ–∫–∏–π - –∑–Ω–∞—á–∏—Ç –º–æ–¥–µ–ª—å –Ω–µ —É–≤–µ—Ä–µ–Ω–∞ –≤ —Å–≤–æ–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö!\n",
      "‚úÖ WER –º–æ–∂–µ—Ç –±—ã—Ç—å 0 (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞), –Ω–æ loss –≤—ã—Å–æ–∫–∏–π (–Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# –ë–õ–û–ö 5: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss –¥–ª—è —ç—Ç–æ–≥–æ sample\n",
    "\n",
    "if perfect_sample_idx is None:\n",
    "    print(\"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º - sample –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "else:\n",
    "    print(\"\\nüí∞ –í–´–ß–ò–°–õ–ï–ù–ò–ï LOSS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º sample\n",
    "    sample = train_dataset[perfect_sample_idx]\n",
    "    input_features = sample['input_features'].unsqueeze(0).to(device)\n",
    "    labels = sample['labels'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Forward pass —Å labels –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è loss\n",
    "    print(\"–í—ã—á–∏—Å–ª—è–µ–º loss —á–µ—Ä–µ–∑ forward pass...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_features=input_features, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    print(f\"\\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:\")\n",
    "    print(f\"Loss –¥–ª—è —ç—Ç–æ–≥–æ sample: {loss.item():.4f}\")\n",
    "    print(f\"Logits shape: {logits.shape}\")  # (batch_size, sequence_length, vocab_size)\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –∏–º–µ–Ω–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ loss\n",
    "    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º labels\n",
    "    labels_cpu = labels[0].cpu().tolist()\n",
    "    labels_no_ignore = [t for t in labels_cpu if t != -100]\n",
    "    \n",
    "    print(f\"\\nüîç –ê–ù–ê–õ–ò–ó:\")\n",
    "    print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è loss: {len(labels_no_ignore)}\")\n",
    "    \n",
    "    # –ü–æ—Å—á–∏—Ç–∞–µ–º loss –≤—Ä—É—á–Ω—É—é –¥–ª—è –ø–µ—Ä–≤—ã—Ö –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    print(f\"\\nüéØ Loss –ø–æ –ø–µ—Ä–≤—ã–º 10 —Ç–æ–∫–µ–Ω–∞–º:\")\n",
    "    for i in range(min(10, len(labels_no_ignore))):\n",
    "        token_id = labels_cpu[i]\n",
    "        \n",
    "        if token_id == -100:\n",
    "            continue\n",
    "            \n",
    "        # –ü–æ–ª—É—á–∞–µ–º –ª–æ–≥–∏—Ç—ã –¥–ª—è —ç—Ç–æ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "        token_logits = logits[0, i, :]  # shape: (vocab_size,)\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n",
    "        probs = F.softmax(token_logits, dim=0)\n",
    "        \n",
    "        # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "        correct_prob = probs[token_id].item()\n",
    "        \n",
    "        # Loss –¥–ª—è —ç—Ç–æ–≥–æ —Ç–æ–∫–µ–Ω–∞: -log(P(correct_token))\n",
    "        token_loss = -torch.log(probs[token_id]).item()\n",
    "        \n",
    "        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω\n",
    "        token_text = processor.tokenizer.decode([token_id])\n",
    "        \n",
    "        print(f\"  –¢–æ–∫–µ–Ω {i}: id={token_id:5d} '{token_text:10s}' | P={correct_prob:.4f} | loss={token_loss:.4f}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ –ï—Å–ª–∏ –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã, –Ω–æ loss –≤—ã—Å–æ–∫–∏–π - –∑–Ω–∞—á–∏—Ç –º–æ–¥–µ–ª—å –Ω–µ —É–≤–µ—Ä–µ–Ω–∞ –≤ —Å–≤–æ–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö!\")\n",
    "    print(f\"‚úÖ WER –º–æ–∂–µ—Ç –±—ã—Ç—å 0 (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞), –Ω–æ loss –≤—ã—Å–æ–∫–∏–π (–Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "m8qn14a76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ FORWARD PASS (teacher forcing)\n",
      "================================================================================\n",
      "–í—ã–ø–æ–ª–Ω—è–µ–º forward pass —Å labels...\n",
      "\n",
      "üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:\n",
      "Labels length: 12\n",
      "Predicted length: 12\n",
      "\n",
      "üîç –ü–ï–†–í–´–ï 10 –¢–û–ö–ï–ù–û–í:\n",
      "–ü–æ–∑–∏—Ü–∏—è    Labels          Predicted       Match      Token\n",
      "--------------------------------------------------------------------------------\n",
      "0          50258           50263           ‚ùå          L:'<|startoftranscript|>' P:'<|ru|>'\n",
      "1          50363           50263           ‚ùå          L:'<|notimestamps|>' P:'<|ru|>'\n",
      "2          8068            50257           ‚ùå          L:'–°' P:'<|endoftext|>'\n",
      "3          39263           50257           ‚ùå          L:'–ø—Ä' P:'<|endoftext|>'\n",
      "4          4558            39263           ‚ùå          L:'—è—Ç' P:'–ø—Ä'\n",
      "5          8525            4558            ‚ùå          L:'–∞—Ç—å—Å—è' P:'—è—Ç'\n",
      "6          12848           50257           ‚ùå          L:' —Ç—É—Ç' P:'<|endoftext|>'\n",
      "7          757             510             ‚ùå          L:' –Ω' P:' here'\n",
      "8          4953            757             ‚ùå          L:'–µ–≥' P:' –Ω'\n",
      "9          6790            4953            ‚ùå          L:'–¥–µ' P:'–µ–≥'\n",
      "\n",
      "üìù –ü–û–õ–ù–´–ï –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û–°–¢–ò:\n",
      "Labels: '<|startoftranscript|><|notimestamps|>–°–ø—Ä—è—Ç–∞—Ç—å—Å—è —Ç—É—Ç –Ω–µ–≥–¥–µ.<|endoftext|>'\n",
      "Predicted: '<|ru|><|ru|><|endoftext|><|endoftext|>–ø—Ä—è—Ç<|endoftext|> here –Ω–µ–≥–¥–µ<|endoftext|>'\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# –ë–õ–û–ö 6: Forward pass - –ø—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç (teacher forcing)\n",
    "\n",
    "if perfect_sample_idx is None:\n",
    "    print(\"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º - sample –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "else:\n",
    "    print(\"\\nüîÑ FORWARD PASS (teacher forcing)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º sample\n",
    "    sample = train_dataset[perfect_sample_idx]\n",
    "    input_features = sample['input_features'].unsqueeze(0).to(device)\n",
    "    labels = sample['labels'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Forward pass —Å labels (teacher forcing)\n",
    "    print(\"–í—ã–ø–æ–ª–Ω—è–µ–º forward pass —Å labels...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_features=input_features, labels=labels)\n",
    "        logits = outputs.logits  # (batch, sequence_length, vocab_size)\n",
    "    \n",
    "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ (argmax)\n",
    "    predicted_token_ids = torch.argmax(logits, dim=-1)[0]  # (sequence_length,)\n",
    "    predicted_tokens = predicted_token_ids.cpu().tolist()\n",
    "    \n",
    "    # Labels\n",
    "    labels_cpu = labels[0].cpu().tolist()\n",
    "    labels_no_ignore = [t for t in labels_cpu if t != -100]\n",
    "    \n",
    "    print(f\"\\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:\")\n",
    "    print(f\"Labels length: {len(labels_no_ignore)}\")\n",
    "    print(f\"Predicted length: {len(predicted_tokens)}\")\n",
    "    \n",
    "    print(f\"\\nüîç –ü–ï–†–í–´–ï 10 –¢–û–ö–ï–ù–û–í:\")\n",
    "    print(f\"{'–ü–æ–∑–∏—Ü–∏—è':<10} {'Labels':<15} {'Predicted':<15} {'Match':<10} {'Token'}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i in range(min(10, len(labels_no_ignore))):\n",
    "        label_tok = labels_cpu[i]\n",
    "        if label_tok == -100:\n",
    "            continue\n",
    "        \n",
    "        pred_tok = predicted_tokens[i]\n",
    "        match = \"‚úÖ\" if label_tok == pred_tok else \"‚ùå\"\n",
    "        \n",
    "        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ–±–∞ —Ç–æ–∫–µ–Ω–∞\n",
    "        label_decoded = processor.tokenizer.decode([label_tok])\n",
    "        pred_decoded = processor.tokenizer.decode([pred_tok])\n",
    "        \n",
    "        print(f\"{i:<10} {label_tok:<15} {pred_tok:<15} {match:<10} L:'{label_decoded}' P:'{pred_decoded}'\")\n",
    "    \n",
    "    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "    print(f\"\\nüìù –ü–û–õ–ù–´–ï –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û–°–¢–ò:\")\n",
    "    \n",
    "    # Labels (–±–µ–∑ -100)\n",
    "    labels_text = processor.tokenizer.decode(labels_no_ignore, skip_special_tokens=False)\n",
    "    print(f\"Labels: {repr(labels_text)}\")\n",
    "    \n",
    "    # Predicted\n",
    "    predicted_text = processor.tokenizer.decode(predicted_tokens, skip_special_tokens=False)\n",
    "    print(f\"Predicted: {repr(predicted_text)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "r7ucrl3q02o",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ –ü–†–ê–í–ò–õ–¨–ù–´–ï –°–ü–ï–¶ –¢–û–ö–ï–ù–´ –î–õ–Ø WHISPER\n",
      "================================================================================\n",
      "\n",
      "üìã –û—Å–Ω–æ–≤–Ω—ã–µ —Å–ø–µ—Ü —Ç–æ–∫–µ–Ω—ã:\n",
      "<|startoftranscript|>: 50258\n",
      "<|endoftext|>: 50257\n",
      "\n",
      "üåê –Ø–∑—ã–∫–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã:\n",
      "<|ru|> (—Ä—É—Å—Å–∫–∏–π): 50263\n",
      "<|en|> (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π): 50259\n",
      "\n",
      "üìù –ó–∞–¥–∞—á–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã:\n",
      "<|transcribe|>: 50359\n",
      "<|translate|>: 50358\n",
      "\n",
      "‚è±Ô∏è Timestamp —Ç–æ–∫–µ–Ω—ã:\n",
      "<|notimestamps|>: 50363\n",
      "\n",
      "‚úÖ –ü–†–ê–í–ò–õ–¨–ù–ê–Ø –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û–°–¢–¨ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (transcribe, no timestamps):\n",
      "–¢–æ–∫–µ–Ω—ã: [50258, 50263, 50359, 50363]\n",
      "  50258: <|startoftranscript|>\n",
      "  50263: <|ru|>\n",
      "  50359: <|transcribe|>\n",
      "  50363: <|notimestamps|>\n",
      "\n",
      "‚ùå –ß–¢–û –°–ï–ô–ß–ê–° –í LABELS (sample #31):\n",
      "–ü–µ—Ä–≤—ã–µ 4 —Ç–æ–∫–µ–Ω–∞: [50258, 50363, 8068, 39263]\n",
      "  50258: <|startoftranscript|>\n",
      "  50363: <|notimestamps|>\n",
      "  8068: –°\n",
      "  39263: –ø—Ä\n",
      "\n",
      "üîç –°–†–ê–í–ù–ï–ù–ò–ï:\n",
      "‚ùå Labels –ù–ï —Å–æ–¥–µ—Ä–∂–∞—Ç –≤—Å–µ –ø—Ä–µ—Ñ–∏–∫—Å —Ç–æ–∫–µ–Ω—ã!\n",
      "–û–∂–∏–¥–∞–ª–æ—Å—å: [50258, 50263, 50359, 50363]\n",
      "–ü–æ–ª—É—á–µ–Ω–æ: [50258, 50363, 8068, 39263]\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# –ë–õ–û–ö 7: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ü–†–ê–í–ò–õ–¨–ù–´–• —Å–ø–µ—Ü —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
    "\n",
    "print(\"\\nüéØ –ü–†–ê–í–ò–õ–¨–ù–´–ï –°–ü–ï–¶ –¢–û–ö–ï–ù–´ –î–õ–Ø WHISPER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Whisper tokenizer –∏–º–µ–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è —è–∑—ã–∫–∞ –∏ –∑–∞–¥–∞—á–∏\n",
    "print(\"\\nüìã –û—Å–Ω–æ–≤–Ω—ã–µ —Å–ø–µ—Ü —Ç–æ–∫–µ–Ω—ã:\")\n",
    "print(f\"<|startoftranscript|>: {processor.tokenizer.convert_tokens_to_ids('<|startoftranscript|>')}\")\n",
    "print(f\"<|endoftext|>: {processor.tokenizer.eos_token_id}\")\n",
    "\n",
    "# –Ø–∑—ã–∫–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã\n",
    "print(f\"\\nüåê –Ø–∑—ã–∫–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã:\")\n",
    "print(f\"<|ru|> (—Ä—É—Å—Å–∫–∏–π): {processor.tokenizer.convert_tokens_to_ids('<|ru|>')}\")\n",
    "print(f\"<|en|> (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π): {processor.tokenizer.convert_tokens_to_ids('<|en|>')}\")\n",
    "\n",
    "# –ó–∞–¥–∞—á–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã\n",
    "print(f\"\\nüìù –ó–∞–¥–∞—á–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã:\")\n",
    "print(f\"<|transcribe|>: {processor.tokenizer.convert_tokens_to_ids('<|transcribe|>')}\")\n",
    "print(f\"<|translate|>: {processor.tokenizer.convert_tokens_to_ids('<|translate|>')}\")\n",
    "\n",
    "# Timestamp —Ç–æ–∫–µ–Ω—ã\n",
    "print(f\"\\n‚è±Ô∏è Timestamp —Ç–æ–∫–µ–Ω—ã:\")\n",
    "print(f\"<|notimestamps|>: {processor.tokenizer.convert_tokens_to_ids('<|notimestamps|>')}\")\n",
    "\n",
    "# –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ transcribe\n",
    "print(f\"\\n‚úÖ –ü–†–ê–í–ò–õ–¨–ù–ê–Ø –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û–°–¢–¨ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (transcribe, no timestamps):\")\n",
    "correct_prefix = [\n",
    "    processor.tokenizer.convert_tokens_to_ids('<|startoftranscript|>'),\n",
    "    processor.tokenizer.convert_tokens_to_ids('<|ru|>'),\n",
    "    processor.tokenizer.convert_tokens_to_ids('<|transcribe|>'),\n",
    "    processor.tokenizer.convert_tokens_to_ids('<|notimestamps|>')\n",
    "]\n",
    "print(f\"–¢–æ–∫–µ–Ω—ã: {correct_prefix}\")\n",
    "for tok_id in correct_prefix:\n",
    "    print(f\"  {tok_id}: {processor.tokenizer.decode([tok_id])}\")\n",
    "\n",
    "# –ß—Ç–æ –µ—Å—Ç—å –≤ –Ω–∞—à–∏—Ö labels\n",
    "print(f\"\\n‚ùå –ß–¢–û –°–ï–ô–ß–ê–° –í LABELS (sample #{perfect_sample_idx}):\")\n",
    "sample = train_dataset[perfect_sample_idx]\n",
    "labels_list = sample['labels'].cpu().tolist()\n",
    "labels_first_4 = labels_list[:4]\n",
    "print(f\"–ü–µ—Ä–≤—ã–µ 4 —Ç–æ–∫–µ–Ω–∞: {labels_first_4}\")\n",
    "for tok_id in labels_first_4:\n",
    "    if tok_id != -100:\n",
    "        print(f\"  {tok_id}: {processor.tokenizer.decode([tok_id])}\")\n",
    "\n",
    "print(f\"\\nüîç –°–†–ê–í–ù–ï–ù–ò–ï:\")\n",
    "if labels_first_4[:4] == correct_prefix:\n",
    "    print(\"‚úÖ Labels —Å–æ–¥–µ—Ä–∂–∞—Ç –í–°–ï –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å —Ç–æ–∫–µ–Ω—ã!\")\n",
    "else:\n",
    "    print(\"‚ùå Labels –ù–ï —Å–æ–¥–µ—Ä–∂–∞—Ç –≤—Å–µ –ø—Ä–µ—Ñ–∏–∫—Å —Ç–æ–∫–µ–Ω—ã!\")\n",
    "    print(f\"–û–∂–∏–¥–∞–ª–æ—Å—å: {correct_prefix}\")\n",
    "    print(f\"–ü–æ–ª—É—á–µ–Ω–æ: {labels_first_4}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "m76b38j99vc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç –ê–¢–†–ò–ë–£–¢–´ –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–ê\n",
      "================================================================================\n",
      "\n",
      "üìã Processor –∏ Tokenizer:\n",
      "Type processor: <class 'transformers.models.whisper.processing_whisper.WhisperProcessor'>\n",
      "Type tokenizer: <class 'transformers.models.whisper.tokenization_whisper_fast.WhisperTokenizerFast'>\n",
      "\n",
      "üåê –Ø–∑—ã–∫–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:\n",
      "tokenizer.language: None\n",
      "tokenizer.task: None\n",
      "tokenizer.predict_timestamps: False\n",
      "\n",
      "‚úÖ prefix_tokens: [50258, 50363]\n",
      "–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ:\n",
      "  50258: <|startoftranscript|>\n",
      "  50363: <|notimestamps|>\n",
      "\n",
      "üß™ –¢–ï–°–¢ –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–ò (—Ç–µ–∫—É—â–∏–π processor):\n",
      "–¢–µ–∫—Å—Ç: –û–Ω–∞ —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–∞—è –∂–µ–Ω—â–∏–Ω–∞.\n",
      "–¢–æ–∫–µ–Ω—ã: [50258, 50363, 7876, 1931, 36459, 7809, 6323, 28393, 7861, 13, 50257]\n",
      "–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: '<|startoftranscript|><|notimestamps|>–û–Ω–∞ —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–∞—è –∂–µ–Ω—â–∏–Ω–∞.<|endoftext|>'\n",
      "\n",
      "üì¶ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ:\n",
      "Type: <class 'transformers.models.whisper.tokenization_whisper_fast.WhisperTokenizerFast'>\n",
      "language: None\n",
      "task: None\n",
      "prefix_tokens: [50258, 50363]\n",
      "\n",
      "================================================================================\n",
      "üß™ –°–û–ó–î–ê–ù–ò–ï –ù–û–í–û–ì–û –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–ê –° –Ø–í–ù–´–ú–ò language –ò task\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'language': 'ru', 'task': 'transcribe'} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ–∑–¥–∞–Ω\n",
      "   language: ru\n",
      "   task: transcribe\n",
      "   predict_timestamps: False\n",
      "   prefix_tokens: [50258, 50263, 50359, 50363]\n",
      "     50258: <|startoftranscript|>\n",
      "     50263: <|ru|>\n",
      "     50359: <|transcribe|>\n",
      "     50363: <|notimestamps|>\n",
      "\n",
      "2Ô∏è‚É£ –í–ê–†–ò–ê–ù–¢ 1: –í—ã–∑–æ–≤ –ë–ï–ó —è–≤–Ω–æ–≥–æ —É–∫–∞–∑–∞–Ω–∏—è language/task (–∫–∞–∫ –≤ __getitem__):\n",
      "–¢–æ–∫–µ–Ω—ã: [50258, 50263, 50359, 50363, 7876, 1931, 36459, 7809, 6323, 28393, 7861, 13, 50257]\n",
      "–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: '<|startoftranscript|><|ru|><|transcribe|><|notimestamps|>–û–Ω–∞ —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–∞—è –∂–µ–Ω—â–∏–Ω–∞.<|endoftext|>'\n",
      "–ü–µ—Ä–≤—ã–µ 4 —Ç–æ–∫–µ–Ω–∞: [50258, 50263, 50359, 50363]\n",
      "\n",
      "3Ô∏è‚É£ –í–ê–†–ò–ê–ù–¢ 2: –í—ã–∑–æ–≤ –° –Ø–í–ù–´–ú —É–∫–∞–∑–∞–Ω–∏–µ–º language/task:\n",
      "–¢–æ–∫–µ–Ω—ã: [50258, 50263, 50359, 50363, 7876, 1931, 36459, 7809, 6323, 28393, 7861, 13, 50257]\n",
      "–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: '<|startoftranscript|><|ru|><|transcribe|><|notimestamps|>–û–Ω–∞ —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–∞—è –∂–µ–Ω—â–∏–Ω–∞.<|endoftext|>'\n",
      "–ü–µ—Ä–≤—ã–µ 4 —Ç–æ–∫–µ–Ω–∞: [50258, 50263, 50359, 50363]\n",
      "\n",
      "4Ô∏è‚É£ –°–†–ê–í–ù–ï–ù–ò–ï:\n",
      "‚úÖ –í–∞—Ä–∏–∞–Ω—Ç 1 –¥–∞–µ—Ç –ü–†–ê–í–ò–õ–¨–ù–´–ï –ø—Ä–µ—Ñ–∏–∫—Å —Ç–æ–∫–µ–Ω—ã!\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# –ë–õ–û–ö 8: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ —Å–ø–æ—Å–æ–±–æ–≤ –≤—ã–∑–æ–≤–∞\n",
    "\n",
    "print(\"\\nüîç –ê–¢–†–ò–ë–£–¢–´ –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–ê\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∏–º processor –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "print(\"\\nüìã Processor –∏ Tokenizer:\")\n",
    "print(f\"Type processor: {type(processor)}\")\n",
    "print(f\"Type tokenizer: {type(processor.tokenizer)}\")\n",
    "\n",
    "# –ö–ª—é—á–µ–≤—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã\n",
    "print(f\"\\nüåê –Ø–∑—ã–∫–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:\")\n",
    "print(f\"tokenizer.language: {getattr(processor.tokenizer, 'language', 'NOT SET')}\")\n",
    "print(f\"tokenizer.task: {getattr(processor.tokenizer, 'task', 'NOT SET')}\")\n",
    "print(f\"tokenizer.predict_timestamps: {getattr(processor.tokenizer, 'predict_timestamps', 'NOT SET')}\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∏–º prefix_tokens\n",
    "if hasattr(processor.tokenizer, 'prefix_tokens'):\n",
    "    prefix = processor.tokenizer.prefix_tokens\n",
    "    print(f\"\\n‚úÖ prefix_tokens: {prefix}\")\n",
    "    print(\"–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ:\")\n",
    "    for tok_id in prefix:\n",
    "        print(f\"  {tok_id}: {processor.tokenizer.decode([tok_id])}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå prefix_tokens –ù–ï –°–£–©–ï–°–¢–í–£–ï–¢!\")\n",
    "\n",
    "# –¢–µ—Å—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n",
    "print(f\"\\nüß™ –¢–ï–°–¢ –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–ò (—Ç–µ–∫—É—â–∏–π processor):\")\n",
    "test_text = \"–û–Ω–∞ —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–∞—è –∂–µ–Ω—â–∏–Ω–∞.\"\n",
    "test_tokens = processor.tokenizer(test_text, return_tensors=\"pt\").input_ids[0]\n",
    "print(f\"–¢–µ–∫—Å—Ç: {test_text}\")\n",
    "print(f\"–¢–æ–∫–µ–Ω—ã: {test_tokens.tolist()}\")\n",
    "print(f\"–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: {repr(processor.tokenizer.decode(test_tokens, skip_special_tokens=False))}\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∏–º, –∫–∞–∫–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "print(f\"\\nüì¶ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ:\")\n",
    "dataset_tokenizer = train_dataset.tokenizer\n",
    "print(f\"Type: {type(dataset_tokenizer)}\")\n",
    "print(f\"language: {getattr(dataset_tokenizer, 'language', 'NOT SET')}\")\n",
    "print(f\"task: {getattr(dataset_tokenizer, 'task', 'NOT SET')}\")\n",
    "\n",
    "if hasattr(dataset_tokenizer, 'prefix_tokens'):\n",
    "    prefix_dataset = dataset_tokenizer.prefix_tokens\n",
    "    print(f\"prefix_tokens: {prefix_dataset}\")\n",
    "else:\n",
    "    print(\"‚ùå prefix_tokens –ù–ï –°–£–©–ï–°–¢–í–£–ï–¢!\")\n",
    "\n",
    "# ========== –ù–û–í–´–ô –¢–ï–°–¢ ==========\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üß™ –°–û–ó–î–ê–ù–ò–ï –ù–û–í–û–ì–û –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–ê –° –Ø–í–ù–´–ú–ò language –ò task\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from transformers import WhisperTokenizer\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å language –∏ task\n",
    "print(\"\\n1Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞:\")\n",
    "tokenizer_with_lang = WhisperTokenizer.from_pretrained(\n",
    "    \"openai/whisper-base\",\n",
    "    language=\"ru\",\n",
    "    task=\"transcribe\"\n",
    ")\n",
    "print(f\"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ–∑–¥–∞–Ω\")\n",
    "print(f\"   language: {getattr(tokenizer_with_lang, 'language', 'NOT SET')}\")\n",
    "print(f\"   task: {getattr(tokenizer_with_lang, 'task', 'NOT SET')}\")\n",
    "print(f\"   predict_timestamps: {getattr(tokenizer_with_lang, 'predict_timestamps', 'NOT SET')}\")\n",
    "\n",
    "if hasattr(tokenizer_with_lang, 'prefix_tokens'):\n",
    "    prefix_new = tokenizer_with_lang.prefix_tokens\n",
    "    print(f\"   prefix_tokens: {prefix_new}\")\n",
    "    for tok_id in prefix_new:\n",
    "        print(f\"     {tok_id}: {tokenizer_with_lang.decode([tok_id])}\")\n",
    "\n",
    "# –í–∞—Ä–∏–∞–Ω—Ç 1: –í—ã–∑–æ–≤ –ë–ï–ó —è–≤–Ω–æ–≥–æ —É–∫–∞–∑–∞–Ω–∏—è language/task (–∫–∞–∫ –≤ __getitem__)\n",
    "print(\"\\n2Ô∏è‚É£ –í–ê–†–ò–ê–ù–¢ 1: –í—ã–∑–æ–≤ –ë–ï–ó —è–≤–Ω–æ–≥–æ —É–∫–∞–∑–∞–Ω–∏—è language/task (–∫–∞–∫ –≤ __getitem__):\")\n",
    "tokens_variant1 = tokenizer_with_lang(test_text, return_tensors=\"pt\").input_ids[0]\n",
    "print(f\"–¢–æ–∫–µ–Ω—ã: {tokens_variant1.tolist()}\")\n",
    "decoded1 = tokenizer_with_lang.decode(tokens_variant1, skip_special_tokens=False)\n",
    "print(f\"–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: {repr(decoded1)}\")\n",
    "print(f\"–ü–µ—Ä–≤—ã–µ 4 —Ç–æ–∫–µ–Ω–∞: {tokens_variant1[:4].tolist()}\")\n",
    "\n",
    "# –í–∞—Ä–∏–∞–Ω—Ç 2: –í—ã–∑–æ–≤ –° –Ø–í–ù–´–ú —É–∫–∞–∑–∞–Ω–∏–µ–º language/task\n",
    "print(\"\\n3Ô∏è‚É£ –í–ê–†–ò–ê–ù–¢ 2: –í—ã–∑–æ–≤ –° –Ø–í–ù–´–ú —É–∫–∞–∑–∞–Ω–∏–µ–º language/task:\")\n",
    "# –ü—Ä–æ–≤–µ—Ä–∏–º, –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –ª–∏ __call__ —ç—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "try:\n",
    "    tokens_variant2 = tokenizer_with_lang(\n",
    "        test_text,\n",
    "        return_tensors=\"pt\",\n",
    "        language=\"ru\",\n",
    "        task=\"transcribe\"\n",
    "    ).input_ids[0]\n",
    "    print(f\"–¢–æ–∫–µ–Ω—ã: {tokens_variant2.tolist()}\")\n",
    "    decoded2 = tokenizer_with_lang.decode(tokens_variant2, skip_special_tokens=False)\n",
    "    print(f\"–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: {repr(decoded2)}\")\n",
    "    print(f\"–ü–µ—Ä–≤—ã–µ 4 —Ç–æ–∫–µ–Ω–∞: {tokens_variant2[:4].tolist()}\")\n",
    "except TypeError as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞: {e}\")\n",
    "    print(\"–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –ù–ï –ø—Ä–∏–Ω–∏–º–∞–µ—Ç language/task –≤ __call__\")\n",
    "\n",
    "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ\n",
    "print(\"\\n4Ô∏è‚É£ –°–†–ê–í–ù–ï–ù–ò–ï:\")\n",
    "expected_prefix = [50258, 50263, 50359, 50363]  # <|startoftranscript|> <|ru|> <|transcribe|> <|notimestamps|>\n",
    "actual_prefix = tokens_variant1[:4].tolist()\n",
    "\n",
    "if actual_prefix == expected_prefix:\n",
    "    print(\"‚úÖ –í–∞—Ä–∏–∞–Ω—Ç 1 –¥–∞–µ—Ç –ü–†–ê–í–ò–õ–¨–ù–´–ï –ø—Ä–µ—Ñ–∏–∫—Å —Ç–æ–∫–µ–Ω—ã!\")\n",
    "else:\n",
    "    print(\"‚ùå –í–∞—Ä–∏–∞–Ω—Ç 1 –¥–∞–µ—Ç –ù–ï–ü–†–ê–í–ò–õ–¨–ù–´–ï –ø—Ä–µ—Ñ–∏–∫—Å —Ç–æ–∫–µ–Ω—ã!\")\n",
    "    print(f\"   –û–∂–∏–¥–∞–ª–æ—Å—å: {expected_prefix}\")\n",
    "    print(f\"   –ü–æ–ª—É—á–µ–Ω–æ:  {actual_prefix}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whm9uwkb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ë–õ–û–ö 9: –ü—Ä–æ–≤–µ—Ä–∫–∞ setup_processor() –∏–∑ DataManager\n",
    "\n",
    "print(\"\\nüß™ –ü–†–û–í–ï–†–ö–ê DataManager.setup_processor()\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –ù–û–í–´–ô DataManager (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –∏–∑ –±–ª–æ–∫–∞ 1)\n",
    "print(\"\\n1Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ DataManager:\")\n",
    "from src.data import DataManager\n",
    "from src.config import load_config\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥\n",
    "new_config = load_config(\"../configs/whisper_base.yaml\")\n",
    "print(f\"‚úÖ –ö–æ–Ω—Ñ–∏–≥ –∑–∞–≥—Ä—É–∂–µ–Ω\")\n",
    "print(f\"   data.language: {new_config.data.language}\")\n",
    "print(f\"   data.task: {new_config.data.task}\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º DataManager\n",
    "new_data_manager = DataManager(new_config)\n",
    "print(f\"‚úÖ DataManager —Å–æ–∑–¥–∞–Ω\")\n",
    "\n",
    "# –í—ã–∑—ã–≤–∞–µ–º setup_processor() —Å –Ø–í–ù–´–ú–ò –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ language –∏ task\n",
    "print(\"\\n2Ô∏è‚É£ –í—ã–∑–æ–≤ setup_processor() —Å —è–≤–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:\")\n",
    "new_data_manager.setup_processor(\n",
    "    model_name=\"openai/whisper-base\",\n",
    "    model_type=\"whisper\",\n",
    "    language=new_config.data.language,\n",
    "    task=new_config.data.task\n",
    ")\n",
    "print(f\"‚úÖ setup_processor() –≤—ã–ø–æ–ª–Ω–µ–Ω —Å language='{new_config.data.language}', task='{new_config.data.task}'\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–π processor\n",
    "new_processor = new_data_manager.processor\n",
    "new_tokenizer = new_processor.tokenizer\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ –ü–†–û–í–ï–†–ö–ê –ê–¢–†–ò–ë–£–¢–û–í –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–ê:\")\n",
    "print(f\"Type processor: {type(new_processor)}\")\n",
    "print(f\"Type tokenizer: {type(new_tokenizer)}\")\n",
    "print(f\"\\nüåê –Ø–∑—ã–∫–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:\")\n",
    "print(f\"   language: {getattr(new_tokenizer, 'language', 'NOT SET')}\")\n",
    "print(f\"   task: {getattr(new_tokenizer, 'task', 'NOT SET')}\")\n",
    "print(f\"   predict_timestamps: {getattr(new_tokenizer, 'predict_timestamps', 'NOT SET')}\")\n",
    "\n",
    "if hasattr(new_tokenizer, 'prefix_tokens'):\n",
    "    new_prefix = new_tokenizer.prefix_tokens\n",
    "    print(f\"\\n‚úÖ prefix_tokens: {new_prefix}\")\n",
    "    print(\"   –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ:\")\n",
    "    for tok_id in new_prefix:\n",
    "        print(f\"     {tok_id}: {new_tokenizer.decode([tok_id])}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå prefix_tokens –ù–ï –°–£–©–ï–°–¢–í–£–ï–¢!\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–µ—Ñ–∏–∫—Å–∞\n",
    "expected_prefix = [50258, 50263, 50359, 50363]  # <|startoftranscript|> <|ru|> <|transcribe|> <|notimestamps|>\n",
    "if hasattr(new_tokenizer, 'prefix_tokens'):\n",
    "    actual_prefix = new_tokenizer.prefix_tokens\n",
    "    if actual_prefix == expected_prefix:\n",
    "        print(\"\\n‚úÖ‚úÖ‚úÖ –ü–†–ï–§–ò–ö–° –¢–û–ö–ï–ù–´ –ü–†–ê–í–ò–õ–¨–ù–´–ï!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå‚ùå‚ùå –ü–†–ï–§–ò–ö–° –¢–û–ö–ï–ù–´ –ù–ï–ü–†–ê–í–ò–õ–¨–ù–´–ï!\")\n",
    "        print(f\"   –û–∂–∏–¥–∞–ª–æ—Å—å: {expected_prefix}\")\n",
    "        print(f\"   –ü–æ–ª—É—á–µ–Ω–æ:  {actual_prefix}\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º dataset —Å –ù–û–í–´–ú processor\n",
    "print(\"\\n4Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –Ω–æ–≤—ã–º processor:\")\n",
    "new_train_dataset = new_data_manager.create_dataset('train')\n",
    "print(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω: {len(new_train_dataset)} samples\")\n",
    "\n",
    "# –ë–µ—Ä—ë–º –æ–¥–∏–Ω —Å—ç–º–ø–ª\n",
    "print(\"\\n5Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ labels –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞:\")\n",
    "test_sample = new_train_dataset[0]\n",
    "test_labels = test_sample['labels'].cpu().tolist()\n",
    "test_labels_first_4 = test_labels[:4]\n",
    "\n",
    "print(f\"–ü–µ—Ä–≤—ã–µ 4 —Ç–æ–∫–µ–Ω–∞ –∏–∑ labels: {test_labels_first_4}\")\n",
    "print(\"–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ:\")\n",
    "for tok_id in test_labels_first_4:\n",
    "    if tok_id != -100:\n",
    "        print(f\"  {tok_id}: {new_tokenizer.decode([tok_id])}\")\n",
    "\n",
    "# –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞\n",
    "print(\"\\n6Ô∏è‚É£ –§–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê:\")\n",
    "if test_labels_first_4 == expected_prefix:\n",
    "    print(\"‚úÖ‚úÖ‚úÖ LABELS –°–û–î–ï–†–ñ–ê–¢ –í–°–ï 4 –ü–†–ï–§–ò–ö–° –¢–û–ö–ï–ù–ê!\")\n",
    "    print(\"     setup_processor() –†–ê–ë–û–¢–ê–ï–¢ –ü–†–ê–í–ò–õ–¨–ù–û!\")\n",
    "else:\n",
    "    print(\"‚ùå‚ùå‚ùå LABELS –ù–ï –°–û–î–ï–†–ñ–ê–¢ –í–°–ï –ü–†–ï–§–ò–ö–° –¢–û–ö–ï–ù–´!\")\n",
    "    print(f\"     –û–∂–∏–¥–∞–ª–æ—Å—å: {expected_prefix}\")\n",
    "    print(f\"     –ü–æ–ª—É—á–µ–Ω–æ:  {test_labels_first_4}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "k8ldp8zcy8o",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ FORWARD PASS –î–õ–Ø –ë–ê–¢–ß–ê –ò–ó 8 –ê–£–î–ò–û (—Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º processor)\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è:\n",
      "‚úÖ –ö–æ–Ω—Ñ–∏–≥ –∑–∞–≥—Ä—É–∂–µ–Ω: language=ru, task=transcribe\n",
      "‚úÖ DataManager —Å–æ–∑–¥–∞–Ω\n",
      "‚úÖ Processor —Å–æ–∑–¥–∞–Ω —Å language='ru', task='transcribe'\n",
      "   prefix_tokens: [50258, 50263, 50359, 50363]\n",
      "   ‚úÖ –ü—Ä–µ—Ñ–∏–∫—Å —Ç–æ–∫–µ–Ω—ã –ü–†–ê–í–ò–õ–¨–ù–´–ï!\n",
      "\n",
      "2Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏:\n",
      "‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ cuda\n",
      "\n",
      "3Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞:\n",
      "‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω: 26654 samples\n",
      "‚úÖ –ë–∞—Ç—á –∑–∞–≥—Ä—É–∂–µ–Ω\n",
      "   Batch size: 8\n",
      "   Input features shape: torch.Size([8, 80, 3000])\n",
      "   Labels shape: torch.Size([8, 27])\n",
      "\n",
      "4Ô∏è‚É£ Forward pass –¥–ª—è –±–∞—Ç—á–∞:\n",
      "‚úÖ Forward pass –∑–∞–≤–µ—Ä—à—ë–Ω\n",
      "   –°—Ä–µ–¥–Ω–∏–π loss –ø–æ –±–∞—Ç—á—É (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π): 2.9804\n",
      "   Logits shape: torch.Size([8, 27, 51865])\n",
      "\n",
      "5Ô∏è‚É£ –í–´–ß–ò–°–õ–ï–ù–ò–ï LOSS –í–†–£–ß–ù–£–Æ –¢–†–ï–ú–Ø –°–ü–û–°–û–ë–ê–ú–ò:\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ Loss –° –£–ß–ï–¢–û–ú –ø–µ—Ä–≤—ã—Ö 4 —Ç–æ–∫–µ–Ω–æ–≤:\n",
      "   Loss: 2.9804\n",
      "   –¢–æ–∫–µ–Ω–æ–≤ —É—á—Ç–µ–Ω–æ: 125\n",
      "\n",
      "2Ô∏è‚É£ Loss –ë–ï–ó –£–ß–ï–¢–ê –ø–µ—Ä–≤—ã—Ö 4 —Ç–æ–∫–µ–Ω–æ–≤:\n",
      "   Loss: 2.2322\n",
      "   –¢–æ–∫–µ–Ω–æ–≤ —É—á—Ç–µ–Ω–æ: 93\n",
      "   –†–∞–∑–Ω–∏—Ü–∞ —Å –≤–∞—Ä–∏–∞–Ω—Ç–æ–º 1: 0.7482\n",
      "\n",
      "3Ô∏è‚É£ Loss –ë–ï–ó –£–ß–ï–¢–ê —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞:\n",
      "   Loss: 1.9508\n",
      "   –¢–æ–∫–µ–Ω–æ–≤ —É—á—Ç–µ–Ω–æ: 117\n",
      "   –†–∞–∑–Ω–∏—Ü–∞ —Å –≤–∞—Ä–∏–∞–Ω—Ç–æ–º 1: 1.0296\n",
      "\n",
      "üìä –°–†–ê–í–ù–ï–ù–ò–ï:\n",
      "   –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π loss (PyTorch): 2.9804\n",
      "   –†—É—á–Ω–æ–π loss (–≤—Å–µ —Ç–æ–∫–µ–Ω—ã):      2.9804\n",
      "   –†—É—á–Ω–æ–π loss (–±–µ–∑ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤): 2.2322\n",
      "   –†—É—á–Ω–æ–π loss (–±–µ–∑ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞):  1.9508\n",
      "\n",
      "6Ô∏è‚É£ –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ö–ê–ñ–î–û–ì–û –ü–†–ò–ú–ï–†–ê:\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "–ü–†–ò–ú–ï–† 1/8\n",
      "================================================================================\n",
      "\n",
      "üí∞ LOSS:\n",
      "   –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π:           2.7330\n",
      "   –†—É—á–Ω–æ–π (–≤—Å–µ —Ç–æ–∫–µ–Ω—ã):      2.7330\n",
      "   –†—É—á–Ω–æ–π (–±–µ–∑ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤): 2.0639 (—Ç–æ–∫–µ–Ω–æ–≤: 13)\n",
      "   –†—É—á–Ω–æ–π (–±–µ–∑ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞):  1.7710 (—Ç–æ–∫–µ–Ω–æ–≤: 16)\n",
      "   –í–∫–ª–∞–¥ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤:        0.6691\n",
      "   –í–∫–ª–∞–¥ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞:         0.9619\n",
      "\n",
      "üìù –õ–ï–ô–ë–õ–´:\n",
      "   Reference:  –û–Ω–∞ —É–≤–∏–¥–∞–ª–∞ –º—É–∂–∞ –µ—â–µ –∏–∑–¥–∞–ª–µ–∫–∞.\n",
      "   Labels:     [50258, 50263, 50359, 50363, 7876, 1931, 21974, 6232, 22081, 386, 9910, 3943, 856, 1218, 36502, 13, 50257]\n",
      "   Predicted:  [50263, 50263, 50358, 50363, 20280, 7876, 21974, 19097, 9910, 386, 9910, 3943, 22500, 1218, 36502, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]\n",
      "\n",
      "üìù –¢–ï–ö–°–¢–´:\n",
      "   Reference:  –û–Ω–∞ —É–≤–∏–¥–∞–ª–∞ –º—É–∂–∞ –µ—â–µ –∏–∑–¥–∞–ª–µ–∫–∞.\n",
      "   Labels:     –û–Ω–∞ —É–≤–∏–¥–∞–ª–∞ –º—É–∂–∞ –µ—â–µ –∏–∑–¥–∞–ª–µ–∫–∞.\n",
      "   Predicted:   –û–Ω–∞–û —É–≤–∏–¥–µ–ª–∞ –µ—â–µ–∞ –µ—â–µ –∏–∑ –¥–∞–ª–∞–ª–µ–∫–∞\n",
      "\n",
      "üí° LOSS –ü–û –ü–†–ï–§–ò–ö–° –¢–û–ö–ï–ù–ê–ú (–ø–µ—Ä–≤—ã–µ 4):\n",
      "   –¢–æ–∫–µ–Ω 0: id=50258 '<|startoftranscript|>' | P=0.000000 | loss=18.1241\n",
      "   –¢–æ–∫–µ–Ω 1: id=50263 '<|ru|>              ' | P=0.856992 | loss=0.1543\n",
      "   –¢–æ–∫–µ–Ω 2: id=50359 '<|transcribe|>      ' | P=0.372962 | loss=0.9863\n",
      "   –¢–æ–∫–µ–Ω 3: id=50363 '<|notimestamps|>    ' | P=0.693649 | loss=0.3658\n",
      "\n",
      "================================================================================\n",
      "–ü–†–ò–ú–ï–† 2/8\n",
      "================================================================================\n",
      "\n",
      "üí∞ LOSS:\n",
      "   –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π:           2.5556\n",
      "   –†—É—á–Ω–æ–π (–≤—Å–µ —Ç–æ–∫–µ–Ω—ã):      2.5556\n",
      "   –†—É—á–Ω–æ–π (–±–µ–∑ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤): 1.7065 (—Ç–æ–∫–µ–Ω–æ–≤: 11)\n",
      "   –†—É—á–Ω–æ–π (–±–µ–∑ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞):  1.4214 (—Ç–æ–∫–µ–Ω–æ–≤: 14)\n",
      "   –í–∫–ª–∞–¥ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤:        0.8491\n",
      "   –í–∫–ª–∞–¥ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞:         1.1343\n",
      "\n",
      "üìù –õ–ï–ô–ë–õ–´:\n",
      "   Reference:  –ù–æ –ö–∏—Ç–∏ –Ω–µ —Å–ª—É—à–∞–ª–∞ –µ–µ —Å–ª–æ–≤.\n",
      "   Labels:     [50258, 50263, 50359, 50363, 6371, 354, 3422, 18686, 1725, 41839, 6232, 14803, 20319, 13, 50257]\n",
      "   Predicted:  [50263, 50263, 50358, 50363, 7264, 7876, 981, 18686, 1725, 41839, 1218, 14803, 20319, 13, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]\n",
      "\n",
      "üìù –¢–ï–ö–°–¢–´:\n",
      "   Reference:  –ù–æ –ö–∏—Ç–∏ –Ω–µ —Å–ª—É—à–∞–ª–∞ –µ–µ —Å–ª–æ–≤.\n",
      "   Labels:     –ù–æ –ö–∏—Ç–∏ –Ω–µ —Å–ª—É—à–∞–ª–∞ –µ–µ —Å–ª–æ–≤.\n",
      "   Predicted:   –ù–æ–û –∫–∏—Ç–∏ –Ω–µ —Å–ª—É—à–∞–ª –µ–µ —Å–ª–æ–≤.\n",
      "\n",
      "üí° LOSS –ü–û –ü–†–ï–§–ò–ö–° –¢–û–ö–ï–ù–ê–ú (–ø–µ—Ä–≤—ã–µ 4):\n",
      "   –¢–æ–∫–µ–Ω 0: id=50258 '<|startoftranscript|>' | P=0.000000 | loss=18.4352\n",
      "   –¢–æ–∫–µ–Ω 1: id=50263 '<|ru|>              ' | P=0.671532 | loss=0.3982\n",
      "   –¢–æ–∫–µ–Ω 2: id=50359 '<|transcribe|>      ' | P=0.496184 | loss=0.7008\n",
      "   –¢–æ–∫–µ–Ω 3: id=50363 '<|notimestamps|>    ' | P=0.971746 | loss=0.0287\n",
      "\n",
      "================================================================================\n",
      "–ü–†–ò–ú–ï–† 3/8\n",
      "================================================================================\n",
      "\n",
      "üí∞ LOSS:\n",
      "   –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π:           3.2510\n",
      "   –†—É—á–Ω–æ–π (–≤—Å–µ —Ç–æ–∫–µ–Ω—ã):      3.2510\n",
      "   –†—É—á–Ω–æ–π (–±–µ–∑ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤): 2.6347 (—Ç–æ–∫–µ–Ω–æ–≤: 10)\n",
      "   –†—É—á–Ω–æ–π (–±–µ–∑ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞):  2.1542 (—Ç–æ–∫–µ–Ω–æ–≤: 13)\n",
      "   –í–∫–ª–∞–¥ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤:        0.6163\n",
      "   –í–∫–ª–∞–¥ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞:         1.0968\n",
      "\n",
      "üìù –õ–ï–ô–ë–õ–´:\n",
      "   Reference:  –ù—É, –∫–∞–∫ –º–æ–π –ø—Ä–∏—è—Ç–µ–ª—å –æ—Ç–Ω–æ—Å–∏—Ç—Å—è?\n",
      "   Labels:     [50258, 50263, 50359, 50363, 20058, 11, 3014, 23400, 5082, 37764, 44539, 8254, 30, 50257]\n",
      "   Predicted:  [50263, 50263, 50358, 50363, 11011, 3014, 3014, 4777, 5082, 4558, 37245, 8254, 30, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]\n",
      "\n",
      "üìù –¢–ï–ö–°–¢–´:\n",
      "   Reference:  –ù—É, –∫–∞–∫ –º–æ–π –ø—Ä–∏—è—Ç–µ–ª—å –æ—Ç–Ω–æ—Å–∏—Ç—Å—è?\n",
      "   Labels:     –ù—É, –∫–∞–∫ –º–æ–π –ø—Ä–∏—è—Ç–µ–ª—å –æ—Ç–Ω–æ—Å–∏—Ç—Å—è?\n",
      "   Predicted:   –ö–∞–∫ –∫–∞–∫ –∫–∞–∫ –º—ã –ø—Ä–∏—è—Ç –Ω–æ—Å–∏—Ç—Å—è?\n",
      "\n",
      "üí° LOSS –ü–û –ü–†–ï–§–ò–ö–° –¢–û–ö–ï–ù–ê–ú (–ø–µ—Ä–≤—ã–µ 4):\n",
      "   –¢–æ–∫–µ–Ω 0: id=50258 '<|startoftranscript|>' | P=0.000000 | loss=17.5093\n",
      "   –¢–æ–∫–µ–Ω 1: id=50263 '<|ru|>              ' | P=0.657648 | loss=0.4191\n",
      "   –¢–æ–∫–µ–Ω 2: id=50359 '<|transcribe|>      ' | P=0.449554 | loss=0.7995\n",
      "   –¢–æ–∫–µ–Ω 3: id=50363 '<|notimestamps|>    ' | P=0.644818 | loss=0.4388\n",
      "\n",
      "================================================================================\n",
      "–ü–†–ò–ú–ï–† 4/8\n",
      "================================================================================\n",
      "\n",
      "üí∞ LOSS:\n",
      "   –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π:           3.5529\n",
      "   –†—É—á–Ω–æ–π (–≤—Å–µ —Ç–æ–∫–µ–Ω—ã):      3.5529\n",
      "   –†—É—á–Ω–æ–π (–±–µ–∑ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤): 3.1123 (—Ç–æ–∫–µ–Ω–æ–≤: 13)\n",
      "   –†—É—á–Ω–æ–π (–±–µ–∑ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞):  2.7875 (—Ç–æ–∫–µ–Ω–æ–≤: 16)\n",
      "   –í–∫–ª–∞–¥ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤:        0.4406\n",
      "   –í–∫–ª–∞–¥ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞:         0.7654\n",
      "\n",
      "üìù –õ–ï–ô–ë–õ–´:\n",
      "   Reference:  –û–Ω –ø–æ–¥—ä–µ—Ö–∞–ª –∫ –∫—Ä—ã–ª—å—Ü—É.\n",
      "   Labels:     [50258, 50263, 50359, 50363, 7876, 489, 4095, 8515, 8008, 1218, 981, 7502, 698, 6762, 22542, 13, 50257]\n",
      "   Predicted:  [50259, 50259, 50358, 50363, 3688, 40024, 4095, 8515, 8008, 1218, 981, 4679, 1635, 755, 13, 13, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]\n",
      "\n",
      "üìù –¢–ï–ö–°–¢–´:\n",
      "   Reference:  –û–Ω –ø–æ–¥—ä–µ—Ö–∞–ª –∫ –∫—Ä—ã–ª—å—Ü—É.\n",
      "   Labels:     –û–Ω –ø–æ–¥—ä–µ—Ö–∞–ª –∫ –∫—Ä—ã–ª—å—Ü—É.\n",
      "   Predicted:   –û–ø–∞ –ø–æ–¥—ä–µ—Ö–∞–ª –∫—Ä–∏–∏—Ç–∫..\n",
      "\n",
      "üí° LOSS –ü–û –ü–†–ï–§–ò–ö–° –¢–û–ö–ï–ù–ê–ú (–ø–µ—Ä–≤—ã–µ 4):\n",
      "   –¢–æ–∫–µ–Ω 0: id=50258 '<|startoftranscript|>' | P=0.000000 | loss=15.8000\n",
      "   –¢–æ–∫–µ–Ω 1: id=50263 '<|ru|>              ' | P=0.039439 | loss=3.2330\n",
      "   –¢–æ–∫–µ–Ω 2: id=50359 '<|transcribe|>      ' | P=0.403992 | loss=0.9064\n",
      "   –¢–æ–∫–µ–Ω 3: id=50363 '<|notimestamps|>    ' | P=0.999983 | loss=0.0000\n",
      "\n",
      "================================================================================\n",
      "–ü–†–ò–ú–ï–† 5/8\n",
      "================================================================================\n",
      "\n",
      "üí∞ LOSS:\n",
      "   –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π:           1.9561\n",
      "   –†—É—á–Ω–æ–π (–≤—Å–µ —Ç–æ–∫–µ–Ω—ã):      1.9561\n",
      "   –†—É—á–Ω–æ–π (–±–µ–∑ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤): 1.3821 (—Ç–æ–∫–µ–Ω–æ–≤: 23)\n",
      "   –†—É—á–Ω–æ–π (–±–µ–∑ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞):  1.2747 (—Ç–æ–∫–µ–Ω–æ–≤: 26)\n",
      "   –í–∫–ª–∞–¥ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤:        0.5740\n",
      "   –í–∫–ª–∞–¥ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞:         0.6813\n",
      "\n",
      "üìù –õ–ï–ô–ë–õ–´:\n",
      "   Reference:  –¢–∞–∫–æ–≥–æ —Ä–æ–¥–∞ –µ—Å—Ç—å —Ä—è–¥ –Ω–µ—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π –∏ –µ—Å—Ç—å –º—ã—Å–ª—å –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–≥–æ –º–∏—Ä–∞.\n",
      "   Labels:     [50258, 50263, 50359, 50363, 39005, 2350, 17925, 386, 5640, 32921, 1725, 753, 9294, 4310, 2292, 8583, 1006, 5640, 4777, 461, 6762, 8048, 23218, 4699, 41454, 13, 50257]\n",
      "   Predicted:  [50263, 50263, 50358, 50363, 22391, 2350, 17925, 386, 5640, 32921, 1725, 41365, 9294, 4310, 2292, 2241, 11, 29225, 4777, 48271, 6762, 8567, 23218, 2350, 41454, 13, 50257]\n",
      "\n",
      "üìù –¢–ï–ö–°–¢–´:\n",
      "   Reference:  –¢–∞–∫–æ–≥–æ —Ä–æ–¥–∞ –µ—Å—Ç—å —Ä—è–¥ –Ω–µ—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π –∏ –µ—Å—Ç—å –º—ã—Å–ª—å –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–≥–æ –º–∏—Ä–∞.\n",
      "   Labels:     –¢–∞–∫–æ–≥–æ —Ä–æ–¥–∞ –µ—Å—Ç—å —Ä—è–¥ –Ω–µ—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π –∏ –µ—Å—Ç—å –º—ã—Å–ª—å –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–≥–æ –º–∏—Ä–∞.\n",
      "   Predicted:   –ó–∞–æ–≥–æ —Ä–æ–¥–∞ –µ—Å—Ç—å —Ä—è–¥ –Ω–µ —á–µ–ª–æ–≤–µ—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏, –≤–µ—Å—å –º—ã—Å—ã–ª—å –ü—Ä–º–µ—Ç–æ–≥–æ –º–∏—Ä–∞.\n",
      "\n",
      "üí° LOSS –ü–û –ü–†–ï–§–ò–ö–° –¢–û–ö–ï–ù–ê–ú (–ø–µ—Ä–≤—ã–µ 4):\n",
      "   –¢–æ–∫–µ–Ω 0: id=50258 '<|startoftranscript|>' | P=0.000000 | loss=19.6704\n",
      "   –¢–æ–∫–µ–Ω 1: id=50263 '<|ru|>              ' | P=0.791026 | loss=0.2344\n",
      "   –¢–æ–∫–µ–Ω 2: id=50359 '<|transcribe|>      ' | P=0.326320 | loss=1.1199\n",
      "   –¢–æ–∫–µ–Ω 3: id=50363 '<|notimestamps|>    ' | P=0.998500 | loss=0.0015\n",
      "\n",
      "================================================================================\n",
      "–ü–†–ò–ú–ï–† 6/8\n",
      "================================================================================\n",
      "\n",
      "üí∞ LOSS:\n",
      "   –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π:           3.2291\n",
      "   –†—É—á–Ω–æ–π (–≤—Å–µ —Ç–æ–∫–µ–Ω—ã):      3.2291\n",
      "   –†—É—á–Ω–æ–π (–±–µ–∑ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤): 2.2777 (—Ç–æ–∫–µ–Ω–æ–≤: 9)\n",
      "   –†—É—á–Ω–æ–π (–±–µ–∑ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞):  1.8854 (—Ç–æ–∫–µ–Ω–æ–≤: 12)\n",
      "   –í–∫–ª–∞–¥ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤:        0.9514\n",
      "   –í–∫–ª–∞–¥ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞:         1.3437\n",
      "\n",
      "üìù –õ–ï–ô–ë–õ–´:\n",
      "   Reference:  –ï–≥–æ –Ω–∞—á–∏–Ω–∞–µ—Ç —Ä–≤–∞—Ç—å.\n",
      "   Labels:     [50258, 50263, 50359, 50363, 10156, 4567, 21995, 1094, 1475, 859, 2209, 13, 50257]\n",
      "   Predicted:  [50263, 50263, 50358, 50364, 3272, 50257, 21995, 1094, 1475, 859, 2209, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]\n",
      "\n",
      "üìù –¢–ï–ö–°–¢–´:\n",
      "   Reference:  –ï–≥–æ –Ω–∞—á–∏–Ω–∞–µ—Ç —Ä–≤–∞—Ç—å.\n",
      "   Labels:     –ï–≥–æ –Ω–∞—á–∏–Ω–∞–µ—Ç —Ä–≤–∞—Ç—å.\n",
      "   Predicted:   –ò –Ω–∞—á–∏–Ω–∞–µ—Ç —Ä–≤–∞—Ç—å\n",
      "\n",
      "üí° LOSS –ü–û –ü–†–ï–§–ò–ö–° –¢–û–ö–ï–ù–ê–ú (–ø–µ—Ä–≤—ã–µ 4):\n",
      "   –¢–æ–∫–µ–Ω 0: id=50258 '<|startoftranscript|>' | P=0.000000 | loss=19.3534\n",
      "   –¢–æ–∫–µ–Ω 1: id=50263 '<|ru|>              ' | P=0.812087 | loss=0.2081\n",
      "   –¢–æ–∫–µ–Ω 2: id=50359 '<|transcribe|>      ' | P=0.341518 | loss=1.0744\n",
      "   –¢–æ–∫–µ–Ω 3: id=50363 '<|notimestamps|>    ' | P=0.430473 | loss=0.8429\n",
      "\n",
      "================================================================================\n",
      "–ü–†–ò–ú–ï–† 7/8\n",
      "================================================================================\n",
      "\n",
      "üí∞ LOSS:\n",
      "   –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π:           2.2989\n",
      "   –†—É—á–Ω–æ–π (–≤—Å–µ —Ç–æ–∫–µ–Ω—ã):      2.2989\n",
      "   –†—É—á–Ω–æ–π (–±–µ–∑ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤): 1.2658 (—Ç–æ–∫–µ–Ω–æ–≤: 10)\n",
      "   –†—É—á–Ω–æ–π (–±–µ–∑ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞):  1.0462 (—Ç–æ–∫–µ–Ω–æ–≤: 13)\n",
      "   –í–∫–ª–∞–¥ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤:        1.0331\n",
      "   –í–∫–ª–∞–¥ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞:         1.2527\n",
      "\n",
      "üìù –õ–ï–ô–ë–õ–´:\n",
      "   Reference:  –ù–µ—Ç, —ç—Ç–æ –Ω–µ–ª—å–∑—è, –Ω–∞–¥–æ –Ω–∞—á–∞—Ç—å.\n",
      "   Labels:     [50258, 50263, 50359, 50363, 37473, 11, 2691, 33813, 11, 13256, 8970, 2209, 13, 50257]\n",
      "   Predicted:  [50263, 50263, 50359, 50363, 21249, 11, 2691, 33813, 13, 13256, 8970, 2209, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]\n",
      "\n",
      "üìù –¢–ï–ö–°–¢–´:\n",
      "   Reference:  –ù–µ—Ç, —ç—Ç–æ –Ω–µ–ª—å–∑—è, –Ω–∞–¥–æ –Ω–∞—á–∞—Ç—å.\n",
      "   Labels:     –ù–µ—Ç, —ç—Ç–æ –Ω–µ–ª—å–∑—è, –Ω–∞–¥–æ –Ω–∞—á–∞—Ç—å.\n",
      "   Predicted:   –ù–µ—Ç, —ç—Ç–æ –Ω–µ–ª—å–∑—è. –Ω–∞–¥–æ –Ω–∞—á–∞—Ç—å\n",
      "\n",
      "üí° LOSS –ü–û –ü–†–ï–§–ò–ö–° –¢–û–ö–ï–ù–ê–ú (–ø–µ—Ä–≤—ã–µ 4):\n",
      "   –¢–æ–∫–µ–Ω 0: id=50258 '<|startoftranscript|>' | P=0.000000 | loss=18.5840\n",
      "   –¢–æ–∫–µ–Ω 1: id=50263 '<|ru|>              ' | P=0.765312 | loss=0.2675\n",
      "   –¢–æ–∫–µ–Ω 2: id=50359 '<|transcribe|>      ' | P=0.609468 | loss=0.4952\n",
      "   –¢–æ–∫–µ–Ω 3: id=50363 '<|notimestamps|>    ' | P=0.835295 | loss=0.1800\n",
      "\n",
      "================================================================================\n",
      "–ü–†–ò–ú–ï–† 8/8\n",
      "================================================================================\n",
      "\n",
      "üí∞ LOSS:\n",
      "   –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π:           6.8582\n",
      "   –†—É—á–Ω–æ–π (–≤—Å–µ —Ç–æ–∫–µ–Ω—ã):      6.8582\n",
      "   –†—É—á–Ω–æ–π (–±–µ–∑ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤): 7.5603 (—Ç–æ–∫–µ–Ω–æ–≤: 4)\n",
      "   –†—É—á–Ω–æ–π (–±–µ–∑ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞):  5.4338 (—Ç–æ–∫–µ–Ω–æ–≤: 7)\n",
      "   –í–∫–ª–∞–¥ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤:        -0.7021\n",
      "   –í–∫–ª–∞–¥ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞:         1.4244\n",
      "\n",
      "üìù –õ–ï–ô–ë–õ–´:\n",
      "   Reference:  –º–æ–≥–ª–∏\n",
      "   Labels:     [50258, 50263, 50359, 50363, 919, 2353, 1675, 50257]\n",
      "   Predicted:  [50259, 50259, 50358, 50363, 3493, 919, 50257, 1675, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]\n",
      "\n",
      "üìù –¢–ï–ö–°–¢–´:\n",
      "   Reference:  –º–æ–≥–ª–∏\n",
      "   Labels:     –º–æ–≥–ª–∏\n",
      "   Predicted:   –ú–º–ª–∏\n",
      "\n",
      "üí° LOSS –ü–û –ü–†–ï–§–ò–ö–° –¢–û–ö–ï–ù–ê–ú (–ø–µ—Ä–≤—ã–µ 4):\n",
      "   –¢–æ–∫–µ–Ω 0: id=50258 '<|startoftranscript|>' | P=0.000000 | loss=16.8287\n",
      "   –¢–æ–∫–µ–Ω 1: id=50263 '<|ru|>              ' | P=0.004304 | loss=5.4483\n",
      "   –¢–æ–∫–µ–Ω 2: id=50359 '<|transcribe|>      ' | P=0.259326 | loss=1.3497\n",
      "   –¢–æ–∫–µ–Ω 3: id=50363 '<|notimestamps|>    ' | P=0.368777 | loss=0.9976\n",
      "\n",
      "================================================================================\n",
      "‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–Å–ù!\n",
      "================================================================================\n",
      "\n",
      "üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ë–ê–¢–ß–£:\n",
      "   –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π loss:           2.9804\n",
      "   –†—É—á–Ω–æ–π loss (–≤—Å–µ —Ç–æ–∫–µ–Ω—ã):      2.9804\n",
      "   –†—É—á–Ω–æ–π loss (–±–µ–∑ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤): 2.2322\n",
      "   –†—É—á–Ω–æ–π loss (–±–µ–∑ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞):  1.9508\n",
      "\n",
      "   –í–∫–ª–∞–¥ 4 –ø—Ä–µ—Ñ–∏–∫—Å —Ç–æ–∫–µ–Ω–æ–≤ –≤ loss: 0.7482\n",
      "   –í–∫–ª–∞–¥ 1 –ø—Ä–µ—Ñ–∏–∫—Å —Ç–æ–∫–µ–Ω–∞ –≤ loss:  1.0296\n",
      "\n",
      "   –û–∂–∏–¥–∞–µ–º—ã–π loss (–ø–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è): ~3-4\n",
      "   –°—Ç–∞—Ä—ã–π loss (—Å –±–∞–≥–æ–º): >12.0\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ LOSS –ó–ù–ê–ß–ò–¢–ï–õ–¨–ù–û –°–ù–ò–ó–ò–õ–°–Ø!\n",
      "     –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±–∞–≥–∞ —Å prefix_tokens –°–†–ê–ë–û–¢–ê–õ–û!\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# –ë–õ–û–ö 10: Forward pass –¥–ª—è –±–∞—Ç—á–∞ –∏–∑ 8 –∞—É–¥–∏–æ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º processor (—Å–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –±–ª–æ–∫)\n",
    "\n",
    "print(\"\\nüöÄ FORWARD PASS –î–õ–Ø –ë–ê–¢–ß–ê –ò–ó 8 –ê–£–î–ò–û (—Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º processor)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "from src.data import DataManager, AudioCollator\n",
    "from src.config import load_config\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ DataManager\n",
    "print(\"\\n1Ô∏è‚É£ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è:\")\n",
    "config = load_config(\"../configs/whisper_base.yaml\")\n",
    "print(f\"‚úÖ –ö–æ–Ω—Ñ–∏–≥ –∑–∞–≥—Ä—É–∂–µ–Ω: language={config.data.language}, task={config.data.task}\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º DataManager\n",
    "data_manager = DataManager(config)\n",
    "print(f\"‚úÖ DataManager —Å–æ–∑–¥–∞–Ω\")\n",
    "\n",
    "# –í—ã–∑—ã–≤–∞–µ–º setup_processor() —Å –Ø–í–ù–´–ú–ò –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ language –∏ task\n",
    "data_manager.setup_processor(\n",
    "    model_name=\"openai/whisper-base\",\n",
    "    model_type=\"whisper\",\n",
    "    language=config.data.language,\n",
    "    task=config.data.task\n",
    ")\n",
    "print(f\"‚úÖ Processor —Å–æ–∑–¥–∞–Ω —Å language='{config.data.language}', task='{config.data.task}'\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º prefix_tokens\n",
    "processor = data_manager.processor\n",
    "tokenizer = processor.tokenizer\n",
    "if hasattr(tokenizer, 'prefix_tokens'):\n",
    "    print(f\"   prefix_tokens: {tokenizer.prefix_tokens}\")\n",
    "    expected = [50258, 50263, 50359, 50363]\n",
    "    if tokenizer.prefix_tokens == expected:\n",
    "        print(f\"   ‚úÖ –ü—Ä–µ—Ñ–∏–∫—Å —Ç–æ–∫–µ–Ω—ã –ü–†–ê–í–ò–õ–¨–ù–´–ï!\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå –û–®–ò–ë–ö–ê: –æ–∂–∏–¥–∞–ª–æ—Å—å {expected}\")\n",
    "\n",
    "# 2Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "print(\"\\n2Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏:\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(f\"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {device}\")\n",
    "\n",
    "# 3Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ DataLoader\n",
    "print(\"\\n3Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞:\")\n",
    "train_dataset = data_manager.create_dataset('train')\n",
    "print(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω: {len(train_dataset)} samples\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º collator (–ø—Ä–∏–Ω–∏–º–∞–µ—Ç config, model_type, transpose_features)\n",
    "collator = AudioCollator(\n",
    "    config=config.data,\n",
    "    model_type=\"whisper\",\n",
    "    transpose_features=False\n",
    ")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º DataLoader —Å batch_size=8\n",
    "dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–π –±–∞—Ç—á\n",
    "batch = next(iter(dataloader))\n",
    "input_features = batch['input_features'].to(device)\n",
    "labels = batch['labels'].to(device)\n",
    "references = batch['text']\n",
    "\n",
    "print(f\"‚úÖ –ë–∞—Ç—á –∑–∞–≥—Ä—É–∂–µ–Ω\")\n",
    "print(f\"   Batch size: {input_features.shape[0]}\")\n",
    "print(f\"   Input features shape: {input_features.shape}\")\n",
    "print(f\"   Labels shape: {labels.shape}\")\n",
    "\n",
    "# 4Ô∏è‚É£ Forward pass –¥–ª—è –≤—Å–µ–≥–æ –±–∞—Ç—á–∞\n",
    "print(\"\\n4Ô∏è‚É£ Forward pass –¥–ª—è –±–∞—Ç—á–∞:\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_features=input_features, labels=labels)\n",
    "    batch_loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "\n",
    "print(f\"‚úÖ Forward pass –∑–∞–≤–µ—Ä—à—ë–Ω\")\n",
    "print(f\"   –°—Ä–µ–¥–Ω–∏–π loss –ø–æ –±–∞—Ç—á—É (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π): {batch_loss.item():.4f}\")\n",
    "print(f\"   Logits shape: {logits.shape}\")\n",
    "\n",
    "# 5Ô∏è‚É£ –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss –≤—Ä—É—á–Ω—É—é —Ç—Ä–µ–º—è —Å–ø–æ—Å–æ–±–∞–º–∏\n",
    "print(\"\\n5Ô∏è‚É£ –í–´–ß–ò–°–õ–ï–ù–ò–ï LOSS –í–†–£–ß–ù–£–Æ –¢–†–ï–ú–Ø –°–ü–û–°–û–ë–ê–ú–ò:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# –í—ã—á–∏—Å–ª—è–µ–º loss –≤—Ä—É—á–Ω—É—é –¥–ª—è –≤—Å–µ–≥–æ –±–∞—Ç—á–∞\n",
    "def compute_manual_loss(logits, labels, ignore_first_n=0):\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª—è–µ—Ç cross-entropy loss –≤—Ä—É—á–Ω—É—é, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –ø–µ—Ä–≤—ã–µ N —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "    \n",
    "    Args:\n",
    "        logits: (batch_size, seq_len, vocab_size)\n",
    "        labels: (batch_size, seq_len)\n",
    "        ignore_first_n: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è (0, 1, –∏–ª–∏ 4)\n",
    "    \n",
    "    Returns:\n",
    "        average loss (—Å–∫–∞–ª—è—Ä)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, vocab_size = logits.shape\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –ø–µ—Ä–≤—ã—Ö N —Ç–æ–∫–µ–Ω–æ–≤\n",
    "    mask = torch.ones_like(labels, dtype=torch.bool)\n",
    "    if ignore_first_n > 0:\n",
    "        mask[:, :ignore_first_n] = False\n",
    "    \n",
    "    # –¢–∞–∫–∂–µ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º -100 —Ç–æ–∫–µ–Ω—ã\n",
    "    mask = mask & (labels != -100)\n",
    "    \n",
    "    # Flatten –ª–æ–≥–∏—Ç—ã –∏ labels\n",
    "    logits_flat = logits.view(-1, vocab_size)  # (batch*seq_len, vocab_size)\n",
    "    labels_flat = labels.view(-1)  # (batch*seq_len,)\n",
    "    mask_flat = mask.view(-1)  # (batch*seq_len,)\n",
    "    \n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º cross-entropy –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏\n",
    "    loss_per_token = F.cross_entropy(logits_flat, labels_flat, reduction='none')  # (batch*seq_len,)\n",
    "    \n",
    "    # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É –∏ –≤—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ\n",
    "    masked_loss = loss_per_token * mask_flat.float()\n",
    "    total_loss = masked_loss.sum()\n",
    "    num_tokens = mask_flat.sum()\n",
    "    \n",
    "    if num_tokens > 0:\n",
    "        avg_loss = total_loss / num_tokens\n",
    "    else:\n",
    "        avg_loss = torch.tensor(0.0)\n",
    "    \n",
    "    return avg_loss.item(), num_tokens.item()\n",
    "\n",
    "# –í–∞—Ä–∏–∞–Ω—Ç 1: –° —É—á–µ—Ç–æ–º –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤ (–≤–∫–ª—é—á–∞—è –ø–µ—Ä–≤—ã–µ 4)\n",
    "loss_with_all, tokens_with_all = compute_manual_loss(logits, labels, ignore_first_n=0)\n",
    "print(f\"\\n1Ô∏è‚É£ Loss –° –£–ß–ï–¢–û–ú –ø–µ—Ä–≤—ã—Ö 4 —Ç–æ–∫–µ–Ω–æ–≤:\")\n",
    "print(f\"   Loss: {loss_with_all:.4f}\")\n",
    "print(f\"   –¢–æ–∫–µ–Ω–æ–≤ —É—á—Ç–µ–Ω–æ: {tokens_with_all}\")\n",
    "\n",
    "# –í–∞—Ä–∏–∞–Ω—Ç 2: –ë–ï–ó —É—á–µ—Ç–∞ –ø–µ—Ä–≤—ã—Ö 4 —Ç–æ–∫–µ–Ω–æ–≤\n",
    "loss_without_4, tokens_without_4 = compute_manual_loss(logits, labels, ignore_first_n=4)\n",
    "print(f\"\\n2Ô∏è‚É£ Loss –ë–ï–ó –£–ß–ï–¢–ê –ø–µ—Ä–≤—ã—Ö 4 —Ç–æ–∫–µ–Ω–æ–≤:\")\n",
    "print(f\"   Loss: {loss_without_4:.4f}\")\n",
    "print(f\"   –¢–æ–∫–µ–Ω–æ–≤ —É—á—Ç–µ–Ω–æ: {tokens_without_4}\")\n",
    "print(f\"   –†–∞–∑–Ω–∏—Ü–∞ —Å –≤–∞—Ä–∏–∞–Ω—Ç–æ–º 1: {loss_with_all - loss_without_4:.4f}\")\n",
    "\n",
    "# –í–∞—Ä–∏–∞–Ω—Ç 3: –ë–ï–ó —É—á–µ—Ç–∞ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "loss_without_1, tokens_without_1 = compute_manual_loss(logits, labels, ignore_first_n=1)\n",
    "print(f\"\\n3Ô∏è‚É£ Loss –ë–ï–ó –£–ß–ï–¢–ê —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞:\")\n",
    "print(f\"   Loss: {loss_without_1:.4f}\")\n",
    "print(f\"   –¢–æ–∫–µ–Ω–æ–≤ —É—á—Ç–µ–Ω–æ: {tokens_without_1}\")\n",
    "print(f\"   –†–∞–∑–Ω–∏—Ü–∞ —Å –≤–∞—Ä–∏–∞–Ω—Ç–æ–º 1: {loss_with_all - loss_without_1:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä –°–†–ê–í–ù–ï–ù–ò–ï:\")\n",
    "print(f\"   –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π loss (PyTorch): {batch_loss.item():.4f}\")\n",
    "print(f\"   –†—É—á–Ω–æ–π loss (–≤—Å–µ —Ç–æ–∫–µ–Ω—ã):      {loss_with_all:.4f}\")\n",
    "print(f\"   –†—É—á–Ω–æ–π loss (–±–µ–∑ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤): {loss_without_4:.4f}\")\n",
    "print(f\"   –†—É—á–Ω–æ–π loss (–±–µ–∑ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞):  {loss_without_1:.4f}\")\n",
    "\n",
    "# 6Ô∏è‚É£ –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –∫–∞–∂–¥–æ–º—É –ø—Ä–∏–º–µ—Ä—É\n",
    "print(\"\\n6Ô∏è‚É£ –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ö–ê–ñ–î–û–ì–û –ü–†–ò–ú–ï–†–ê:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx in range(input_features.shape[0]):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"–ü–†–ò–ú–ï–† {idx + 1}/8\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞\n",
    "    example_input = input_features[idx:idx+1]\n",
    "    example_labels = labels[idx:idx+1]\n",
    "    example_reference = references[idx]\n",
    "    example_logits = logits[idx:idx+1]\n",
    "    \n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º loss –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞\n",
    "    with torch.no_grad():\n",
    "        example_output = model(input_features=example_input, labels=example_labels)\n",
    "        example_loss_auto = example_output.loss\n",
    "    \n",
    "    # Loss –≤—Ä—É—á–Ω—É—é —Ç—Ä–µ–º—è —Å–ø–æ—Å–æ–±–∞–º–∏ –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞\n",
    "    loss1, tok1 = compute_manual_loss(example_logits, example_labels, ignore_first_n=0)\n",
    "    loss2, tok2 = compute_manual_loss(example_logits, example_labels, ignore_first_n=4)\n",
    "    loss3, tok3 = compute_manual_loss(example_logits, example_labels, ignore_first_n=1)\n",
    "    \n",
    "    print(f\"\\nüí∞ LOSS:\")\n",
    "    print(f\"   –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π:           {example_loss_auto.item():.4f}\")\n",
    "    print(f\"   –†—É—á–Ω–æ–π (–≤—Å–µ —Ç–æ–∫–µ–Ω—ã):      {loss1:.4f}\")\n",
    "    print(f\"   –†—É—á–Ω–æ–π (–±–µ–∑ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤): {loss2:.4f} (—Ç–æ–∫–µ–Ω–æ–≤: {tok2})\")\n",
    "    print(f\"   –†—É—á–Ω–æ–π (–±–µ–∑ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞):  {loss3:.4f} (—Ç–æ–∫–µ–Ω–æ–≤: {tok3})\")\n",
    "    print(f\"   –í–∫–ª–∞–¥ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤:        {loss1 - loss2:.4f}\")\n",
    "    print(f\"   –í–∫–ª–∞–¥ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞:         {loss1 - loss3:.4f}\")\n",
    "    \n",
    "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (argmax)\n",
    "    predicted_token_ids = torch.argmax(example_logits.squeeze(0), dim=-1)\n",
    "    predicted_tokens = predicted_token_ids.cpu().tolist()\n",
    "    \n",
    "    # Labels (—É–±–∏—Ä–∞–µ–º -100)\n",
    "    labels_list = example_labels.squeeze(0).cpu().tolist()\n",
    "    labels_clean = [t for t in labels_list if t != -100]\n",
    "\n",
    "    print(f\"\\nüìù –õ–ï–ô–ë–õ–´:\")\n",
    "    print(f\"   Reference:  {example_reference}\")\n",
    "    print(f\"   Labels:     {labels_clean}\")\n",
    "    print(f\"   Predicted:  {predicted_tokens}\")\n",
    "\n",
    "    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –≤ —Ç–µ–∫—Å—Ç\n",
    "    predicted_text = tokenizer.decode(predicted_tokens, skip_special_tokens=True)\n",
    "    labels_text = tokenizer.decode(labels_clean, skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nüìù –¢–ï–ö–°–¢–´:\")\n",
    "    print(f\"   Reference:  {example_reference}\")\n",
    "    print(f\"   Labels:     {labels_text}\")\n",
    "    print(f\"   Predicted:  {predicted_text}\")\n",
    "    \n",
    "    # Loss –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å —Ç–æ–∫–µ–Ω–∞–º (–ø–µ—Ä–≤—ã–µ 4)\n",
    "    print(f\"\\nüí° LOSS –ü–û –ü–†–ï–§–ò–ö–° –¢–û–ö–ï–ù–ê–ú (–ø–µ—Ä–≤—ã–µ 4):\")\n",
    "    for i in range(min(4, len(labels_clean))):\n",
    "        token_id = labels_clean[i]\n",
    "        token_logits = example_logits.squeeze(0)[i]\n",
    "        \n",
    "        probs = F.softmax(token_logits, dim=0)\n",
    "        correct_prob = probs[token_id].item()\n",
    "        token_loss = -torch.log(probs[token_id]).item()\n",
    "        \n",
    "        token_text = tokenizer.decode([token_id])\n",
    "        print(f\"   –¢–æ–∫–µ–Ω {i}: id={token_id:5d} '{token_text:20s}' | P={correct_prob:.6f} | loss={token_loss:.4f}\")\n",
    "\n",
    "# 7Ô∏è‚É£ –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–Å–ù!\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(\"\\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ë–ê–¢–ß–£:\")\n",
    "print(f\"   –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π loss:           {batch_loss.item():.4f}\")\n",
    "print(f\"   –†—É—á–Ω–æ–π loss (–≤—Å–µ —Ç–æ–∫–µ–Ω—ã):      {loss_with_all:.4f}\")\n",
    "print(f\"   –†—É—á–Ω–æ–π loss (–±–µ–∑ 4 –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤): {loss_without_4:.4f}\")\n",
    "print(f\"   –†—É—á–Ω–æ–π loss (–±–µ–∑ 1 –ø—Ä–µ—Ñ–∏–∫—Å–∞):  {loss_without_1:.4f}\")\n",
    "print(f\"\\n   –í–∫–ª–∞–¥ 4 –ø—Ä–µ—Ñ–∏–∫—Å —Ç–æ–∫–µ–Ω–æ–≤ –≤ loss: {loss_with_all - loss_without_4:.4f}\")\n",
    "print(f\"   –í–∫–ª–∞–¥ 1 –ø—Ä–µ—Ñ–∏–∫—Å —Ç–æ–∫–µ–Ω–∞ –≤ loss:  {loss_with_all - loss_without_1:.4f}\")\n",
    "print(f\"\\n   –û–∂–∏–¥–∞–µ–º—ã–π loss (–ø–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è): ~3-4\")\n",
    "print(f\"   –°—Ç–∞—Ä—ã–π loss (—Å –±–∞–≥–æ–º): >12.0\")\n",
    "\n",
    "if batch_loss.item() < 5.0:\n",
    "    print(\"\\n‚úÖ‚úÖ‚úÖ LOSS –ó–ù–ê–ß–ò–¢–ï–õ–¨–ù–û –°–ù–ò–ó–ò–õ–°–Ø!\")\n",
    "    print(\"     –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±–∞–≥–∞ —Å prefix_tokens –°–†–ê–ë–û–¢–ê–õ–û!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Loss –≤—Å—ë –µ—â—ë –≤—ã—Å–æ–∫–∏–π, –≤–æ–∑–º–æ–∂–Ω–æ –µ—Å—Ç—å –¥—Ä—É–≥–∏–µ –ø—Ä–æ–±–ª–µ–º—ã\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8wt2brjhlp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ FORWARD PASS –° –ú–û–î–ï–õ–¨–Æ –ò–ó CHECKPOINT (–∑–∞–≥—Ä—É–∑–∫–∞ –∫–∞–∫ –≤ train.py)\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è:\n",
      "‚úÖ –ö–æ–Ω—Ñ–∏–≥ –∑–∞–≥—Ä—É–∂–µ–Ω: language=ru, task=transcribe\n",
      "\n",
      "2Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ ModelManager.load_checkpoint():\n",
      "Device: cuda\n",
      "‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ ../experiments/baselines/whisper-base\n",
      "   Model name: openai/whisper-base\n",
      "   Epoch: None\n",
      "\n",
      "üîç Processor prefix_tokens: [50258, 50263, 50359, 50363]\n",
      "   ‚úÖ –ü—Ä–µ—Ñ–∏–∫—Å —Ç–æ–∫–µ–Ω—ã –ü–†–ê–í–ò–õ–¨–ù–´–ï!\n",
      "\n",
      "3Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞:\n",
      "‚úÖ DataManager —Å–æ–∑–¥–∞–Ω —Å processor –æ—Ç –º–æ–¥–µ–ª–∏\n",
      "‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω: 26654 samples\n",
      "   –î–∞—Ç–∞—Å–µ—Ç prefix_tokens: [50258, 50263, 50359, 50363]\n",
      "‚úÖ –ë–∞—Ç—á –∑–∞–≥—Ä—É–∂–µ–Ω\n",
      "   Batch size: 8\n",
      "   Input features shape: torch.Size([8, 80, 3000])\n",
      "   Labels shape: torch.Size([8, 27])\n",
      "\n",
      "4Ô∏è‚É£ Forward pass –¥–ª—è –±–∞—Ç—á–∞:\n",
      "‚úÖ Forward pass –∑–∞–≤–µ—Ä—à—ë–Ω\n",
      "   –°—Ä–µ–¥–Ω–∏–π loss –ø–æ –±–∞—Ç—á—É: 3.2272\n",
      "   Logits shape: torch.Size([8, 27, 51865])\n",
      "\n",
      "5Ô∏è‚É£ –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–ï–†–í–´–• 3 –ü–†–ò–ú–ï–†–û–í:\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "–ü–†–ò–ú–ï–† 1/8\n",
      "================================================================================\n",
      "\n",
      "üîç PREFIX TOKENS:\n",
      "First 4 tokens in labels: [50258, 50263, 50359, 50363]\n",
      "‚úÖ Labels contain CORRECT 4 prefix tokens\n",
      "\n",
      "üìä TOKEN COUNTS:\n",
      "Labels tokens:    17\n",
      "Predicted tokens: 27\n",
      "\n",
      "üìù TEXTS:\n",
      "Reference:  –û–Ω–∞ —É–≤–∏–¥–∞–ª–∞ –º—É–∂–∞ –µ—â–µ –∏–∑–¥–∞–ª–µ–∫–∞.\n",
      "Labels:     –û–Ω–∞ —É–≤–∏–¥–∞–ª–∞ –º—É–∂–∞ –µ—â–µ –∏–∑–¥–∞–ª–µ–∫–∞.\n",
      "Predicted:   –û–Ω–∞–Ω —É–≤–∏–¥–µ–ª–∞ –º—É–∂–∞ –µ—â–µ –∏–∑–¥–∞–ª–µ–∫–∞.\n",
      "\n",
      "üí° LOSS PER TOKEN (first 10):\n",
      "  Token 0: id=50258 '<|startoftranscript|>' | P=0.000000 | loss=20.3349\n",
      "  Token 1: id=50263 '<|ru|>              ' | P=0.887056 | loss=0.1198\n",
      "  Token 2: id=50359 '<|transcribe|>      ' | P=0.565096 | loss=0.5708\n",
      "  Token 3: id=50363 '<|notimestamps|>    ' | P=0.991331 | loss=0.0087\n",
      "  Token 4: id= 7876 '–û                   ' | P=0.001775 | loss=6.3341\n",
      "  Token 5: id= 1931 '–Ω–∞                  ' | P=0.071013 | loss=2.6449\n",
      "  Token 6: id=21974 ' —É–≤–∏–¥               ' | P=0.953422 | loss=0.0477\n",
      "  Token 7: id= 6232 '–∞–ª–∞                 ' | P=0.002639 | loss=5.9372\n",
      "  Token 8: id=22081 ' –º—É–∂                ' | P=0.805910 | loss=0.2158\n",
      "  Token 9: id=  386 '–∞                   ' | P=0.891163 | loss=0.1152\n",
      "\n",
      "================================================================================\n",
      "–ü–†–ò–ú–ï–† 2/8\n",
      "================================================================================\n",
      "\n",
      "üîç PREFIX TOKENS:\n",
      "First 4 tokens in labels: [50258, 50263, 50359, 50363]\n",
      "‚úÖ Labels contain CORRECT 4 prefix tokens\n",
      "\n",
      "üìä TOKEN COUNTS:\n",
      "Labels tokens:    15\n",
      "Predicted tokens: 27\n",
      "\n",
      "üìù TEXTS:\n",
      "Reference:  –ù–æ –ö–∏—Ç–∏ –Ω–µ —Å–ª—É—à–∞–ª–∞ –µ–µ —Å–ª–æ–≤.\n",
      "Labels:     –ù–æ –ö–∏—Ç–∏ –Ω–µ —Å–ª—É—à–∞–ª–∞ –µ–µ —Å–ª–æ–≤.\n",
      "Predicted:   –ò–ù–æ–û —Å–º–∞—é–Ω–æ\n",
      "\n",
      "üí° LOSS PER TOKEN (first 10):\n",
      "  Token 0: id=50258 '<|startoftranscript|>' | P=0.000000 | loss=16.8566\n",
      "  Token 1: id=50263 '<|ru|>              ' | P=0.224783 | loss=1.4926\n",
      "  Token 2: id=50359 '<|transcribe|>      ' | P=0.206708 | loss=1.5764\n",
      "  Token 3: id=50363 '<|notimestamps|>    ' | P=0.786376 | loss=0.2403\n",
      "  Token 4: id= 6371 '–ù                   ' | P=0.000010 | loss=11.5508\n",
      "  Token 5: id=  354 '–æ                   ' | P=0.000241 | loss=8.3287\n",
      "  Token 6: id= 3422 ' –ö                  ' | P=0.001152 | loss=6.7664\n",
      "  Token 7: id=18686 '–∏—Ç–∏                 ' | P=0.000351 | loss=7.9539\n",
      "  Token 8: id= 1725 ' –Ω–µ                 ' | P=0.026992 | loss=3.6122\n",
      "  Token 9: id=41839 ' —Å–ª—É—à               ' | P=0.002930 | loss=5.8329\n",
      "\n",
      "================================================================================\n",
      "–ü–†–ò–ú–ï–† 3/8\n",
      "================================================================================\n",
      "\n",
      "üîç PREFIX TOKENS:\n",
      "First 4 tokens in labels: [50258, 50263, 50359, 50363]\n",
      "‚úÖ Labels contain CORRECT 4 prefix tokens\n",
      "\n",
      "üìä TOKEN COUNTS:\n",
      "Labels tokens:    14\n",
      "Predicted tokens: 27\n",
      "\n",
      "üìù TEXTS:\n",
      "Reference:  –ù—É, –∫–∞–∫ –º–æ–π –ø—Ä–∏—è—Ç–µ–ª—å –æ—Ç–Ω–æ—Å–∏—Ç—Å—è?\n",
      "Labels:     –ù—É, –∫–∞–∫ –º–æ–π –ø—Ä–∏—è—Ç–µ–ª—å –æ—Ç–Ω–æ—Å–∏—Ç—Å—è?\n",
      "Predicted:   –ö–∞–∫ –∫–∞–∫ –∫–∞–∫ –º—ã –ø—Ä–∏—è—Ç –æ—Ç–Ω–æ—Å–∏—Ç—Å—è?\n",
      "\n",
      "üí° LOSS PER TOKEN (first 10):\n",
      "  Token 0: id=50258 '<|startoftranscript|>' | P=0.000000 | loss=23.4818\n",
      "  Token 1: id=50263 '<|ru|>              ' | P=0.983116 | loss=0.0170\n",
      "  Token 2: id=50359 '<|transcribe|>      ' | P=0.439295 | loss=0.8226\n",
      "  Token 3: id=50363 '<|notimestamps|>    ' | P=0.998066 | loss=0.0019\n",
      "  Token 4: id=20058 '–ù—É                  ' | P=0.000031 | loss=10.3793\n",
      "  Token 5: id=   11 ',                   ' | P=0.234271 | loss=1.4513\n",
      "  Token 6: id= 3014 ' –∫–∞–∫                ' | P=0.967422 | loss=0.0331\n",
      "  Token 7: id=23400 ' –º–æ–π                ' | P=0.052962 | loss=2.9382\n",
      "  Token 8: id= 5082 ' –ø—Ä–∏                ' | P=0.936419 | loss=0.0657\n",
      "  Token 9: id=37764 '—è—Ç–µ–ª—å               ' | P=0.066387 | loss=2.7122\n",
      "\n",
      "================================================================================\n",
      "‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–Å–ù!\n",
      "================================================================================\n",
      "\n",
      "üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\n",
      "   Batch loss: 3.2272\n",
      "\n",
      "   ‚ùì –í–û–ü–†–û–°: –ü–æ—á–µ–º—É loss ~11-12, —Ö–æ—Ç—è processor –ü–†–ê–í–ò–õ–¨–ù–´–ô (4 prefix_tokens)?\n",
      "\n",
      "   üí° –í–û–ó–ú–û–ñ–ù–ê–Ø –ü–†–ò–ß–ò–ù–ê:\n",
      "      –ú–æ–¥–µ–ª—å openai/whisper-base –æ–±—É—á–∞–ª–∞—Å—å —Å –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ú –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —è–∑—ã–∫–∞!\n",
      "      –û–Ω–∞ –æ–∂–∏–¥–∞–µ—Ç –°–ê–ú–ê –≤—ã–±—Ä–∞—Ç—å —è–∑—ã–∫, –∞ –º—ã —Ñ–æ—Ä—Å–∏—Ä—É–µ–º <|ru|> –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ 1.\n",
      "      –≠—Ç–æ –≤—ã–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫–∏–π loss –Ω–∞ prefix —Ç–æ–∫–µ–Ω–∞—Ö, –ø–æ—Ç–æ–º—É —á—Ç–æ –º–æ–¥–µ–ª—å\n",
      "      '–Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç', –ø–æ—á–µ–º—É —è–∑—ã–∫ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω.\n",
      "\n",
      "   üéØ –°–õ–ï–î–£–Æ–©–ò–ô –®–ê–ì:\n",
      "      –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã Whisper fine-tuning - –∫–∞–∫–æ–π —Ñ–æ—Ä–º–∞—Ç labels\n",
      "      –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–∞–º? –í–∫–ª—é—á–∞—é—Ç –ª–∏ –æ–Ω–∏ <|ru|> –∏ <|transcribe|> –∏–ª–∏ –Ω–µ—Ç?\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# –ë–õ–û–ö 11: Forward pass —Å –º–æ–¥–µ–ª—å—é, –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π —á–µ—Ä–µ–∑ ModelManager.load_checkpoint() (–∫–∞–∫ –≤ train.py)\n",
    "\n",
    "print(\"\\nüöÄ FORWARD PASS –° –ú–û–î–ï–õ–¨–Æ –ò–ó CHECKPOINT (–∑–∞–≥—Ä—É–∑–∫–∞ –∫–∞–∫ –≤ train.py)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from src.models import ModelManager\n",
    "from src.data import DataManager, AudioCollator\n",
    "from src.config import load_config\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "print(\"\\n1Ô∏è‚É£ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è:\")\n",
    "config = load_config(\"../configs/whisper_base.yaml\")\n",
    "print(f\"‚úÖ –ö–æ–Ω—Ñ–∏–≥ –∑–∞–≥—Ä—É–∂–µ–Ω: language={config.data.language}, task={config.data.task}\")\n",
    "\n",
    "# 2Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ ModelManager (–ö–ê–ö –í TRAIN.PY!)\n",
    "print(\"\\n2Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ ModelManager.load_checkpoint():\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "model_manager = ModelManager()\n",
    "baseline_path = \"../experiments/baselines/whisper-base\"\n",
    "\n",
    "# –í–ê–ñ–ù–û: –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¢–û–ß–ù–û —Ç–∞–∫ –∂–µ, –∫–∞–∫ –≤ train.py\n",
    "model, processor, checkpoint_info = model_manager.load_checkpoint(\n",
    "    baseline_path,\n",
    "    device=device,\n",
    "    processor=None,  # Will be auto-created from checkpoint model_metadata\n",
    "    compile=False,\n",
    "    language=config.data.language,  # Required for Whisper processor creation\n",
    "    task=config.data.task  # Required for Whisper processor creation\n",
    ")\n",
    "model.eval()\n",
    "print(f\"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {baseline_path}\")\n",
    "print(f\"   Model name: {checkpoint_info.get('model_name', 'unknown')}\")\n",
    "print(f\"   Epoch: {checkpoint_info.get('epoch', 'None (baseline)')}\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º prefix_tokens\n",
    "tokenizer = processor.tokenizer\n",
    "if hasattr(tokenizer, 'prefix_tokens'):\n",
    "    print(f\"\\nüîç Processor prefix_tokens: {tokenizer.prefix_tokens}\")\n",
    "    expected = [50258, 50263, 50359, 50363]\n",
    "    if tokenizer.prefix_tokens == expected:\n",
    "        print(f\"   ‚úÖ –ü—Ä–µ—Ñ–∏–∫—Å —Ç–æ–∫–µ–Ω—ã –ü–†–ê–í–ò–õ–¨–ù–´–ï!\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå –û–®–ò–ë–ö–ê: –æ–∂–∏–¥–∞–ª–æ—Å—å {expected}\")\n",
    "        print(f\"   –ü–æ–ª—É—á–µ–Ω–æ: {tokenizer.prefix_tokens}\")\n",
    "else:\n",
    "    print(\"   ‚ùå prefix_tokens –ù–ï –°–£–©–ï–°–¢–í–£–ï–¢!\")\n",
    "\n",
    "# 3Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ DataManager –∏ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "print(\"\\n3Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞:\")\n",
    "data_manager = DataManager(config)\n",
    "data_manager.set_already_loaded_processor(processor)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º processor –æ—Ç –º–æ–¥–µ–ª–∏\n",
    "print(f\"‚úÖ DataManager —Å–æ–∑–¥–∞–Ω —Å processor –æ—Ç –º–æ–¥–µ–ª–∏\")\n",
    "\n",
    "train_dataset = data_manager.create_dataset('train')\n",
    "print(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω: {len(train_dataset)} samples\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º prefix_tokens –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "dataset_tokenizer = train_dataset.tokenizer\n",
    "if hasattr(dataset_tokenizer, 'prefix_tokens'):\n",
    "    print(f\"   –î–∞—Ç–∞—Å–µ—Ç prefix_tokens: {dataset_tokenizer.prefix_tokens}\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º collator –∏ DataLoader\n",
    "collator = AudioCollator(\n",
    "    config=config.data,\n",
    "    model_type=\"whisper\",\n",
    "    transpose_features=False\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–π –±–∞—Ç—á\n",
    "batch = next(iter(dataloader))\n",
    "input_features = batch['input_features'].to(device)\n",
    "labels = batch['labels'].to(device)\n",
    "references = batch['text']\n",
    "\n",
    "print(f\"‚úÖ –ë–∞—Ç—á –∑–∞–≥—Ä—É–∂–µ–Ω\")\n",
    "print(f\"   Batch size: {input_features.shape[0]}\")\n",
    "print(f\"   Input features shape: {input_features.shape}\")\n",
    "print(f\"   Labels shape: {labels.shape}\")\n",
    "\n",
    "# 4Ô∏è‚É£ Forward pass –¥–ª—è –≤—Å–µ–≥–æ –±–∞—Ç—á–∞\n",
    "print(\"\\n4Ô∏è‚É£ Forward pass –¥–ª—è –±–∞—Ç—á–∞:\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_features=input_features, labels=labels)\n",
    "    batch_loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "\n",
    "print(f\"‚úÖ Forward pass –∑–∞–≤–µ—Ä—à—ë–Ω\")\n",
    "print(f\"   –°—Ä–µ–¥–Ω–∏–π loss –ø–æ –±–∞—Ç—á—É: {batch_loss.item():.4f}\")\n",
    "print(f\"   Logits shape: {logits.shape}\")\n",
    "\n",
    "# 5Ô∏è‚É£ –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤—ã—Ö 3 –ø—Ä–∏–º–µ—Ä–æ–≤ (–∫–∞–∫ –≤ train.py DEBUG)\n",
    "print(\"\\n5Ô∏è‚É£ –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–ï–†–í–´–• 3 –ü–†–ò–ú–ï–†–û–í:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx in range(min(3, input_features.shape[0])):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"–ü–†–ò–ú–ï–† {idx + 1}/{input_features.shape[0]}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    example_labels = labels[idx].cpu().tolist()\n",
    "    example_labels_clean = [t for t in example_labels if t != -100]\n",
    "    \n",
    "    # Check prefix tokens\n",
    "    print(f\"\\nüîç PREFIX TOKENS:\")\n",
    "    print(f\"First 4 tokens in labels: {example_labels_clean[:4]}\")\n",
    "    expected_prefix = [50258, 50263, 50359, 50363]\n",
    "    if example_labels_clean[:4] == expected_prefix:\n",
    "        print(\"‚úÖ Labels contain CORRECT 4 prefix tokens\")\n",
    "    else:\n",
    "        print(f\"‚ùå WRONG prefix tokens!\")\n",
    "        print(f\"   Expected: {expected_prefix}\")\n",
    "        print(f\"   Got:      {example_labels_clean[:4]}\")\n",
    "    \n",
    "    # Predicted tokens (argmax)\n",
    "    example_logits = logits[idx]\n",
    "    predicted_token_ids = torch.argmax(example_logits, dim=-1).cpu().tolist()\n",
    "    \n",
    "    print(f\"\\nüìä TOKEN COUNTS:\")\n",
    "    print(f\"Labels tokens:    {len(example_labels_clean)}\")\n",
    "    print(f\"Predicted tokens: {len(predicted_token_ids)}\")\n",
    "    \n",
    "    # Decode to text\n",
    "    reference_text = references[idx]\n",
    "    labels_text = tokenizer.decode(example_labels_clean, skip_special_tokens=True)\n",
    "    predicted_text = tokenizer.decode(predicted_token_ids, skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nüìù TEXTS:\")\n",
    "    print(f\"Reference:  {reference_text}\")\n",
    "    print(f\"Labels:     {labels_text}\")\n",
    "    print(f\"Predicted:  {predicted_text}\")\n",
    "    \n",
    "    # Loss by first 10 tokens\n",
    "    print(f\"\\nüí° LOSS PER TOKEN (first 10):\")\n",
    "    for i in range(min(10, len(example_labels_clean))):\n",
    "        token_id = example_labels_clean[i]\n",
    "        if i < len(example_logits):\n",
    "            token_logits = example_logits[i]\n",
    "            probs = F.softmax(token_logits, dim=0)\n",
    "            correct_prob = probs[token_id].item()\n",
    "            token_loss = -torch.log(probs[token_id]).item()\n",
    "            token_text = tokenizer.decode([token_id])\n",
    "            print(f\"  Token {i}: id={token_id:5d} '{token_text:20s}' | P={correct_prob:.6f} | loss={token_loss:.4f}\")\n",
    "\n",
    "# 6Ô∏è‚É£ –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–Å–ù!\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\")\n",
    "print(f\"   Batch loss: {batch_loss.item():.4f}\")\n",
    "print(f\"\\n   ‚ùì –í–û–ü–†–û–°: –ü–æ—á–µ–º—É loss ~11-12, —Ö–æ—Ç—è processor –ü–†–ê–í–ò–õ–¨–ù–´–ô (4 prefix_tokens)?\")\n",
    "print(f\"\\n   üí° –í–û–ó–ú–û–ñ–ù–ê–Ø –ü–†–ò–ß–ò–ù–ê:\")\n",
    "print(f\"      –ú–æ–¥–µ–ª—å openai/whisper-base –æ–±—É—á–∞–ª–∞—Å—å —Å –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ú –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —è–∑—ã–∫–∞!\")\n",
    "print(f\"      –û–Ω–∞ –æ–∂–∏–¥–∞–µ—Ç –°–ê–ú–ê –≤—ã–±—Ä–∞—Ç—å —è–∑—ã–∫, –∞ –º—ã —Ñ–æ—Ä—Å–∏—Ä—É–µ–º <|ru|> –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ 1.\")\n",
    "print(f\"      –≠—Ç–æ –≤—ã–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫–∏–π loss –Ω–∞ prefix —Ç–æ–∫–µ–Ω–∞—Ö, –ø–æ—Ç–æ–º—É —á—Ç–æ –º–æ–¥–µ–ª—å\")\n",
    "print(f\"      '–Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç', –ø–æ—á–µ–º—É —è–∑—ã–∫ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω.\")\n",
    "print(f\"\\n   üéØ –°–õ–ï–î–£–Æ–©–ò–ô –®–ê–ì:\")\n",
    "print(f\"      –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã Whisper fine-tuning - –∫–∞–∫–æ–π —Ñ–æ—Ä–º–∞—Ç labels\")\n",
    "print(f\"      –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–∞–º? –í–∫–ª—é—á–∞—é—Ç –ª–∏ –æ–Ω–∏ <|ru|> –∏ <|transcribe|> –∏–ª–∏ –Ω–µ—Ç?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf5d371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhisperConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"apply_spec_augment\": false,\n",
      "  \"architectures\": [\n",
      "    \"WhisperForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"dropout\": 0.0,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"max_length\": 448,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"max_target_positions\": 448,\n",
      "  \"median_filter_width\": 7,\n",
      "  \"model_type\": \"whisper\",\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_mel_bins\": 80,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"scale_embedding\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"transformers_version\": \"4.57.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 51865\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aeb1a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech2TextConfig {\n",
      "  \"activation_dropout\": 0.15,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"Speech2TextForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.15,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"conv_channels\": 1024,\n",
      "  \"conv_kernel_sizes\": [\n",
      "    5,\n",
      "    5\n",
      "  ],\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.15,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"input_channels\": 1,\n",
      "  \"input_feat_per_channel\": 80,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_source_positions\": 6000,\n",
      "  \"max_target_positions\": 1024,\n",
      "  \"model_type\": \"speech_to_text\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_conv_layers\": 2,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 10000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Speech2TextForConditionalGeneration\n",
    "\n",
    "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-medium-mustc-multilingual-st\",use_safetensors=True)\n",
    "\n",
    "print(model.config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basenn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
