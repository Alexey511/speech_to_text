# Configuration for Speech2Text Cross-Lingual (baseline and fine-tuning)
# English model + Multilingual tokenizer for Russian
# Use for evaluation: python evaluation.py --model-path experiments/baselines/s2t-cross-lingual
# Use for training: python train.py --model-path experiments/baselines/s2t-cross-lingual

# Project metadata
project_name: "speech-to-text-ru"
description: "Speech2Text Cross-Lingual (English model + Multilingual tokenizer) for Russian"
version: "0.1.0"

# Experiment configuration (common for both training and evaluation)
experiment:
  output_dir: "experiments"
  # experiment_name not specified - will be auto-generated based on model name
  seed: 42

# Data configuration
data:
  language: "ru"
  task: "transcribe"  # Task for STT models (transcribe, translate for Whisper)
  train_split: "train"
  validation_split: "dev"
  test_split: "test"
  sample_rate: 16000
  data_dir: "data"
  dataset_path: "cv-corpus-22.0-2025-06-20/ru"
  num_workers: 4
  preprocessing_num_workers: 8
  normalize: true
  trim_silence: true
  silence_threshold: 0.01

  processing_and_augmentation_device: "cpu"

  # Duration filtering (disabled for eval, enabled for training)
  filter_by_duration: true
  max_duration: 30.0
  min_duration: 0.2

  pin_memory: true

  # Audio augmentation settings (enabled only during training)
  augmentation:
    enabled: true

    # Time-domain augmentations
    add_noise: true
    noise_probability: 0.3
    noise_factor: 0.02

    speed_perturbation: true
    speed_probability: 0.3
    speed_factor_range: [0.8, 1.2]
    speed_num_steps: 20

    pitch_shift: true
    pitch_probability: 0.3
    pitch_shift_range: [-2, 2]

    volume_perturbation: true
    volume_probability: 0.3
    volume_range_db: [-10, 10]

    fade_inout: true
    fade_probability: 0.2
    fade_duration: 0.1

    # Frequency-domain augmentations
    spec_augment: true
    spec_augment_probability: 0.4
    time_mask_max_size: 20
    freq_mask_max_size: 30
    num_time_masks: 2
    num_freq_masks: 2

    # Environmental augmentations
    reverb: true
    reverb_probability: 0.2
    reverb_time: 0.3
    reverb_decay_rate: 0.3
    reverb_impulse_amplitude: 0.1
    reverb_mix_level: 0.3

# Model configuration - Cross-Lingual Transfer Setup
model:
  model_name: "facebook/s2t-small-librispeech-asr"  # English speech encoder
  model_type: "speech2text"

  # Cross-lingual transfer: use multilingual tokenizer
  # Language is specified in data.language (not in model config)
  tokenizer_name_or_path: "facebook/s2t-medium-mustc-multilingual-st"

  # Freezing strategy for fine-tuning
  freeze_feature_encoder: true  # Freeze feature encoder - acoustic features are language-agnostic
  freeze_encoder: true  # Freeze encoder - acoustic features are language-agnostic
  freeze_decoder: false  # Decoder learns language-specific patterns, should be trainable
  unfreeze_last_n_encoder_layers: 0
  unfreeze_last_n_decoder_layers: 0

  # Fine-grained unfreezing for critical components (applied AFTER freeze/unfreeze operations)
  # Critical for cross-lingual transfer: if decoder is frozen, embeddings must be unfrozen
  unfreeze_embed_tokens: false  # Unfreeze decoder token embeddings (critical for new vocabulary)
  unfreeze_embed_positions_decoder: false  # Unfreeze decoder positional embeddings
  unfreeze_lm_head: false  # Unfreeze output projection (lm_head)
  unfreeze_layer_norm_decoder: false  # Unfreeze final decoder layer norm

  # Dropout for training (set to 0.0 for evaluation)
  attention_dropout: 0.15
  activation_dropout: 0.15
  dropout: 0.15
  use_gpu: true
  compile_model: false

# Training configuration
training:
  num_train_epochs: 30
  train_batch_size: 64
  eval_batch_size: 128
  gradient_accumulation_steps: 2
  max_grad_norm: 1.0
  learning_rate: 3e-3
  weight_decay: 0.01
  label_smoothing_factor: 0.0  # Label smoothing for regularization (0.0 = no smoothing)

  # Learning rate scheduler configuration
  scheduler_name: "warmup_plateau_decay"  # Scheduler type: "reduce_on_plateau", "cosine", "onecycle", "linear"

  # ReduceLROnPlateau scheduler settings
  reduce_on_plateau:
    factor: 0.5  # Reduce LR by this factor when plateau detected
    patience: 2  # Wait N epochs without improvement before reducing LR
    min_lr: 1e-7  # Minimum learning rate
    mode: "min"  # Minimize the monitored metric
    threshold: 1e-4  # Threshold for measuring improvement

  # CosineAnnealingLR scheduler settings
  cosine:
    T_max: 10  # Maximum number of iterations (typically num_train_epochs)
    eta_min: 1e-7  # Minimum learning rate
    last_epoch: -1  # The index of last epoch

  # OneCycleLR scheduler settings
  onecycle:
    max_lr: 5e-4  # Upper learning rate boundary
    pct_start: 0.3  # Percentage of cycle spent increasing LR
    anneal_strategy: "cos"  # 'cos' or 'linear'
    div_factor: 25.0  # Initial LR = max_lr / div_factor
    final_div_factor: 1e4  # Final LR = max_lr / final_div_factor

  # LinearLR scheduler settings (warmup + linear decay)
  linear:
    warmup_ratio: 0.1  # Warmup for 10% of total training steps, then linear decay to 0

  # Warmup + Plateau + Linear Decay scheduler settings
  warmup_plateau_decay:
    warmup_ratio: 0.05  # Warmup for 5% of total steps (LR: 0 -> 1)
    plateau_ratio: 0.5  # Plateau ends at 55% of total steps (LR: 1 constant)
    # After plateau_ratio, LR decays linearly from 1 to 0 until end

  max_steps: null
  eval_steps: 500
  save_steps: 1000
  logging_steps: 100
  load_best_model_at_end: true
  metric_for_best_model: "eval_wer"
  greater_is_better: false
  save_total_limit: 3
  fp16: true
  num_workers: 8
  remove_unused_columns: true
  use_cpu_offload: false
  # Early stopping configuration
  use_early_stopping: true
  early_stopping_patience: 3
  early_stopping_threshold: 0.01

# Evaluation configuration
evaluation:
  batch_size: 128
  num_workers: 8

  calculate_wer: true
  calculate_cer: true
  calculate_bleu: true
  num_beams: 1
  max_length: 448
  language: "ru"
  task: "transcribe"
  # Anti-repetition parameters (prevent model from looping/hallucinating)
  repetition_penalty: 1.2
  no_repeat_ngram_size: 3

# Logging configuration
logging:
  use_wandb: false
  wandb_project: "speech-to-text-ru"
  wandb_entity: null
  log_level: "INFO"
