# Configuration for Whisper Small (baseline and fine-tuning)
# Use for evaluation: python evaluation.py --model-path experiments/baselines/whisper-small
# Use for training: python train.py --model-path experiments/baselines/whisper-small

# Project metadata
project_name: "speech-to-text-ru"
description: "Whisper Small for Russian speech-to-text"
version: "0.1.0"

# Experiment configuration (common for both training and evaluation)
experiment:
  output_dir: "experiments"
  # experiment_name not specified - will be auto-generated based on model name
  seed: 42

# Data configuration
data:
  language: "ru"
  task: "transcribe"  # Task for STT models (transcribe, translate for Whisper)
  train_split: "train"
  validation_split: "dev"
  test_split: "test"
  sample_rate: 16000
  data_dir: "data"
  dataset_path: "cv-corpus-22.0-2025-06-20/ru"
  num_workers: 4
  preprocessing_num_workers: 8
  normalize: true
  trim_silence: true
  silence_threshold: 0.01

  processing_and_augmentation_device: "cpu"

  # Duration filtering (disabled for eval, enabled for training)
  filter_by_duration: true
  max_duration: 30.0
  min_duration: 0.2

  pin_memory: true

  # Audio augmentation settings (enabled only during training)
  augmentation:
    enabled: true

    # Time-domain augmentations
    add_noise: true
    noise_probability: 0.3
    noise_factor: 0.02

    speed_perturbation: true
    speed_probability: 1.0
    speed_factor_range: [0.8, 1.2]
    speed_num_steps: 20

    pitch_shift: true
    pitch_probability: 0.3
    pitch_shift_range: [-2, 2]

    volume_perturbation: true
    volume_probability: 0.3
    volume_range_db: [-10, 10]

    fade_inout: true
    fade_probability: 0.2
    fade_duration: 0.1

    # Frequency-domain augmentations
    spec_augment: true
    spec_augment_probability: 0.4
    time_mask_max_size: 20
    freq_mask_max_size: 30
    num_time_masks: 2
    num_freq_masks: 2

    # Environmental augmentations
    reverb: true
    reverb_probability: 0.2
    reverb_time: 0.3
    reverb_decay_rate: 0.3
    reverb_impulse_amplitude: 0.1
    reverb_mix_level: 0.3

# Model configuration
model:
  model_name: "openai/whisper-small"
  model_type: "whisper"
  # Freezing strategy for fine-tuning
  freeze_feature_encoder: true  # Freeze feature encoder - acoustic features are language-agnostic
  freeze_encoder: true  # Freeze encoder - acoustic features are language-agnostic
  freeze_decoder: false  # Decoder learns language-specific patterns, should be trainable
  unfreeze_last_n_encoder_layers: 0
  unfreeze_last_n_decoder_layers: 0

  # Fine-grained unfreezing for critical components
  unfreeze_embed_tokens: false
  unfreeze_embed_positions_decoder: false
  unfreeze_lm_head: false
  unfreeze_layer_norm_decoder: false

  # Dropout parameters (standard HuggingFace names) - Whisper defaults to 0.0
  activation_dropout: 0.0  # Dropout for activation functions
  attention_dropout: 0.0  # Dropout for attention weights
  dropout: 0.0  # General dropout rate
  use_gpu: true
  compile_model: false

# Training configuration
training:
  num_train_epochs: 5
  train_batch_size: 16
  eval_batch_size: 48
  gradient_accumulation_steps: 2
  max_grad_norm: 1.0
  learning_rate: 1e-5
  weight_decay: 0.01
  label_smoothing_factor: 0.0  # Label smoothing for regularization (0.0 = no smoothing)

  # Learning rate scheduler configuration
  scheduler_name: "warmup_plateau_decay"  # Scheduler type: "reduce_on_plateau", "cosine", "onecycle", "linear"

  # ReduceLROnPlateau scheduler settings
  reduce_on_plateau:
    factor: 0.5  # Reduce LR by this factor when plateau detected
    patience: 2  # Wait N epochs without improvement before reducing LR
    min_lr: 1e-7  # Minimum learning rate
    mode: "min"  # Minimize the monitored metric
    threshold: 1e-4  # Threshold for measuring improvement

  # CosineAnnealingLR scheduler settings
  cosine:
    T_max: 10  # Maximum number of iterations (typically num_train_epochs)
    eta_min: 1e-7  # Minimum learning rate
    last_epoch: -1  # The index of last epoch

  # OneCycleLR scheduler settings
  onecycle:
    max_lr: 1e-3  # Upper learning rate boundary
    pct_start: 0.3  # Percentage of cycle spent increasing LR
    anneal_strategy: "cos"  # 'cos' or 'linear'
    div_factor: 25.0  # Initial LR = max_lr / div_factor
    final_div_factor: 1e4  # Final LR = max_lr / final_div_factor

  # LinearLR scheduler settings (warmup + linear decay)
  linear:
    warmup_ratio: 0.1  # Warmup for 10% of total training steps, then linear decay to 0

  # Warmup + Plateau + Linear Decay scheduler settings
  warmup_plateau_decay:
    warmup_ratio: 0.1  # Warmup for 10% of total steps (LR: 0 -> 1)
    plateau_ratio: 0.3  # Plateau ends at 50% of total steps (LR: 1 constant)
    # After plateau_ratio, LR decays linearly from 1 to 0 until end

  max_steps: null
  eval_steps: 500
  save_steps: 1000
  logging_steps: 100
  load_best_model_at_end: true
  metric_for_best_model: "eval_wer"
  greater_is_better: false
  save_total_limit: 3
  fp16: true
  num_workers: 4
  remove_unused_columns: true
  use_cpu_offload: false
  # Early stopping configuration
  use_early_stopping: true
  early_stopping_patience: 3
  early_stopping_threshold: 0.01

# Evaluation configuration
evaluation:
  batch_size: 48
  num_workers: 4

  calculate_wer: true
  calculate_cer: true
  calculate_bleu: true
  num_beams: 1
  max_length: 448
  language: "ru"
  task: "transcribe"
  # Anti-repetition parameters (prevent model from looping/hallucinating)
  repetition_penalty: 1.2
  no_repeat_ngram_size: 3

# Logging configuration
logging:
  use_wandb: false
  wandb_project: "speech-to-text-ru"
  wandb_entity: null
  log_level: "INFO"
