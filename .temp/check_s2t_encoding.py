"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –∏ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤ Speech2Text –º–æ–¥–µ–ª–∏
"""
import sys
sys.path.append('..')

from transformers import Speech2TextProcessor, Speech2TextTokenizer, Speech2TextFeatureExtractor

print("="*80)
print("–ü–†–û–í–ï–†–ö–ê SPEECH2TEXT –ú–û–î–ï–õ–ò")
print("="*80)

# 1. –ú–æ–Ω–æ–ª–∏–Ω–≥–≤–∞–ª—å–Ω–∞—è –∞–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å (LibriSpeech)
print("\n1Ô∏è‚É£ –ú–û–ù–û–õ–ò–ù–ì–í–ê–õ–¨–ù–ê–Ø –ú–û–î–ï–õ–¨ (facebook/s2t-small-librispeech-asr)")
print("-"*80)

model_name = "facebook/s2t-small-librispeech-asr"
processor = Speech2TextProcessor.from_pretrained(model_name)
tokenizer = processor.tokenizer

print(f"Tokenizer type: {type(tokenizer)}")
print(f"\nüìã –û—Å–Ω–æ–≤–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã:")
print(f"  vocab_size: {tokenizer.vocab_size}")
print(f"  bos_token: {tokenizer.bos_token} (id={tokenizer.bos_token_id})")
print(f"  eos_token: {tokenizer.eos_token} (id={tokenizer.eos_token_id})")
print(f"  pad_token: {tokenizer.pad_token} (id={tokenizer.pad_token_id})")
print(f"  unk_token: {tokenizer.unk_token} (id={tokenizer.unk_token_id})")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
lang_code_to_id = getattr(tokenizer, 'lang_code_to_id', {})
print(f"\nüåê –Ø–∑—ã–∫–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã:")
print(f"  lang_code_to_id: {lang_code_to_id}")
print(f"  –¢–∏–ø: {'MULTILINGUAL' if lang_code_to_id else 'MONOLINGUAL'}")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º prefix_tokens
prefix_tokens = getattr(tokenizer, 'prefix_tokens', None)
print(f"\nüéØ –ü—Ä–µ—Ñ–∏–∫—Å —Ç–æ–∫–µ–Ω—ã:")
print(f"  prefix_tokens: {prefix_tokens}")

# –ü—Ä–æ–±—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç
test_text = "–û–Ω–∞ —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–∞—è –∂–µ–Ω—â–∏–Ω–∞."
tokens = tokenizer(test_text, return_tensors="pt").input_ids[0]
print(f"\nüß™ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:")
print(f"  –¢–µ–∫—Å—Ç: {test_text}")
print(f"  –¢–æ–∫–µ–Ω—ã: {tokens.tolist()}")
print(f"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(tokens)}")
print(f"  –ü–µ—Ä–≤—ã–µ 5 —Ç–æ–∫–µ–Ω–æ–≤: {tokens[:5].tolist()}")

# –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ
decoded = tokenizer.decode(tokens, skip_special_tokens=False)
decoded_clean = tokenizer.decode(tokens, skip_special_tokens=True)
print(f"  –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ (—Å —Å–ø–µ—Ü —Ç–æ–∫–µ–Ω–∞–º–∏): {repr(decoded)}")
print(f"  –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ (–±–µ–∑ —Å–ø–µ—Ü —Ç–æ–∫–µ–Ω–æ–≤): {repr(decoded_clean)}")

# 2. –ú—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (MuST-C)
print("\n\n2Ô∏è‚É£ –ú–£–õ–¨–¢–ò–õ–ò–ù–ì–í–ê–õ–¨–ù–ê–Ø –ú–û–î–ï–õ–¨ (facebook/s2t-medium-mustc-multilingual-st)")
print("-"*80)

model_name_multi = "facebook/s2t-medium-mustc-multilingual-st"
processor_multi = Speech2TextProcessor.from_pretrained(model_name_multi)
tokenizer_multi = processor_multi.tokenizer

print(f"Tokenizer type: {type(tokenizer_multi)}")
print(f"\nüìã –û—Å–Ω–æ–≤–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã:")
print(f"  vocab_size: {tokenizer_multi.vocab_size}")
print(f"  bos_token: {tokenizer_multi.bos_token} (id={tokenizer_multi.bos_token_id})")
print(f"  eos_token: {tokenizer_multi.eos_token} (id={tokenizer_multi.eos_token_id})")
print(f"  pad_token: {tokenizer_multi.pad_token} (id={tokenizer_multi.pad_token_id})")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —è–∑—ã–∫–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã
lang_code_to_id_multi = getattr(tokenizer_multi, 'lang_code_to_id', {})
print(f"\nüåê –Ø–∑—ã–∫–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã:")
print(f"  lang_code_to_id: {lang_code_to_id_multi}")
print(f"  –î–æ—Å—Ç—É–ø–Ω—ã–µ —è–∑—ã–∫–∏: {list(lang_code_to_id_multi.keys()) if lang_code_to_id_multi else 'None'}")
print(f"  –¢–∏–ø: {'MULTILINGUAL' if lang_code_to_id_multi else 'MONOLINGUAL'}")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ä—É—Å—Å–∫–∏–π
if lang_code_to_id_multi and 'ru' in lang_code_to_id_multi:
    print(f"  ‚úÖ –†—É—Å—Å–∫–∏–π —è–∑—ã–∫ –î–û–°–¢–£–ü–ï–ù: ru -> {lang_code_to_id_multi['ru']}")
else:
    print(f"  ‚ùå –†—É—Å—Å–∫–∏–π —è–∑—ã–∫ –ù–ï –î–û–°–¢–£–ü–ï–ù")

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º target language –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
if lang_code_to_id_multi and 'ru' in lang_code_to_id_multi:
    tokenizer_multi.tgt_lang = 'ru'
    print(f"\n  Set tgt_lang = 'ru'")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º prefix_tokens
prefix_tokens_multi = getattr(tokenizer_multi, 'prefix_tokens', None)
print(f"\nüéØ –ü—Ä–µ—Ñ–∏–∫—Å —Ç–æ–∫–µ–Ω—ã:")
print(f"  prefix_tokens: {prefix_tokens_multi}")

# –ü—Ä–æ–±—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å
tokens_multi = tokenizer_multi(test_text, return_tensors="pt").input_ids[0]
print(f"\nüß™ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:")
print(f"  –¢–µ–∫—Å—Ç: {test_text}")
print(f"  –¢–æ–∫–µ–Ω—ã: {tokens_multi.tolist()}")
print(f"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(tokens_multi)}")
print(f"  –ü–µ—Ä–≤—ã–µ 5 —Ç–æ–∫–µ–Ω–æ–≤: {tokens_multi[:5].tolist()}")

# –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ
decoded_multi = tokenizer_multi.decode(tokens_multi, skip_special_tokens=False)
decoded_clean_multi = tokenizer_multi.decode(tokens_multi, skip_special_tokens=True)
print(f"  –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ (—Å —Å–ø–µ—Ü —Ç–æ–∫–µ–Ω–∞–º–∏): {repr(decoded_multi)}")
print(f"  –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ (–±–µ–∑ —Å–ø–µ—Ü —Ç–æ–∫–µ–Ω–æ–≤): {repr(decoded_clean_multi)}")

print("\n" + "="*80)
print("–í–´–í–û–î–´:")
print("="*80)

print("\nüìù –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ Speech2Text:")
print("  1. –ú–æ–Ω–æ–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (LibriSpeech):")
print("     - –¢–æ–ª—å–∫–æ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫")
print("     - –ù–µ—Ç —è–∑—ã–∫–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ (lang_code_to_id –ø—É—Å—Ç–æ–π)")
print("     - –ù–µ—Ç prefix_tokens")
print("     - –ü—Ä–æ—Å—Ç–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: —Ç–µ–∫—Å—Ç -> —Ç–æ–∫–µ–Ω—ã -> </s>")
print("")
print("  2. –ú—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (MuST-C):")
print("     - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è–∑—ã–∫–æ–≤ —á–µ—Ä–µ–∑ lang_code_to_id")
print("     - –í–æ–∑–º–æ–∂–Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ tgt_lang –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
print("     - forced_bos_token_id –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∑–∞–¥–∞–Ω–∏—è —è–∑—ã–∫–∞")
print("     - –ù–ï–¢ –∞–Ω–∞–ª–æ–≥–∞ prefix_tokens –∫–∞–∫ –≤ Whisper!")

print("\n‚ö†Ô∏è –ö–õ–Æ–ß–ï–í–û–ï –û–¢–õ–ò–ß–ò–ï –û–¢ WHISPER:")
print("  - Whisper: prefix_tokens –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò –≤ labels")
print("  - Speech2Text: —è–∑—ã–∫ –∑–∞–¥–∞—ë—Ç—Å—è —á–µ—Ä–µ–∑ forced_bos_token_id –ø—Ä–∏ –ì–ï–ù–ï–†–ê–¶–ò–ò")
print("  - Speech2Text –ù–ï –¥–æ–±–∞–≤–ª—è–µ—Ç —è–∑—ã–∫–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –≤ labels –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏")
print("  - –î–ª—è –º–æ–Ω–æ–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö S2T –º–æ–¥–µ–ª–µ–π –≤–æ–æ–±—â–µ –Ω–µ—Ç —è–∑—ã–∫–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤")

print("\n‚úÖ –í—ã–≤–æ–¥—ã –æ —Ä–∏—Å–∫–∞—Ö:")
print("  1. –ú–æ–Ω–æ–ª–∏–Ω–≥–≤–∞–ª—å–Ω–∞—è S2T (LibriSpeech): –ë–ï–ó–û–ü–ê–°–ù–ê")
print("     - –ù–µ—Ç prefix_tokens, –Ω–µ—Ç —è–∑—ã–∫–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤")
print("     - –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ–π –ø—Ä–æ–±–ª–µ–º—ã —Å Whisper –ù–ï –ë–£–î–ï–¢")
print("")
print("  2. –ú—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–∞—è S2T (MuST-C): –û–¢–ù–û–°–ò–¢–ï–õ–¨–ù–û –ë–ï–ó–û–ü–ê–°–ù–ê")
print("     - –Ø–∑—ã–∫ –∑–∞–¥–∞—ë—Ç—Å—è —á–µ—Ä–µ–∑ forced_bos_token_id (—Ç–æ–ª—å–∫–æ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)")
print("     - –í labels —è–∑—ã–∫–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –ù–ï –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è")
print("     - –ù–∞—à –∫–æ–¥ –≤ DataManager.setup_processor() –ø—Ä–∞–≤–∏–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç tgt_lang")

print("\n" + "="*80)
