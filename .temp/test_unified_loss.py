"""Test unified loss calculation logic with label_smoothing=0.0 and label_smoothing=0.05"""
import sys
from pathlib import Path

sys.path.insert(0, str(Path.cwd()))

import torch
from transformers import WhisperForConditionalGeneration

def test_unified_loss_logic():
    """Test that unified loss calculation works for both smoothing=0.0 and smoothing=0.05"""

    print("Testing unified loss calculation logic...")
    print("=" * 60)

    # Load a small Whisper model for testing
    model_name = "openai/whisper-tiny"
    print(f"Loading {model_name}...")

    model = WhisperForConditionalGeneration.from_pretrained(model_name)
    model.eval()  # Set to eval mode for consistent results

    # Create dummy inputs
    batch_size = 2
    seq_len = 10

    input_features = torch.randn(batch_size, 80, 3000)
    vocab_size = model.config.vocab_size
    labels = torch.randint(0, vocab_size, (batch_size, seq_len))
    labels[0, -2:] = -100  # Add padding

    print(f"Input shape: {input_features.shape}")
    print(f"Labels shape: {labels.shape}")
    print()

    # ===== Unified logic (same as in train.py) =====

    def compute_loss_unified(model, input_features, labels, label_smoothing_factor):
        """Unified loss calculation (same logic as in train.py)"""
        with torch.no_grad():
            # Forward pass with labels
            outputs = model(input_features=input_features, labels=labels)

            # Get logits and compute loss manually with CrossEntropyLoss
            logits = outputs.logits

            # Flatten
            vocab_size = logits.size(-1)
            logits_flat = logits.view(-1, vocab_size)
            labels_flat = labels.view(-1)

            # Create loss function with label smoothing
            criterion = torch.nn.CrossEntropyLoss(
                ignore_index=-100,
                label_smoothing=label_smoothing_factor
            )
            loss = criterion(logits_flat, labels_flat)

        return loss

    # Test 1: label_smoothing = 0.0 (no smoothing)
    loss_no_smoothing = compute_loss_unified(model, input_features, labels, 0.0)
    print(f"Unified loss (smoothing=0.0): {loss_no_smoothing.item():.6f}")

    # Compare with built-in loss
    with torch.no_grad():
        outputs_builtin = model(input_features=input_features, labels=labels)
        builtin_loss = outputs_builtin.loss
    print(f"Built-in loss (reference):      {builtin_loss.item():.6f}")

    diff = abs(loss_no_smoothing.item() - builtin_loss.item())
    print(f"Difference:                     {diff:.8f}")
    print()

    if diff < 1e-5:
        print("‚úÖ Test 1 PASSED: Unified loss with smoothing=0.0 matches built-in loss")
    else:
        print("‚ùå Test 1 FAILED: Losses differ!")
        return False

    # Test 2: label_smoothing = 0.05 (light smoothing)
    print()
    loss_smoothing = compute_loss_unified(model, input_features, labels, 0.05)
    print(f"Unified loss (smoothing=0.05): {loss_smoothing.item():.6f}")
    print(f"Difference from no smoothing:  {abs(loss_smoothing.item() - loss_no_smoothing.item()):.6f}")
    print()

    if abs(loss_smoothing.item() - loss_no_smoothing.item()) > 1e-6:
        print("‚úÖ Test 2 PASSED: Label smoothing changes the loss as expected")
    else:
        print("‚ùå Test 2 FAILED: Smoothing has no effect!")
        return False

    # Test 3: Verify padding tokens are ignored
    print()
    labels_all_padding = torch.full_like(labels, -100)
    loss_all_padding = compute_loss_unified(model, input_features, labels_all_padding, 0.0)

    # Loss should be 0 when all labels are padding
    # Actually, CrossEntropyLoss returns 0.0 when all targets are ignored
    print(f"Loss with all padding tokens: {loss_all_padding.item():.6f}")

    if torch.isnan(loss_all_padding) or loss_all_padding.item() == 0.0:
        print("‚úÖ Test 3 PASSED: Padding tokens handled correctly")
    else:
        # Some implementations might return a small non-zero value, that's OK too
        print("‚ö†Ô∏è  Test 3: Padding tokens handled (loss is non-zero but should be ignored in training)")

    return True

if __name__ == "__main__":
    print("Testing unified loss calculation logic (as used in train.py)")
    print()

    success = test_unified_loss_logic()

    print()
    print("=" * 60)
    if success:
        print("üéâ All tests passed! Unified logic works correctly.")
        print("   Safe to use in training with both smoothing=0.0 and smoothing=0.05")
    else:
        print("‚ùå Some tests failed! Need to investigate.")
