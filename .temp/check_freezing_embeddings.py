"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–º–æ—Ä–æ–∑–∫–∏ embeddings –¥–ª—è Whisper –∏ Speech2Text
"""
import sys
from pathlib import Path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.config import ModelConfig
from src.models import WhisperSTT, Speech2TextSTT
from transformers import WhisperProcessor, Speech2TextProcessor

print("="*80)
print("1. WHISPER MODEL - –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏ –∑–∞–º–æ—Ä–æ–∑–∫–∞")
print("="*80)

# –°–æ–∑–¥–∞—ë–º Whisper –º–æ–¥–µ–ª—å
whisper_config = ModelConfig(
    model_name="openai/whisper-tiny",
    model_type="whisper",
    freeze_encoder=False,
    freeze_decoder=False
)

from transformers import WhisperTokenizer, WhisperFeatureExtractor
tokenizer = WhisperTokenizer.from_pretrained("openai/whisper-tiny", language="ru", task="transcribe")
feature_extractor = WhisperFeatureExtractor.from_pretrained("openai/whisper-tiny")
whisper_processor = WhisperProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)

whisper_model = WhisperSTT(whisper_config, processor=whisper_processor)

print("\n–°—Ç—Ä—É–∫—Ç—É—Ä–∞ Whisper –º–æ–¥–µ–ª–∏:")
print("  - model.model.encoder")
print("    ‚îî‚îÄ conv1, conv2 (feature extraction)")
print("    ‚îî‚îÄ embed_positions (positional encoding)")
print("    ‚îî‚îÄ layers[0..N] (transformer layers)")
print("  - model.model.decoder")
print("    ‚îî‚îÄ embed_tokens (token embeddings) ‚ö†Ô∏è")
print("    ‚îî‚îÄ embed_positions (positional encoding)")
print("    ‚îî‚îÄ layers[0..N] (transformer layers)")
print("  - model.proj_out (output projection to vocab) ‚ö†Ô∏è")

# –ü—Ä–æ–≤–µ—Ä–∏–º, –≥–¥–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è embeddings
print("\nüîç –ì–¥–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è embeddings –≤ Whisper:")
print(f"  - Decoder input embeddings: model.model.decoder.embed_tokens")
print(f"    Shape: {whisper_model.model.model.decoder.embed_tokens.weight.shape}")
print(f"  - Output projection: model.proj_out")
print(f"    Shape: {whisper_model.model.proj_out.weight.shape}")

# –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –≤—Å–µ —Å–ª–æ–∏ –æ–±—É—á–∞–µ–º—ã–µ (–¥–æ –∑–∞–º–æ—Ä–æ–∑–∫–∏)
print("\n‚úÖ –î–æ –∑–∞–º–æ—Ä–æ–∑–∫–∏ - –≤—Å–µ —Å–ª–æ–∏ –æ–±—É—á–∞–µ–º—ã–µ:")
encoder_trainable = sum(p.requires_grad for p in whisper_model.model.model.encoder.parameters())
decoder_trainable = sum(p.requires_grad for p in whisper_model.model.model.decoder.parameters())
print(f"  - Encoder trainable params: {encoder_trainable}")
print(f"  - Decoder trainable params: {decoder_trainable}")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º embeddings –¥–æ –∑–∞–º–æ—Ä–æ–∑–∫–∏
embed_tokens_trainable = whisper_model.model.model.decoder.embed_tokens.weight.requires_grad
proj_out_trainable = whisper_model.model.proj_out.weight.requires_grad
print(f"  - embed_tokens.requires_grad: {embed_tokens_trainable}")
print(f"  - proj_out.requires_grad: {proj_out_trainable}")

print("\n" + "-"*80)
print("–ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º ENCODER (freeze_encoder)")
print("-"*80)
whisper_model.freeze_encoder()

encoder_trainable_after = sum(p.requires_grad for p in whisper_model.model.model.encoder.parameters())
decoder_trainable_after = sum(p.requires_grad for p in whisper_model.model.model.decoder.parameters())
print(f"  - Encoder trainable params: {encoder_trainable_after} (–±—ã–ª–æ {encoder_trainable})")
print(f"  - Decoder trainable params: {decoder_trainable_after} (–±—ã–ª–æ {decoder_trainable})")

embed_tokens_trainable_after = whisper_model.model.model.decoder.embed_tokens.weight.requires_grad
proj_out_trainable_after = whisper_model.model.proj_out.weight.requires_grad
print(f"  - embed_tokens.requires_grad: {embed_tokens_trainable_after} ‚úÖ –ù–ï –∑–∞–º–µ—Ä–∑!")
print(f"  - proj_out.requires_grad: {proj_out_trainable_after} ‚úÖ –ù–ï –∑–∞–º–µ—Ä–∑!")

print("\n" + "-"*80)
print("–ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º DECODER (freeze_decoder)")
print("-"*80)
whisper_model.freeze_decoder()

decoder_trainable_after2 = sum(p.requires_grad for p in whisper_model.model.model.decoder.parameters())
print(f"  - Decoder trainable params: {decoder_trainable_after2} (–±—ã–ª–æ {decoder_trainable_after})")

embed_tokens_trainable_after2 = whisper_model.model.model.decoder.embed_tokens.weight.requires_grad
proj_out_trainable_after2 = whisper_model.model.proj_out.weight.requires_grad
print(f"  - embed_tokens.requires_grad: {embed_tokens_trainable_after2} ‚ùå –ó–ê–ú–ï–†–ó!")
print(f"  - proj_out.requires_grad: {proj_out_trainable_after2} ‚úÖ –ù–ï –∑–∞–º–µ—Ä–∑ (–Ω–µ —á–∞—Å—Ç—å decoder)")

print("\n" + "="*80)
print("2. SPEECH2TEXT MODEL - –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏ –∑–∞–º–æ—Ä–æ–∑–∫–∞")
print("="*80)

# –°–æ–∑–¥–∞—ë–º Speech2Text –º–æ–¥–µ–ª—å
s2t_config = ModelConfig(
    model_name="facebook/s2t-small-librispeech-asr",
    model_type="speech2text",
    freeze_encoder=False,
    freeze_decoder=False
)

from transformers import Speech2TextFeatureExtractor, Speech2TextTokenizer
s2t_feature_extractor = Speech2TextFeatureExtractor.from_pretrained("facebook/s2t-small-librispeech-asr")
s2t_tokenizer = Speech2TextTokenizer.from_pretrained("facebook/s2t-small-librispeech-asr")
s2t_processor = Speech2TextProcessor(feature_extractor=s2t_feature_extractor, tokenizer=s2t_tokenizer)

s2t_model = Speech2TextSTT(s2t_config, processor=s2t_processor)

print("\n–°—Ç—Ä—É–∫—Ç—É—Ä–∞ Speech2Text –º–æ–¥–µ–ª–∏:")
print("  - model.model.encoder")
print("    ‚îî‚îÄ conv layers (feature extraction)")
print("    ‚îî‚îÄ embed_positions (positional encoding)")
print("    ‚îî‚îÄ layers[0..N] (transformer layers)")
print("  - model.model.decoder")
print("    ‚îî‚îÄ embed_tokens (token embeddings) ‚ö†Ô∏è")
print("    ‚îî‚îÄ embed_positions (positional encoding)")
print("    ‚îî‚îÄ layers[0..N] (transformer layers)")
print("  - model.lm_head (output projection to vocab) ‚ö†Ô∏è")

# –ü—Ä–æ–≤–µ—Ä–∏–º, –≥–¥–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è embeddings
print("\nüîç –ì–¥–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è embeddings –≤ Speech2Text:")
print(f"  - Decoder input embeddings: model.model.decoder.embed_tokens")
print(f"    Shape: {s2t_model.model.model.decoder.embed_tokens.weight.shape}")
print(f"  - Output projection: model.lm_head")
print(f"    Shape: {s2t_model.model.lm_head.weight.shape}")

# –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –≤—Å–µ —Å–ª–æ–∏ –æ–±—É—á–∞–µ–º—ã–µ (–¥–æ –∑–∞–º–æ—Ä–æ–∑–∫–∏)
print("\n‚úÖ –î–æ –∑–∞–º–æ—Ä–æ–∑–∫–∏ - –≤—Å–µ —Å–ª–æ–∏ –æ–±—É—á–∞–µ–º—ã–µ:")
encoder_trainable = sum(p.requires_grad for p in s2t_model.model.model.encoder.parameters())
decoder_trainable = sum(p.requires_grad for p in s2t_model.model.model.decoder.parameters())
lm_head_trainable = sum(p.requires_grad for p in s2t_model.model.lm_head.parameters())
print(f"  - Encoder trainable params: {encoder_trainable}")
print(f"  - Decoder trainable params: {decoder_trainable}")
print(f"  - lm_head trainable params: {lm_head_trainable}")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º embeddings –¥–æ –∑–∞–º–æ—Ä–æ–∑–∫–∏
embed_tokens_trainable = s2t_model.model.model.decoder.embed_tokens.weight.requires_grad
lm_head_trainable_flag = s2t_model.model.lm_head.weight.requires_grad
print(f"  - embed_tokens.requires_grad: {embed_tokens_trainable}")
print(f"  - lm_head.requires_grad: {lm_head_trainable_flag}")

print("\n" + "-"*80)
print("–ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º ENCODER (freeze_encoder)")
print("-"*80)
s2t_model.freeze_encoder()

encoder_trainable_after = sum(p.requires_grad for p in s2t_model.model.model.encoder.parameters())
decoder_trainable_after = sum(p.requires_grad for p in s2t_model.model.model.decoder.parameters())
lm_head_trainable_after = sum(p.requires_grad for p in s2t_model.model.lm_head.parameters())
print(f"  - Encoder trainable params: {encoder_trainable_after} (–±—ã–ª–æ {encoder_trainable})")
print(f"  - Decoder trainable params: {decoder_trainable_after} (–±—ã–ª–æ {decoder_trainable})")
print(f"  - lm_head trainable params: {lm_head_trainable_after} (–±—ã–ª–æ {lm_head_trainable})")

embed_tokens_trainable_after = s2t_model.model.model.decoder.embed_tokens.weight.requires_grad
lm_head_trainable_flag_after = s2t_model.model.lm_head.weight.requires_grad
print(f"  - embed_tokens.requires_grad: {embed_tokens_trainable_after} ‚úÖ –ù–ï –∑–∞–º–µ—Ä–∑!")
print(f"  - lm_head.requires_grad: {lm_head_trainable_flag_after} ‚úÖ –ù–ï –∑–∞–º–µ—Ä–∑!")

print("\n" + "-"*80)
print("–ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º DECODER (freeze_decoder)")
print("-"*80)
s2t_model.freeze_decoder()

decoder_trainable_after2 = sum(p.requires_grad for p in s2t_model.model.model.decoder.parameters())
lm_head_trainable_after2 = sum(p.requires_grad for p in s2t_model.model.lm_head.parameters())
print(f"  - Decoder trainable params: {decoder_trainable_after2} (–±—ã–ª–æ {decoder_trainable_after})")
print(f"  - lm_head trainable params: {lm_head_trainable_after2} (–±—ã–ª–æ {lm_head_trainable_after})")

embed_tokens_trainable_after2 = s2t_model.model.model.decoder.embed_tokens.weight.requires_grad
lm_head_trainable_flag_after2 = s2t_model.model.lm_head.weight.requires_grad
print(f"  - embed_tokens.requires_grad: {embed_tokens_trainable_after2} ‚ùå –ó–ê–ú–ï–†–ó!")
print(f"  - lm_head.requires_grad: {lm_head_trainable_flag_after2} ‚úÖ –ù–ï –∑–∞–º–µ—Ä–∑ (–Ω–µ —á–∞—Å—Ç—å decoder)")

print("\n" + "="*80)
print("–†–ï–ó–Æ–ú–ï")
print("="*80)
print("""
üìä WHISPER:
  - freeze_encoder(): –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç encoder (conv, embed_positions, layers)
    ‚îî‚îÄ embed_tokens (decoder) –û–°–¢–ê–Å–¢–°–Ø –û–ë–£–ß–ê–ï–ú–´–ú ‚úÖ
    ‚îî‚îÄ proj_out –û–°–¢–ê–Å–¢–°–Ø –û–ë–£–ß–ê–ï–ú–´–ú ‚úÖ

  - freeze_decoder(): –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç decoder (embed_tokens, embed_positions, layers)
    ‚îî‚îÄ embed_tokens –ó–ê–ú–û–†–ê–ñ–ò–í–ê–ï–¢–°–Ø ‚ùå
    ‚îî‚îÄ proj_out –û–°–¢–ê–Å–¢–°–Ø –û–ë–£–ß–ê–ï–ú–´–ú ‚úÖ (–Ω–µ —á–∞—Å—Ç—å model.model.decoder)

üìä SPEECH2TEXT:
  - freeze_encoder(): –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç encoder (conv, embed_positions, layers)
    ‚îî‚îÄ embed_tokens (decoder) –û–°–¢–ê–Å–¢–°–Ø –û–ë–£–ß–ê–ï–ú–´–ú ‚úÖ
    ‚îî‚îÄ lm_head –û–°–¢–ê–Å–¢–°–Ø –û–ë–£–ß–ê–ï–ú–´–ú ‚úÖ

  - freeze_decoder(): –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç decoder (embed_tokens, embed_positions, layers)
    ‚îî‚îÄ embed_tokens –ó–ê–ú–û–†–ê–ñ–ò–í–ê–ï–¢–°–Ø ‚ùå
    ‚îî‚îÄ lm_head –û–°–¢–ê–Å–¢–°–Ø –û–ë–£–ß–ê–ï–ú–´–ú ‚úÖ (–Ω–µ —á–∞—Å—Ç—å model.model.decoder)

‚ö†Ô∏è –í–ê–ñ–ù–û:
1. –ü—Ä–∏ freeze_decoder() embeddings (embed_tokens) –ó–ê–ú–û–†–ê–ñ–ò–í–ê–Æ–¢–°–Ø!
2. Output projection (proj_out/lm_head) –ù–ï –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç—Å—è (–Ω–µ —á–∞—Å—Ç—å decoder)
3. –î–ª—è cross-lingual transfer –Ω—É–∂–Ω—ã –û–ë–£–ß–ê–ï–ú–´–ï embeddings!
4. –ü–æ—ç—Ç–æ–º—É –Ω–∞—à–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: freeze_encoder=true, freeze_decoder=FALSE
""")
