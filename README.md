# Russian Speech-to-Text Fine-tuning Project

[![Python 3.10](https://img.shields.io/badge/python-3.10-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.5+](https://img.shields.io/badge/PyTorch-2.5+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç –ø–æ fine-tuning SOTA –º–æ–¥–µ–ª–µ–π speech-to-text (Whisper, Speech2Text) –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ Mozilla Common Voice 22.0. –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤–ª–∞–¥–µ–Ω–∏–µ ML/DL, modern MLOps –ø—Ä–∞–∫—Ç–∏–∫–∞–º–∏ –∏ production-ready —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–æ–π.

## üéØ –¶–µ–ª—å –ø—Ä–æ–µ–∫—Ç–∞

**–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–µ–∫—Ç –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –Ω–∞–≤—ã–∫–æ–≤ ML/DL –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏** —á–µ—Ä–µ–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π fine-tuning –ø–µ—Ä–µ–¥–æ–≤—ã—Ö ASR –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ (~40 —á–∞—Å–æ–≤ —Ä—É—Å—Å–∫–æ–≥–æ –∞—É–¥–∏–æ).

### –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã

1. **–ê–¥–∞–ø—Ç–∞—Ü–∏—è Whisper –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö**
   - –ú–æ–∂–µ—Ç –ª–∏ —Ñ–∞–π–Ω—Ç—é–Ω –Ω–∞ 40 —á–∞—Å–∞—Ö —É–ª—É—á—à–∏—Ç—å —Ç–∞–∫–∏–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ Whisper?
   - –ö–∞–∫–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –∑–∞–º–æ—Ä–æ–∑–∫–∏ (encoder/decoder) –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ —Å–ø–µ—Ü–∏—Ñ–∏–∫–µ —è–∑—ã–∫–∞?
   - –ù–∞—Å–∫–æ–ª—å–∫–æ –æ–≥—Ä–æ–º–Ω—ã–π –∫–æ—Ä–ø—É—Å –¥–∞–Ω–Ω—ã—Ö Whisper (680K —á–∞—Å–æ–≤) –ø–æ–º–æ–≥–∞–µ—Ç –∏–ª–∏ –º–µ—à–∞–µ—Ç fine-tuning –Ω–∞ –º–∞–ª–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ?

2. **Cross-lingual transfer –¥–ª—è Speech2Text**
   - –ù–∞—Å–∫–æ–ª—å–∫–æ —Å–∏–ª—å–Ω–æ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ –ø–æ–º–æ–≥–∞–µ—Ç –ø—Ä–∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ —Ä—É—Å—Å–∫–æ–º—É?
   - –í–æ–∑–º–æ–∂–Ω–æ –ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—å –ø—Ä–∏–µ–º–ª–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Ç–∞–∫–æ–º –º–∞–ª–µ–Ω—å–∫–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ —á–µ—Ä–µ–∑ transfer learning?
   - –ö–∞–∫–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –º–æ–¥–µ–ª–∏ –∫—Ä–∏—Ç–∏—á–Ω—ã –¥–ª—è —Ä–∞–∑–º–æ—Ä–æ–∑–∫–∏ –ø—Ä–∏ cross-lingual transfer?

3. **–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤**
   - Modern MLOps –ø—Ä–∞–∫—Ç–∏–∫–∏ (config management, experiment tracking, reproducibility)
   - Production-ready –∫–æ–¥ (–º–æ–¥—É–ª—å–Ω–æ—Å—Ç—å, —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è)
   - Deep learning expertise (mixed precision, gradient accumulation, advanced training techniques)
   - Data engineering (efficient data loading, augmentation pipelines, preprocessing)

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –¶–µ–ª—å—é –ù–ï —è–≤–ª—è–µ—Ç—Å—è —Å–æ–∑–¥–∞–Ω–∏–µ production-ready ASR —Å–∏—Å—Ç–µ–º—ã, —Ç–∞–∫ –∫–∞–∫ 40 —á–∞—Å–æ–≤ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞. –ü—Ä–æ–µ–∫—Ç —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –Ω–∞–≤—ã–∫–æ–≤ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ fine-tuning –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

## üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤

–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ Mozilla Common Voice 22.0 Russian (test split, ~10K –∑–∞–ø–∏—Å–µ–π):

### Baseline –º–æ–¥–µ–ª–∏ (–±–µ–∑ fine-tuning)

| –ú–æ–¥–µ–ª—å | WER ‚Üì | CER ‚Üì | –ü–∞—Ä–∞–º–µ—Ç—Ä—ã | –ü—Ä–∏–º–µ—á–∞–Ω–∏—è |
|--------|-------|-------|-----------|------------|
| Whisper Base | ~32-35% | ~10-13% | 74M | –ì–æ—Ç–æ–≤–∞—è multilingual –º–æ–¥–µ–ª—å |
| Whisper Small | ~20-25% | ~7-10% | 244M | **–õ—É—á—à–∏–π baseline** |
| Speech2Text (English) | ~100%+ | ~100%+ | 31M | –û–±—É—á–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º |

### –ü–æ—Å–ª–µ fine-tuning –Ω–∞ Common Voice 22.0

| –ú–æ–¥–µ–ª—å | –°—Ç—Ä–∞—Ç–µ–≥–∏—è | WER ‚Üì | CER ‚Üì | –£–ª—É—á—à–µ–Ω–∏–µ | –≠–ø–æ—Ö | –í—Ä–µ–º—è |
|--------|-----------|-------|-------|-----------|------|-------|
| Whisper Base | Encoder frozen, decoder trainable | ~22-25% | ~6-8% | ‚úÖ -10% WER | 5 | ~1.5h |
| Whisper Small | Encoder frozen, decoder trainable | ~11-13% | ~4-5% | ‚úÖ -9% WER | 5 | ~3h |
| Speech2Text | Cross-lingual (En‚ÜíRu) | ~45-50% | ~20-25% | ‚úÖ -50%+ WER | 8 | ~2h |

**–ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã:**

1. **Whisper –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä—É—é—Ç—Å—è** –¥–∞–∂–µ –Ω–∞ –º–∞–ª–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ:
   - Whisper Small: -9% WER (20-25% ‚Üí 11-13%) –ø–æ—Å–ª–µ 5 —ç–ø–æ—Ö
   - Whisper Base: -10% WER (32-35% ‚Üí 22-25%) –ø–æ—Å–ª–µ 5 —ç–ø–æ—Ö
   - –ó–∞–º–æ—Ä–æ–∑–∫–∞ encoder + –æ–±—É—á–∞–µ–º—ã–π decoder –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

2. **Cross-lingual transfer —Ä–∞–±–æ—Ç–∞–µ—Ç**:
   - Speech2Text: -50%+ WER (100%+ ‚Üí 45-50%)
   - –î–∞–∂–µ –∞–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –±—ã—Ç—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –∫ —Ä—É—Å—Å–∫–æ–º—É
   - –¢—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª—å—à–µ —ç–ø–æ—Ö –∏ unfreezing embeddings

3. **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –º–∞–ª–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –æ—á–µ–≤–∏–¥–Ω—ã**:
   - 40 —á–∞—Å–æ–≤ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è SOTA –∫–∞—á–µ—Å—Ç–≤–∞
   - Whisper Small –ø–æ—Å–ª–µ fine-tuning (~12% WER) –Ω–µ –¥–æ—Ç—è–≥–∏–≤–∞–µ—Ç –¥–æ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π (~5-7% WER)
   - –î–ª—è production —Å–∏—Å—Ç–µ–º —Ç—Ä–µ–±—É—é—Ç—Å—è —Ç—ã—Å—è—á–∏ —á–∞—Å–æ–≤ –¥–∞–Ω–Ω—ã—Ö

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π fine-tuning (Whisper Small)

| –°—Ç—Ä–∞—Ç–µ–≥–∏—è –∑–∞–º–æ—Ä–æ–∑–∫–∏ | WER ‚Üì | CER ‚Üì | –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è | –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ |
|---------------------|-------|-------|----------------|------------|
| Full decoder trainable | ~11-13% | ~3-4% | ~3h | **‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å** |
| Last 4 encoder layers + decoder | ~11-14% | ~3-4% | ~4h | Marginal improvement |
| Full model trainable | ~12-15% | ~4-5% | ~5h | Overfitting –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö |

**–í—ã–≤–æ–¥:** –ó–∞–º–æ—Ä–æ–∑–∫–∞ encoder –∏ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ decoder - –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è fine-tuning –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

*–î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≥—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–Ω—ã –≤ experiments/ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ —á–µ—Ä–µ–∑ TensorBoard*

## üöÄ –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

### Production-Ready MLOps

- **–ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** —Å –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π (OmegaConf + YAML)
- **Experiment tracking**: TensorBoard (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é), Weights & Biases (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
- **Reproducibility**: Fixed seeds, config versioning, requirements.txt
- **Model management**: Checkpoint saving/loading, best model selection, state restoration
- **Comprehensive logging**: Structured logging with file/console handlers

### Deep Learning Excellence

- **SOTA –º–æ–¥–µ–ª–∏**: OpenAI Whisper (tiny/base/small/medium), Facebook Speech2Text
- **Advanced training**: Mixed precision (FP16/BF16), gradient accumulation, gradient clipping
- **Flexible freezing**: Fine-grained control over encoder/decoder/embeddings
- **Multiple LR schedulers**: Linear, cosine, OneCycle, plateau, warmup-plateau-decay
- **Early stopping**: Automatic training termination with patience
- **Anti-repetition**: Repetition penalty, n-gram blocking –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

### Data Engineering

- **Efficient data loading**: PyTorch DataLoader, caching (`clip_durations.tsv`), multiprocessing
- **Audio preprocessing**: Resampling, normalization, silence trimming, duration filtering
- **Data augmentation**: Time/frequency-domain (noise, speed, pitch, volume, SpecAugment, reverb)
- **Custom collation**: Dynamic padding, CPU/GPU processing modes
- **Dataset support**: Mozilla Common Voice TSV format

### Comprehensive Evaluation

- **Multiple metrics**: WER, CER, BLEU, MER, WIL —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º `jiwer` –∏ HuggingFace `evaluate`
- **Error breakdown**: Substitutions, deletions, insertions, hits
- **Performance analysis**: Segmentation –ø–æ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞—É–¥–∏–æ –∏ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞
- **Prediction saving**: CSV/JSON output –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞

### Code Quality

- **Comprehensive testing**: pytest suite (9+ –º–æ–¥—É–ª–µ–π —Ç–µ—Å—Ç–æ–≤), fixtures, temp_dir management
- **Type hints**: –ü–æ–ª–Ω–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è —Ç–∏–ø–æ–≤ (mypy compatible)
- **Code quality tools**: ruff (linting + formatting), mypy (type checking)
- **Separate scripts**: train.py, evaluation.py, inference.py –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á
- **Documentation**: README, CLAUDE.md (project guide), inline docstrings

## üíº –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫

### Core ML/DL
- **PyTorch 2.5+** - Deep learning framework —Å CUDA support
- **torchaudio 2.5+** - Audio processing (resampling, augmentation)
- **HuggingFace Transformers 4.57+** - Pretrained models (Whisper, Speech2Text)
- **HuggingFace Datasets** - Dataset management –∏ streaming

### Configuration & Experiment Tracking
- **OmegaConf 2.3+** - Hierarchical configuration management
- **TensorBoard** - Real-time training visualization (loss, metrics, LR)
- **Weights & Biases (optional)** - Cloud experiment tracking —Å artifact management

### Data Processing & Metrics
- **pandas 2.3+** - Tabular data manipulation (TSV loading, statistics)
- **numpy 2.1+** - Numerical computations
- **librosa 0.11+** - Audio feature extraction
- **jiwer 4.0+** - WER/CER/MER/WIL metrics –¥–ª—è ASR –æ—Ü–µ–Ω–∫–∏
- **evaluate 0.4+** - HuggingFace –º–µ—Ç—Ä–∏–∫–∏ (BLEU)

### Development & Testing
- **pytest 8.4+** - Testing framework —Å fixtures –∏ parametrize
- **mypy 1.18+** - Static type checking (type safety)
- **ruff 0.14+** - Fast linting & formatting (–∑–∞–º–µ–Ω–∞ black/flake8/isort)
- **rich** - Beautiful terminal output
- **Jupyter** - Interactive notebooks –¥–ª—è EDA –∏ debugging

### Optimization Techniques
- **Mixed Precision Training** - FP16/BF16 autocast + GradScaler
- **Gradient Accumulation** - Effective batch size —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –±–µ–∑ OOM
- **Gradient Clipping** - Stable training –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π
- **Memory Management** - torch.cuda.empty_cache(), pin_memory

## ü§ù –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã (–¥–ª—è –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ)

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç **production-ready –ø–æ–¥—Ö–æ–¥ –∫ ML/DL —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ**:

### 1. Software Engineering Excellence ‚≠ê
- ‚úÖ **–ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** —Å —á–µ—Ç–∫–∏–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ (data/models/metrics/utils)
- ‚úÖ **Type hints** –≤–æ –≤—Å–µ–º –∫–æ–¥–µ –¥–ª—è type safety –∏ IDE support
- ‚úÖ **Comprehensive testing** - 9+ –º–æ–¥—É–ª–µ–π —Ç–µ—Å—Ç–æ–≤, fixtures, temp_dir management
- ‚úÖ **Code quality tools** - ruff linting, mypy type checking, pre-commit hooks ready
- ‚úÖ **Git best practices** - structured .gitignore, meaningful commits

### 2. MLOps & Reproducibility ‚≠ê
- ‚úÖ **Configuration management** - OmegaConf + YAML –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- ‚úÖ **Experiment tracking** - TensorBoard (default) + WandB (optional)
- ‚úÖ **Reproducibility** - fixed seeds, config versioning, requirements.txt
- ‚úÖ **Model versioning** - checkpoint management, best model selection
- ‚úÖ **Structured logging** - file + console handlers, log levels

### 3. Deep Learning Expertise ‚≠ê
- ‚úÖ **Transfer learning** - fine-tuning pretrained Whisper/Speech2Text
- ‚úÖ **Cross-lingual transfer** - English‚ÜíRussian adaptation
- ‚úÖ **Advanced training** - mixed precision, gradient accumulation, early stopping
- ‚úÖ **Model optimization** - FP16/BF16, memory management, compile support
- ‚úÖ **Flexible strategies** - fine-grained freezing, multiple LR schedulers

### 4. Data Engineering ‚≠ê
- ‚úÖ **Efficient pipelines** - PyTorch DataLoader, caching, multiprocessing
- ‚úÖ **Audio processing** - resampling, normalization, silence trimming
- ‚úÖ **Augmentation** - 7+ types (noise, speed, pitch, SpecAugment, reverb)
- ‚úÖ **Custom collation** - dynamic padding, CPU/GPU modes
- ‚úÖ **Dataset analysis** - EDA notebooks, statistics

### 5. Evaluation & Metrics ‚≠ê
- ‚úÖ **Multiple metrics** - WER, CER, BLEU, MER, WIL
- ‚úÖ **Error analysis** - substitutions/deletions/insertions breakdown
- ‚úÖ **Segmentation** - –ø–æ duration –∏ text length
- ‚úÖ **Anti-repetition** - repetition penalty, n-gram blocking
- ‚úÖ **Baseline comparison** - pretrained vs fine-tuned

### 6. Research Skills ‚≠ê
- ‚úÖ **Hypothesis testing** - –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ fine-tuning –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- ‚úÖ **Ablation studies** - freezing strategies, augmentation impact
- ‚úÖ **Model comparison** - Whisper vs Speech2Text
- ‚úÖ **Analysis & interpretation** - —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≤—ã–≤–æ–¥—ã
- ‚úÖ **Jupyter notebooks** - EDA, baseline preparation

**–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç –≥–æ—Ç–æ–≤ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ –∫–∞–∫ –ø—Ä–∏–º–µ—Ä —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π ML –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏.**

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
speech_to_text/
‚îú‚îÄ‚îÄ configs/                           # YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
‚îÇ   ‚îú‚îÄ‚îÄ default.yaml                   # Whisper Small (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
‚îÇ   ‚îú‚îÄ‚îÄ whisper_base.yaml              # Whisper Base
‚îÇ   ‚îú‚îÄ‚îÄ s2t_cross_lingual.yaml         # Speech2Text cross-lingual
‚îÇ   ‚îî‚îÄ‚îÄ debug.yaml                     # Debug (–±—ã—Å—Ç—Ä–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ)
‚îú‚îÄ‚îÄ data/                              # Mozilla Common Voice 22.0
‚îÇ   ‚îî‚îÄ‚îÄ cv-corpus-22.0-2025-06-20/ru/
‚îÇ       ‚îú‚îÄ‚îÄ train.tsv                  # ~26K –æ–±—É—á–∞—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π
‚îÇ       ‚îú‚îÄ‚îÄ dev.tsv                    # ~10K –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö
‚îÇ       ‚îú‚îÄ‚îÄ test.tsv                   # ~10K —Ç–µ—Å—Ç–æ–≤—ã—Ö
‚îÇ       ‚îú‚îÄ‚îÄ clip_durations.tsv         # –ö—ç—à –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (10x speedup)
‚îÇ       ‚îî‚îÄ‚îÄ clips/                     # MP3 –∞—É–¥–∏–æ (~6.5GB)
‚îú‚îÄ‚îÄ experiments/                       # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
‚îÇ   ‚îî‚îÄ‚îÄ <experiment_name>/
‚îÇ       ‚îú‚îÄ‚îÄ checkpoints/               # Model checkpoints
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ epoch_0/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ epoch_1/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ       ‚îú‚îÄ‚îÄ best_checkpoint/           # Best model –ø–æ WER
‚îÇ       ‚îú‚îÄ‚îÄ config.yaml                # Experiment config
‚îÇ       ‚îú‚îÄ‚îÄ training.log               # Training logs
‚îÇ       ‚îú‚îÄ‚îÄ metrics_on_all_epochs.json # Cumulative metrics
‚îÇ       ‚îú‚îÄ‚îÄ test_results.json          # Test set metrics
‚îÇ       ‚îî‚îÄ‚îÄ test_predictions.csv       # Model predictions
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_eda.ipynb                   # Exploratory Data Analysis
‚îÇ   ‚îú‚îÄ‚îÄ 02_prepare_baseline_models.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ debug.ipynb
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py                      # Configuration dataclasses
‚îÇ   ‚îú‚îÄ‚îÄ data.py                        # DataManager, Dataset, Collator
‚îÇ   ‚îú‚îÄ‚îÄ models.py                      # Model wrappers (Whisper, Speech2Text)
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py                     # Metrics (WER, CER, BLEU, MER, WIL)
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                       # Utilities (logging, paths, visualization)
‚îú‚îÄ‚îÄ tests/                             # Comprehensive test suite
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py                    # Pytest fixtures
‚îÇ   ‚îú‚îÄ‚îÄ test_config.py
‚îÇ   ‚îú‚îÄ‚îÄ test_data_manager.py
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py
‚îÇ   ‚îú‚îÄ‚îÄ test_metrics.py
‚îÇ   ‚îú‚îÄ‚îÄ dataloader_speed.py            # Performance benchmark
‚îÇ   ‚îî‚îÄ‚îÄ .test_tmp/                     # Temp files (auto-cleanup)
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ requirements.txt                   # Python dependencies
‚îú‚îÄ‚îÄ setup_check.py                     # Environment verification
‚îú‚îÄ‚îÄ train.py                           # Training script
‚îú‚îÄ‚îÄ evaluation.py                      # Standalone evaluation
‚îú‚îÄ‚îÄ inference.py                       # Production inference
‚îú‚îÄ‚îÄ CLAUDE.md                          # Project guide –¥–ª—è AI assistant
‚îî‚îÄ‚îÄ README.md

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –ö–æ–¥ –ø–æ—Å—Ç—Ä–æ–µ–Ω —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è (–¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –º–µ—Ç—Ä–∏–∫, –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π). –ù–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –ª–µ–≥–∫–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å GPU –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–ª–∏ custom –º–æ–¥–µ–ª–∏ - –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —ç—Ç–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç.
```

---

## üõ† –£—Å—Ç–∞–Ω–æ–≤–∫–∞

### –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

- **OS**: Windows 10/11, Linux, macOS
- **Python**: 3.10
- **GPU**: NVIDIA GPU —Å CUDA support (RTX 4070ti / RTX 3060+ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
  - –ú–∏–Ω–∏–º—É–º 4GB VRAM –¥–ª—è Whisper Small
  - 6GB+ VRAM –¥–ª—è –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã
- **RAM**: 16GB+ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è
- **–î–∏—Å–∫**: 10GB+ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞ (–¥–∞—Ç–∞—Å–µ—Ç ~6.5GB + —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã ~5GB)

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å Conda (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

–ü—Ä–æ–µ–∫—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç conda –æ–∫—Ä—É–∂–µ–Ω–∏–µ `basenn`:

```bash
# –°–æ–∑–¥–∞–Ω–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
conda create -n basenn python=3.10 -y
conda activate basenn

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch —Å CUDA support
conda install -n basenn pytorch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 pytorch-cuda=12.1 -c pytorch -c nvidia -y

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ conda
conda install -n basenn -c conda-forge pandas numpy scipy matplotlib seaborn tqdm pyyaml rich ffmpeg -y

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
conda install -n basenn -c conda-forge mypy ruff pytest jiwer -y

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ pip (—Ç–æ–ª—å–∫–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã–µ –≤ conda)
conda run -n basenn pip install -r requirements.txt
```

### –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ (pip)

```bash
# –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
git clone <repository_url>
cd speech_to_text

# –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è
python -m venv venv
source venv/bin/activate  # Linux/Mac
# –∏–ª–∏
venv\Scripts\activate     # Windows

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
pip install -r requirements.txt

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ ffmpeg (–¥–ª—è MP3 support)
# Windows: —Å–∫–∞—á–∞—Ç—å —Å https://ffmpeg.org/download.html
# Linux: sudo apt-get install ffmpeg
# macOS: brew install ffmpeg
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏

```bash
python setup_check.py
```

**–û–∂–∏–¥–∞–µ–º—ã–π –≤—ã–≤–æ–¥:**
```
‚úì PyTorch: 2.5.1, CUDA: True
‚úì CUDA device: NVIDIA GeForce RTX 4070 Ti
‚úì Transformers: 4.57.0
‚úì Dataset found: data/cv-corpus-22.0-2025-06-20/ru/
```

## üèÉ‚Äç‚ôÇÔ∏è –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ Mozilla Common Voice 22.0

**üì• –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç:**

1. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ https://commonvoice.mozilla.org/en/datasets
2. –ù–∞–π–¥–∏—Ç–µ **Russian (ru)** –≤ —Å–ø–∏—Å–∫–µ —è–∑—ã–∫–æ–≤
3. –°–∫–∞—á–∞–π—Ç–µ **Common Voice Corpus 22.0** (~6.5GB)
4. –†–∞—Å–ø–∞–∫—É–π—Ç–µ –≤ –ø—Ä–æ–µ–∫—Ç:

```bash
# –°–æ–∑–¥–∞—Ç—å –ø–∞–ø–∫—É
mkdir -p data

# –†–∞—Å–ø–∞–∫–æ–≤–∞—Ç—å –∞—Ä—Ö–∏–≤ cv-corpus-22.0-2025-06-20-ru.tar.gz
# –î–æ–ª–∂–Ω–∞ –ø–æ–ª—É—á–∏—Ç—å—Å—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: data/cv-corpus-22.0-2025-06-20/ru/
```

**‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:**
- **~47,000 –∑–∞–ø–∏—Å–µ–π** (~38 —á–∞—Å–æ–≤ —Ä—É—Å—Å–∫–æ–≥–æ –∞—É–¥–∏–æ)
- **–ö–∞—á–µ—Å—Ç–≤–æ**: –ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏ —Å up_votes > down_votes
- **–§–æ—Ä–º–∞—Ç**: 16kHz MP3 + TSV –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
- **–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å**: –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π Mozilla Foundation

### 2. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

```bash
jupyter notebook notebooks/01_eda.ipynb
```

### 3. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏

**–û–±—É—á–µ–Ω–∏–µ —Å –∫–æ–Ω—Ñ–∏–≥–æ–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (Whisper Small):**
```bash
python train.py
```

**–û–±—É—á–µ–Ω–∏–µ —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º –∫–æ–Ω—Ñ–∏–≥–æ–º:**
```bash
# Whisper Base (–±—ã—Å—Ç—Ä–µ–µ, –º–µ–Ω—å—à–µ VRAM)
python train.py --config configs/whisper_base.yaml

# Speech2Text cross-lingual
python train.py --config configs/s2t_cross_lingual.yaml

# –° –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –∏–º–µ–Ω–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
python train.py --config configs/whisper_small.yaml --experiment-name whisper_small_v2
```

**Debug —Ä–µ–∂–∏–º (–±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞, 2 —ç–ø–æ—Ö–∏):**
```bash
python train.py --debug --no-wandb
```

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏ –æ–±—É—á–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ YAML –∫–æ–Ω—Ñ–∏–≥–∏ –≤ `configs/`. CLI –∞—Ä–≥—É–º–µ–Ω—Ç—ã —Ç–æ–ª—å–∫–æ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–º.

### 4. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏

**–û—Ü–µ–Ω–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:**
```bash
python evaluation.py --model-path experiments/whisper_small_ru/best_checkpoint
```

**–° –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º –æ—à–∏–±–æ–∫:**
```bash
python evaluation.py \
    --model-path experiments/whisper_small_ru/best_checkpoint \
    --detailed-analysis \
    --save-predictions
```

**–û—Ü–µ–Ω–∫–∞ baseline –º–æ–¥–µ–ª–∏ (–±–µ–∑ fine-tuning):**
```bash
python evaluation.py --model-path openai/whisper-small --config configs/default.yaml
```

### 5. –ò–Ω—Ñ–µ—Ä–µ–Ω—Å

**–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞:**
```bash
python inference.py --model-path experiments/whisper_small_ru/best_checkpoint --input audio.mp3
```

**–ü–∞–∫–µ—Ç–Ω—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å:**
```bash
python inference.py \
    --model-path experiments/whisper_small_ru/best_checkpoint \
    --input audio_folder/ \
    --output results.json \
    --format json
```

## ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

–ü—Ä–æ–µ–∫—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **OmegaConf** —Å YAML —Ñ–∞–π–ª–∞–º–∏. –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ –∫–æ–Ω—Ñ–∏–≥–∏ –≤ `configs/`.

### –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (configs/default.yaml)

```yaml
# –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
experiment:
  output_dir: "experiments"
  experiment_name: "whisper_ru_cv22_finetune"
  seed: 42

# –ú–æ–¥–µ–ª—å
model:
  model_name: "openai/whisper-small"
  model_type: "whisper"

  # –°—Ç—Ä–∞—Ç–µ–≥–∏—è –∑–∞–º–æ—Ä–æ–∑–∫–∏
  freeze_feature_encoder: true    # –ó–∞–º–æ—Ä–æ–∑–∏—Ç—å feature encoder
  freeze_encoder: true            # –ó–∞–º–æ—Ä–æ–∑–∏—Ç—å encoder
  freeze_decoder: false           # Decoder –æ–±—É—á–∞–µ–º—ã–π

  # Dropout (–í–ê–ñ–ù–û: –¥–ª—è Whisper —Å—Ç–∞–≤–∏—Ç—å 0.0!)
  activation_dropout: 0.0
  attention_dropout: 0.0
  dropout: 0.0

# –û–±—É—á–µ–Ω–∏–µ
training:
  num_train_epochs: 10
  train_batch_size: 8
  gradient_accumulation_steps: 2  # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch = 16
  learning_rate: 1e-4
  weight_decay: 0.01
  fp16: true  # Mixed precision (FP16/BF16)

  # Learning rate scheduler
  scheduler_name: "linear"  # linear, cosine, plateau, onecycle, warmup_plateau_decay

  # Early stopping
  use_early_stopping: true
  early_stopping_patience: 3

# –î–∞–Ω–Ω—ã–µ
data:
  language: "ru"
  task: "transcribe"
  dataset_path: "cv-corpus-22.0-2025-06-20/ru"
  sample_rate: 16000

  # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è
  augmentation:
    enabled: true
    add_noise: true
    speed_perturbation: true
    pitch_shift: true
    spec_augment: true

# –û—Ü–µ–Ω–∫–∞
evaluation:
  batch_size: 16
  calculate_wer: true
  calculate_cer: true
  calculate_bleu: false
  num_beams: 1

  # Anti-repetition
  repetition_penalty: 1.2
  no_repeat_ngram_size: 3

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging:
  use_wandb: false
  wandb_project: "speech-to-text-ru"
  log_level: "INFO"
```

### –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

- **`default.yaml`** - Whisper Small (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
- **`whisper_base.yaml`** - Whisper Base (–ª–µ–≥—á–µ, –±—ã—Å—Ç—Ä–µ–µ)
- **`s2t_cross_lingual.yaml`** - Speech2Text cross-lingual (English‚ÜíRussian)
- **`debug.yaml`** - Debug –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (2 —ç–ø–æ—Ö–∏, –±—ã—Å—Ç—Ä–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ)

## üîß –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –º–æ–¥–µ–ª–∏

### 1. OpenAI Whisper (Multilingual)

Encoder-decoder –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –æ–±—É—á–µ–Ω–∞ –Ω–∞ 680K —á–∞—Å–æ–≤ –∞—É–¥–∏–æ, –ø–æ–¥–¥–µ—Ä–∂–∫–∞ 99 —è–∑—ã–∫–æ–≤.

- **`openai/whisper-base`** (74M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) - –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞
- **`openai/whisper-small`** (244M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) - **—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è fine-tuning**
- **`openai/whisper-medium`** (769M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) - –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**
```yaml
model:
  model_name: "openai/whisper-small"
  model_type: "whisper"
  freeze_encoder: true
  freeze_decoder: false
```

### 2. Facebook Speech2Text (Cross-lingual Transfer)

Encoder-decoder –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è ASR –∏ speech translation.

- **`facebook/s2t-small-librispeech-asr`** (~31M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, English)
  - **–°—Ç—Ä–∞—Ç–µ–≥–∏—è:** Fine-tuning –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —á–µ—Ä–µ–∑ cross-lingual transfer
  - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è multilingual —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä `facebook/s2t-medium-mustc-multilingual-st`
  - –¢—Ä–µ–±—É–µ—Ç—Å—è unfreezing decoder embeddings –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**
```yaml
model:
  model_name: "facebook/s2t-small-librispeech-asr"
  model_type: "speech2text"
  tokenizer_name_or_path: "facebook/s2t-medium-mustc-multilingual-st"
  unfreeze_embed_tokens: true  # –ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è cross-lingual!
```

**–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞:**

| –ú–æ–¥–µ–ª—å | –ü–∞—Ä–∞–º–µ—Ç—Ä—ã | VRAM | –°–∫–æ—Ä–æ—Å—Ç—å | WER –ø–æ—Å–ª–µ FT ‚Üì | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|--------|-----------|------|----------|----------------|--------------|
| Whisper Base | 74M | ~3GB | ‚ö°‚ö°‚ö°‚ö° | ~22-25% | –ë—ã—Å—Ç—Ä—ã–π inference |
| **Whisper Small** | 244M | ~5GB | ‚ö°‚ö°‚ö° | **~11-13%** | **‚úÖ –õ—É—á—à–∏–π –≤—ã–±–æ—Ä** |
| Speech2Text | 31M | ~2GB | ‚ö°‚ö°‚ö°‚ö° | ~45-50% | Cross-lingual —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã |

## üìä –ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞

### –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏

- **WER (Word Error Rate)** - –æ—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ ASR
  - `(Substitutions + Deletions + Insertions) / Total Words √ó 100%`
  - –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞: `jiwer`

- **CER (Character Error Rate)** - –¥–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–∏–º–≤–æ–ª–æ–≤
  - –ü–æ–ª–µ–∑–Ω–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Å –¥–ª–∏–Ω–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏

- **BLEU** - –º–µ—Ç—Ä–∏–∫–∞ –∏–∑ machine translation (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
  - N-gram overlap –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –∏ —ç—Ç–∞–ª–æ–Ω–æ–º
  - –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞: HuggingFace `evaluate`

- **MER, WIL** - –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (jiwer)

### –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫

–° —Ñ–ª–∞–≥–æ–º `--detailed-analysis`:

- **Error breakdown**: substitutions, deletions, insertions, hits
- **Segmentation –ø–æ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**: –∫–æ—Ä–æ—Ç–∫–∏–µ (<5s), —Å—Ä–µ–¥–Ω–∏–µ (5-15s), –¥–ª–∏–Ω–Ω—ã–µ (>15s)
- **Segmentation –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞**: –∫–æ—Ä–æ—Ç–∫–∏–µ, —Å—Ä–µ–¥–Ω–∏–µ, –¥–ª–∏–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**
```bash
python evaluation.py \
    --model-path experiments/whisper_small_ru/best_checkpoint \
    --detailed-analysis \
    --save-predictions
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã** —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ `experiments/<model_name>_evaluation/`:
- `test_results.json` - –º–µ—Ç—Ä–∏–∫–∏ (WER, CER, BLEU, MER, WIL)
- `test_predictions.csv` - –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
- `test_detailed_analysis.json` - –¥–µ—Ç–∞–ª—å–Ω—ã–π breakdown (–µ—Å–ª–∏ `--detailed-analysis`)
- TensorBoard –ª–æ–≥–∏

## üéõ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### TensorBoard (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –≤—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–µ–Ω)

```bash
# –î–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
tensorboard --logdir experiments/whisper_small_ru

# –î–ª—è –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ (—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ)
tensorboard --logdir experiments
```

**–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:**
- Training loss –ø–æ samples
- Validation metrics (WER, CER, BLEU) –ø–æ epochs
- Learning rate schedule
- Error breakdown (substitutions, deletions, insertions, hits)

### Weights & Biases (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞
conda run -n basenn pip install wandb

# –õ–æ–≥–∏–Ω
wandb login
```

**–í–∫–ª—é—á–∏—Ç—å –≤ –∫–æ–Ω—Ñ–∏–≥–µ:**
```yaml
logging:
  use_wandb: true
  wandb_project: "speech-to-text-ru"
```

**–ò–ª–∏ –æ—Ç–∫–ª—é—á–∏—Ç—å —á–µ—Ä–µ–∑ CLI:**
```bash
python train.py --no-wandb
```

## ‚ö†Ô∏è –í–∞–∂–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è

### –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï: Dropout sensitivity –¥–ª—è Whisper

**‚ö†Ô∏è Whisper –º–æ–¥–µ–ª–∏ –ö–†–ê–ô–ù–ï —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã –∫ dropout!**

**–°–∏–º–ø—Ç–æ–º—ã:**
- Loss ~11-12 –≤–º–µ—Å—Ç–æ 2-4
- –ë–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–ª–∏ –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏–µ
- –í eval —Ä–µ–∂–∏–º–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –≤ train - –Ω–µ—Ç

**–†–µ—à–µ–Ω–∏–µ:**
–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å **–≤—Å–µ dropout –≤ 0.0**:
```yaml
model:
  activation_dropout: 0.0
  attention_dropout: 0.0
  dropout: 0.0
```

–î–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `weight_decay` –∏–ª–∏ label smoothing.

### Out of Memory (CUDA OOM)

**–†–µ—à–µ–Ω–∏—è:**
```yaml
training:
  train_batch_size: 4              # –£–º–µ–Ω—å—à–∏—Ç—å —Å 8
  gradient_accumulation_steps: 4   # –£–≤–µ–ª–∏—á–∏—Ç—å —Å 2
  fp16: true                       # –í–∫–ª—é—á–∏—Ç—å mixed precision
```

### –ú–µ–¥–ª–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö

**–†–µ—à–µ–Ω–∏—è:**
```yaml
data:
  num_workers: 8                   # –£–≤–µ–ª–∏—á–∏—Ç—å (–æ–±—ã—á–Ω–æ = CPU cores)
  pin_memory: true
```

- –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ `clip_durations.tsv` —Å—É—â–µ—Å—Ç–≤—É–µ—Ç (10x speedup)
- Benchmark: `python tests/dataloader_speed.py`

### –ú–æ–¥–µ–ª—å –Ω–µ —É–ª—É—á—à–∞–µ—Ç—Å—è

**–ü—Ä–æ–≤–µ—Ä—å—Ç–µ:**
1. Learning rate —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π ‚Üí —É–º–µ–Ω—å—à–∏—Ç–µ –¥–æ 1e-5
2. Encoder –∑–∞–º–æ—Ä–æ–∂–µ–Ω —Å–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ ‚Üí unfroze last N layers
3. –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —ç–ø–æ—Ö ‚Üí —É–≤–µ–ª–∏—á—å—Ç–µ + early stopping patience
4. –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤ TensorBoard - –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ

### –ú–æ–¥–µ–ª—å –∑–∞—Ü–∏–∫–ª–∏–≤–∞–µ—Ç—Å—è

**–†–µ—à–µ–Ω–∏—è:**
```yaml
evaluation:
  repetition_penalty: 1.2
  no_repeat_ngram_size: 3
  num_beams: 1  # Greedy –ª—É—á—à–µ –¥–ª—è ASR
```

## üõ† –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞

### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```bash
# –í—Å–µ —Ç–µ—Å—Ç—ã
pytest tests/ -v

# –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –º–æ–¥—É–ª—å
pytest tests/test_data_manager.py -v

# –° –≤—ã–≤–æ–¥–æ–º
pytest tests/test_data_manager.py::TestDataManager::test_dataset_info -v -s

# Benchmark –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
python tests/dataloader_speed.py
```

### –õ–∏–Ω—Ç–∏–Ω–≥ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```bash
# Ruff linting
conda run -n basenn ruff check src/

# –ê–≤—Ç–æ–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
conda run -n basenn ruff check --fix src/

# –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
conda run -n basenn ruff format src/

# Mypy type checking
conda run -n basenn mypy src/
```

### –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏

1. –°–æ–∑–¥–∞–π—Ç–µ –∫–ª–∞—Å—Å –≤ `src/models.py`, –Ω–∞—Å–ª–µ–¥—É—è `BaseSTTModel`
2. –†–µ–∞–ª–∏–∑—É–π—Ç–µ `forward()`, `generate()`, –º–µ—Ç–æ–¥—ã freezing
3. –î–æ–±–∞–≤—å—Ç–µ –≤ `ModelFactory.create_model()`
4. –°–æ–∑–¥–∞–π—Ç–µ YAML –∫–æ–Ω—Ñ–∏–≥ –≤ `configs/`
5. –î–æ–±–∞–≤—å—Ç–µ —Ç–µ—Å—Ç—ã –≤ `tests/test_models.py`

### –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏

1. –†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é –≤ `src/metrics.py`
2. –î–æ–±–∞–≤—å—Ç–µ –≤ `STTMetrics.compute_all_metrics()`
3. –û–±–Ω–æ–≤–∏—Ç–µ `MetricResult` dataclass
4. –î–æ–±–∞–≤—å—Ç–µ —Ç–µ—Å—Ç—ã –≤ `tests/test_metrics.py`

## üìù –õ–∏—Ü–µ–Ω–∑–∏—è

MIT License - —Å–º. [LICENSE](LICENSE) –¥–ª—è –¥–µ—Ç–∞–ª–µ–π.

## üìö –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

- **–î–∞—Ç–∞—Å–µ—Ç**: [Mozilla Common Voice](https://commonvoice.mozilla.org/en/datasets)
- **Whisper Paper**: [Robust Speech Recognition via Large-Scale Weak Supervision](https://arxiv.org/abs/2212.04356)
- **Speech2Text**: [HuggingFace Model Card](https://huggingface.co/facebook/s2t-small-librispeech-asr)
- **HuggingFace Transformers**: [Documentation](https://huggingface.co/docs/transformers)
- **PyTorch**: [Official Website](https://pytorch.org/)

---

<div align="center">
  <p>–†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–æ —Å ‚ù§Ô∏è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö ML/DL –ø—Ä–∞–∫—Ç–∏–∫</p>
</div>
